{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# disable chained assignments\n",
    "pd.options.mode.chained_assignment = None \n",
    "import os, gc\n",
    "from darts import TimeSeries\n",
    "import optuna, optuna_dashboard\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import seed_everything\n",
    "from datetime import datetime\n",
    "\n",
    "# this stops pytorch from logging GPU info each time your model predicts something\n",
    "# https://github.com/Lightning-AI/lightning/issues/3431\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "\n",
    "# for some warning bugs from darts\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "from utils import *\n",
    "from splits import *\n",
    "from plotter import *\n",
    "\n",
    "# make sure to set these False for scripts, otherwise it'll print lots of logs\n",
    "VERBOSE = False\n",
    "Split = Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Result folder\n",
    "output_folder = 'scratch/NBEATS'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    static_features = ['AgeDist', 'HealthDisp']\n",
    "    past_features = ['DiseaseSpread', 'Transmission', 'VaccinationFull', 'SocialDist']\n",
    "    known_future = ['SinWeekly', 'CosWeekly']\n",
    "    time_index = 'TimeFromStart' # note that this is an index feature commonly used by all timeseries models\n",
    "\n",
    "    features =  static_features + past_features + known_future\n",
    "    targets = ['Cases']\n",
    "    group_id = 'FIPS'\n",
    "    selected_columns = features + targets\n",
    "    input_sequence_length = 13\n",
    "    output_sequence_length = 15\n",
    "    epochs = 60\n",
    "    early_stopping_patience = 3\n",
    "    n_trials = 25\n",
    "    seed = 7\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "targets = Config.targets\n",
    "group_id = Config.group_id\n",
    "time_index = Config.time_index\n",
    "input_sequence_length = Config.input_sequence_length\n",
    "output_sequence_length = Config.output_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FIPS  AgeDist  HealthDisp       Date  DiseaseSpread  Transmission  \\\n",
      "0  2261    0.014         8.8 2020-02-29            0.0           0.0   \n",
      "1  2261    0.014         8.8 2020-03-01            0.0           0.0   \n",
      "2  2261    0.014         8.8 2020-03-02            0.0           0.0   \n",
      "\n",
      "   VaccinationFull  SocialDist  Cases  TimeFromStart  SinWeekly  CosWeekly  \n",
      "0              0.0         0.5    0.0              0     -0.975     -0.223  \n",
      "1              0.0         0.5    0.0              1     -0.782      0.623  \n",
      "2              0.0         0.5    0.0              2      0.000      1.000  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../TFT-pytorch/2022_May_cleaned/Top_100.csv')\n",
    "df['Date'] = to_datetime(df['Date'])\n",
    "df[time_index] = df[time_index].astype(int)\n",
    "\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: train (64000, 12), validation (2800, 12), test (2800, 12).\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = split_data(df, Split, input_sequence_length)\n",
    "train_df, val_df, test_df, feature_scaler, target_scaler = scale_data(\n",
    "    train_df, val_df, test_df, Config.features, targets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import round, mean, float32\n",
    "\n",
    "def get_covariates(df:pd.DataFrame, tail_cut=False):\n",
    "    if tail_cut:\n",
    "        cutoff = df[time_index].max() - output_sequence_length + 1\n",
    "        df = df[df[time_index]<cutoff]\n",
    "\n",
    "    series = TimeSeries.from_group_dataframe(\n",
    "        df, time_col=time_index, group_cols=group_id,\n",
    "        static_cols=Config.static_features, value_cols=targets,\n",
    "    )\n",
    "    past_covariates = TimeSeries.from_group_dataframe(\n",
    "        df, group_cols=group_id,\n",
    "        time_col = time_index, value_cols=Config.past_features\n",
    "    )\n",
    "\n",
    "    # timeseries has default precision float64, this doesn't match \n",
    "    # with pl trainer which has precision float32\n",
    "    for covariates in [series, past_covariates]:\n",
    "        for index in range(len(covariates)):\n",
    "            covariates[index] = covariates[index].astype(float32)\n",
    "\n",
    "    return series, past_covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series, train_past_covariates = get_covariates(train_df)\n",
    "val_series, val_past_covariates = get_covariates(val_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import NBEATSModel\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from torch.nn.modules import MSELoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.3, step=0.1)\n",
    "    layers = trial.suggest_int(\"layers\", 2, 4, step=1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    return NBEATSModel(\n",
    "        input_chunk_length=input_sequence_length, \n",
    "        output_chunk_length=output_sequence_length,\n",
    "        loss_fn=MSELoss(), optimizer_cls=Adam,\n",
    "        batch_size=batch_size, num_layers=layers, \n",
    "        dropout=dropout,   \n",
    "        optimizer_kwargs={'lr': learning_rate}\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def historical_forecast(\n",
    "    model, df, series_list, past_list, target_scaler\n",
    "):\n",
    "    prediction_start = df[time_index].min() + input_sequence_length\n",
    "\n",
    "    preds = []\n",
    "    fips_codes = df[group_id].unique()\n",
    "\n",
    "    for index in tqdm(range(len(fips_codes)), disable=not VERBOSE):\n",
    "        fips, series, past = fips_codes[index], series_list[index], past_list[index]\n",
    "\n",
    "        if len(series) > (input_sequence_length + output_sequence_length):\n",
    "            # list of predictions with sliding window\n",
    "            county_preds = model.historical_forecasts(\n",
    "                series, \n",
    "                past_covariates=past,\n",
    "                start=prediction_start,\n",
    "                retrain=False, last_points_only=False, verbose=False,\n",
    "                forecast_horizon=output_sequence_length, stride=1,\n",
    "            )\n",
    "            # reseting index here is ok since only one time column\n",
    "            county_preds = pd.concat(\n",
    "                [pred.pd_dataframe().reset_index() for pred in county_preds], axis=0\n",
    "            )\n",
    "        else:\n",
    "            county_preds = model.predict(\n",
    "                output_sequence_length,\n",
    "                series, n_jobs=-1,\n",
    "                past_covariates=past,\n",
    "                verbose=False\n",
    "            )\n",
    "            county_preds = county_preds.pd_dataframe().reset_index()\n",
    "\n",
    "        county_preds[group_id] = fips\n",
    "        preds.append(county_preds)\n",
    "\n",
    "    # conver the predicted list to a dataframe\n",
    "    preds = pd.concat(preds, axis=0).reset_index(drop=True)\n",
    "    # scale up\n",
    "    # preds[targets] = target_scaler.inverse_transform(\n",
    "    #     preds[targets].values\n",
    "    # )\n",
    "    # round and remove negative targets since infection can't be neg\n",
    "    preds[targets] = preds[targets].apply(round)\n",
    "    for target in targets:\n",
    "        preds.loc[preds[target]<0, target] = 0\n",
    "        \n",
    "    # since this is sliding window, some cases will have multiple prediction with different forecast horizon\n",
    "    preds = preds.groupby([group_id, time_index], axis=0)[targets].aggregate(mean)\n",
    "\n",
    "    preds.rename({target:'Predicted_'+target for target in targets}, axis=1, inplace=True)\n",
    "\n",
    "    target_df = df[[group_id, time_index, 'Date'] + targets].copy().reset_index(drop=True)\n",
    "    # target_df[targets] = target_scaler.inverse_transform(target_df[targets]).astype(int)\n",
    "\n",
    "    merge_keys = [group_id, time_index]\n",
    "    prediction_df = preds.merge(target_df[['Date'] + merge_keys + targets], on=merge_keys, how='inner')\n",
    "    gc.collect()\n",
    "\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=Config.early_stopping_patience,\n",
    "        min_delta=0\n",
    "    )\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=output_folder, monitor=\"val_loss\"\n",
    "    )\n",
    "\n",
    "    model = create_model(trial)\n",
    "    val_series, val_past_covariates = get_covariates(val_df)\n",
    "    model.fit(\n",
    "        train_series, val_series=val_series, verbose=False,\n",
    "        past_covariates=train_past_covariates, val_past_covariates=val_past_covariates,\n",
    "        trainer = Trainer(\n",
    "            accelerator= \"auto\", max_epochs=Config.epochs,\n",
    "            callbacks=[early_stopping, checkpoint], \n",
    "            logger=False, enable_progress_bar=False\n",
    "        )\n",
    "    )\n",
    "    # gc.collect()\n",
    "    model.load(checkpoint.best_model_path)\n",
    "    val_series, val_past_covariates = get_covariates(val_df, tail_cut=True)\n",
    "    val_prediction_df = historical_forecast(\n",
    "        model, val_df, val_series, val_past_covariates, target_scaler\n",
    "    )\n",
    "\n",
    "    val_loss = mean_squared_error(\n",
    "        val_prediction_df['Cases'], val_prediction_df[\"Predicted_Cases\"]\n",
    "    )\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-24 14:54:53,749]\u001b[0m Using an existing study with name 'nbeats' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-12-24 14:56:33,727]\u001b[0m Trial 11 finished with value: 0.547431684357568 and parameters: {'learning_rate': 9.706170724196345e-05, 'dropout': 0.2, 'layers': 2, 'batch_size': 128}. Best is trial 11 with value: 0.547431684357568.\u001b[0m\n",
      "\u001b[32m[I 2022-12-24 14:56:41,271]\u001b[0m Trial 10 finished with value: 0.5802909576028339 and parameters: {'learning_rate': 0.00024646104484689764, 'dropout': 0.2, 'layers': 4, 'batch_size': 64}. Best is trial 11 with value: 0.547431684357568.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  12\n",
      "Best trial:\n",
      "  Value:  0.547431684357568\n",
      "  Params: \n",
      "    batch_size: 128\n",
      "    dropout: 0.2\n",
      "    layers: 2\n",
      "    learning_rate: 9.706170724196345e-05\n"
     ]
    }
   ],
   "source": [
    "study_name = 'nbeats'\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name, storage=storage_name, direction='minimize', load_if_exists=True\n",
    ")\n",
    "study.optimize(\n",
    "    objective, n_trials=Config.n_trials, n_jobs=-1, \n",
    "    gc_after_trial=True, show_progress_bar=VERBOSE\n",
    ")\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "\n",
    "df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\"))\n",
    "df.round(6).to_csv(os.path.join(output_folder, 'trials.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43fc5fbfa959c1c54ddf7d7acab30a2019a504b895513ba1ba722e7f395657c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
