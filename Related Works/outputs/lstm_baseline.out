2023-01-21 22:45:42.616791: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 22:45:42.770031: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 22:45:43.745813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /u/mi3se/anaconda3/envs/ml/lib:/sw/centos-7.4/anaconda3/current/lib:/sw/centos-7.4/cudnn/current/lib64:/sw/centos-7.4/cuda/current/extras/CUPTI/lib64:/sw/centos-7.4/cuda/current/lib64:/lib::/u/mi3se/anaconda3/envs/ml/lib/
2023-01-21 22:45:43.745910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /u/mi3se/anaconda3/envs/ml/lib:/sw/centos-7.4/anaconda3/current/lib:/sw/centos-7.4/cudnn/current/lib64:/sw/centos-7.4/cuda/current/extras/CUPTI/lib64:/sw/centos-7.4/cuda/current/lib64:/lib::/u/mi3se/anaconda3/envs/ml/lib/
2023-01-21 22:45:43.745918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 22:51:37.526519: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 22:51:37.973636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38266 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0
   FIPS  AgeDist  HealthDisp  ... LinearSpace  SinWeekly  CosWeekly
0  1001   0.1611       4.202  ...         0.0    -0.9749    -0.2225
1  1001   0.1611       4.202  ...         0.0    -0.7818     0.6235
2  1001   0.1611       4.202  ...         0.0     0.0000     1.0000

[3 rows x 14 columns]
Shapes: train (2010880, 14), validation (87976, 14), test (87976, 14).
Shapes: data (1926046, 13, 10), labels (1926046, 15).
Shapes: data (3142, 13, 10), labels (3142, 15).
Shapes: data (3142, 13, 10), labels (3142, 15).
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 13, 64)            19200     
                                                                 
 dropout (Dropout)           (None, 13, 64)            0         
                                                                 
 lstm_1 (LSTM)               (None, 64)                33024     
                                                                 
 dense (Dense)               (None, 15)                975       
                                                                 
=================================================================
Total params: 53,199
Trainable params: 53,199
Non-trainable params: 0
_________________________________________________________________

----Training started at 2023-01-21 22:51:39.322947----

Epoch 1/200
2023-01-21 22:51:42.395733: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201
2023-01-21 22:51:43.223229: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
15048/15048 - 60s - loss: 0.7606 - val_loss: 0.7368 - 60s/epoch - 4ms/step
Epoch 2/200
15048/15048 - 55s - loss: 0.6121 - val_loss: 0.7026 - 55s/epoch - 4ms/step
Epoch 3/200
15048/15048 - 52s - loss: 0.5661 - val_loss: 0.6918 - 52s/epoch - 3ms/step
Epoch 4/200
15048/15048 - 52s - loss: 0.5390 - val_loss: 0.6764 - 52s/epoch - 3ms/step
Epoch 5/200
15048/15048 - 52s - loss: 0.5185 - val_loss: 0.6544 - 52s/epoch - 3ms/step
Epoch 6/200
15048/15048 - 52s - loss: 0.5036 - val_loss: 0.6415 - 52s/epoch - 3ms/step
Epoch 7/200
15048/15048 - 52s - loss: 0.4903 - val_loss: 0.6282 - 52s/epoch - 3ms/step
Epoch 8/200
15048/15048 - 52s - loss: 0.4805 - val_loss: 0.6163 - 52s/epoch - 3ms/step
Epoch 9/200
15048/15048 - 52s - loss: 0.4709 - val_loss: 0.6087 - 52s/epoch - 3ms/step
Epoch 10/200
15048/15048 - 55s - loss: 0.4629 - val_loss: 0.5997 - 55s/epoch - 4ms/step
Epoch 11/200
15048/15048 - 53s - loss: 0.4543 - val_loss: 0.5918 - 53s/epoch - 3ms/step
Epoch 12/200
15048/15048 - 53s - loss: 0.4492 - val_loss: 0.5886 - 53s/epoch - 3ms/step
Epoch 13/200
15048/15048 - 54s - loss: 0.4427 - val_loss: 0.5820 - 54s/epoch - 4ms/step
Epoch 14/200
15048/15048 - 55s - loss: 0.4365 - val_loss: 0.5823 - 55s/epoch - 4ms/step
Epoch 15/200
15048/15048 - 55s - loss: 0.4309 - val_loss: 0.5779 - 55s/epoch - 4ms/step
Epoch 16/200
15048/15048 - 56s - loss: 0.4247 - val_loss: 0.5808 - 56s/epoch - 4ms/step
Epoch 17/200
15048/15048 - 53s - loss: 0.4211 - val_loss: 0.5762 - 53s/epoch - 4ms/step
Epoch 18/200
15048/15048 - 53s - loss: 0.4156 - val_loss: 0.5814 - 53s/epoch - 4ms/step
Epoch 19/200
15048/15048 - 53s - loss: 0.4119 - val_loss: 0.5758 - 53s/epoch - 4ms/step
Epoch 20/200
15048/15048 - 53s - loss: 0.4076 - val_loss: 0.5757 - 53s/epoch - 3ms/step
Epoch 21/200
15048/15048 - 53s - loss: 0.4036 - val_loss: 0.5668 - 53s/epoch - 4ms/step
Epoch 22/200
15048/15048 - 53s - loss: 0.4010 - val_loss: 0.5546 - 53s/epoch - 4ms/step
Epoch 23/200
15048/15048 - 53s - loss: 0.3965 - val_loss: 0.5634 - 53s/epoch - 4ms/step
Epoch 24/200
15048/15048 - 53s - loss: 0.3944 - val_loss: 0.5598 - 53s/epoch - 4ms/step
Epoch 25/200
15048/15048 - 53s - loss: 0.3904 - val_loss: 0.5616 - 53s/epoch - 4ms/step
Epoch 26/200
15048/15048 - 53s - loss: 0.3866 - val_loss: 0.5659 - 53s/epoch - 4ms/step
Epoch 27/200
15048/15048 - 53s - loss: 0.3834 - val_loss: 0.5363 - 53s/epoch - 4ms/step
Epoch 28/200
15048/15048 - 60s - loss: 0.3795 - val_loss: 0.5497 - 60s/epoch - 4ms/step
Epoch 29/200
15048/15048 - 53s - loss: 0.3762 - val_loss: 0.5228 - 53s/epoch - 4ms/step
Epoch 30/200
15048/15048 - 53s - loss: 0.3728 - val_loss: 0.5602 - 53s/epoch - 4ms/step
Epoch 31/200
15048/15048 - 53s - loss: 0.3693 - val_loss: 0.5332 - 53s/epoch - 4ms/step
Epoch 32/200
15048/15048 - 53s - loss: 0.3674 - val_loss: 0.5410 - 53s/epoch - 4ms/step
Epoch 33/200
15048/15048 - 53s - loss: 0.3636 - val_loss: 0.5280 - 53s/epoch - 4ms/step
Epoch 34/200
15048/15048 - 53s - loss: 0.3615 - val_loss: 0.5215 - 53s/epoch - 4ms/step
Epoch 35/200
15048/15048 - 59s - loss: 0.3587 - val_loss: 0.5342 - 59s/epoch - 4ms/step
Epoch 36/200
15048/15048 - 53s - loss: 0.3558 - val_loss: 0.5126 - 53s/epoch - 4ms/step
Epoch 37/200
15048/15048 - 62s - loss: 0.3538 - val_loss: 0.5244 - 62s/epoch - 4ms/step
Epoch 38/200
15048/15048 - 53s - loss: 0.3521 - val_loss: 0.5083 - 53s/epoch - 4ms/step
Epoch 39/200
15048/15048 - 53s - loss: 0.3485 - val_loss: 0.5159 - 53s/epoch - 4ms/step
Epoch 40/200
15048/15048 - 53s - loss: 0.3468 - val_loss: 0.5087 - 53s/epoch - 4ms/step
Epoch 41/200
15048/15048 - 61s - loss: 0.3447 - val_loss: 0.4996 - 61s/epoch - 4ms/step
Epoch 42/200
15048/15048 - 53s - loss: 0.3427 - val_loss: 0.4974 - 53s/epoch - 4ms/step
Epoch 43/200
15048/15048 - 57s - loss: 0.3403 - val_loss: 0.5043 - 57s/epoch - 4ms/step
Epoch 44/200
15048/15048 - 60s - loss: 0.3372 - val_loss: 0.4987 - 60s/epoch - 4ms/step
Epoch 45/200
15048/15048 - 53s - loss: 0.3363 - val_loss: 0.5009 - 53s/epoch - 4ms/step
Epoch 46/200
15048/15048 - 53s - loss: 0.3345 - val_loss: 0.5034 - 53s/epoch - 4ms/step
Epoch 47/200
15048/15048 - 53s - loss: 0.3316 - val_loss: 0.4934 - 53s/epoch - 4ms/step
Epoch 48/200
15048/15048 - 56s - loss: 0.3309 - val_loss: 0.4963 - 56s/epoch - 4ms/step
Epoch 49/200
15048/15048 - 65s - loss: 0.3303 - val_loss: 0.4937 - 65s/epoch - 4ms/step
Epoch 50/200
15048/15048 - 56s - loss: 0.3278 - val_loss: 0.4944 - 56s/epoch - 4ms/step
Epoch 51/200
15048/15048 - 53s - loss: 0.3253 - val_loss: 0.4896 - 53s/epoch - 4ms/step
Epoch 52/200
15048/15048 - 53s - loss: 0.3238 - val_loss: 0.5022 - 53s/epoch - 4ms/step
Epoch 53/200
15048/15048 - 53s - loss: 0.3211 - val_loss: 0.4843 - 53s/epoch - 4ms/step
Epoch 54/200
15048/15048 - 53s - loss: 0.3210 - val_loss: 0.4801 - 53s/epoch - 4ms/step
Epoch 55/200
15048/15048 - 53s - loss: 0.3185 - val_loss: 0.4695 - 53s/epoch - 4ms/step
Epoch 56/200
15048/15048 - 53s - loss: 0.3164 - val_loss: 0.4655 - 53s/epoch - 4ms/step
Epoch 57/200
15048/15048 - 57s - loss: 0.3153 - val_loss: 0.4712 - 57s/epoch - 4ms/step
Epoch 58/200
15048/15048 - 53s - loss: 0.3126 - val_loss: 0.4908 - 53s/epoch - 4ms/step
Epoch 59/200
15048/15048 - 55s - loss: 0.3111 - val_loss: 0.4844 - 55s/epoch - 4ms/step
Epoch 60/200
15048/15048 - 53s - loss: 0.3099 - val_loss: 0.4666 - 53s/epoch - 4ms/step
Epoch 61/200
15048/15048 - 53s - loss: 0.3098 - val_loss: 0.4660 - 53s/epoch - 4ms/step

----Training ended at 2023-01-21 23:57:55.575774, elapsed time 1:06:16.252827.
Best model by validation loss saved at scratch/results_LSTM/model.h5.
Loading best model.

Train prediction
15048/15048 - 29s - 29s/epoch - 2ms/step
               FIPS         Cases  Predicted_Cases
count  1.963750e+06  1.963750e+06     1.963750e+06
mean   3.038365e+04  2.314343e+01     2.202668e+01
std    1.516010e+04  1.325581e+02     9.256154e+01
min    1.001000e+03  0.000000e+00     0.000000e+00
25%    1.817700e+04  0.000000e+00     1.000000e+00
50%    2.917600e+04  2.000000e+00     4.000000e+00
75%    4.508100e+04  1.100000e+01     1.200000e+01
max    5.604500e+04  2.061825e+04     4.497000e+03
Target Cases, MAE 10.647, RMSE 72.357, RMSLE 0.99345, SMAPE 0.96093. NNSE 0.77045.


Validation prediction
25/25 - 0s - 56ms/epoch - 2ms/step
               FIPS         Cases  Predicted_Cases
count  40846.000000  40846.000000     40846.000000
mean   30383.649268     35.844495        27.481271
std    15160.280886    149.521039        94.281355
min     1001.000000      0.000000         0.000000
25%    18177.000000      0.000000         3.000000
50%    29176.000000      2.000000         7.000000
75%    45081.000000     20.000000        17.000000
max    56045.000000   8204.000000      3596.000000
Target Cases, MAE 21.057, RMSE 92.026, RMSLE 1.3241, SMAPE 1.0889. NNSE 0.72526.


Test prediction
25/25 - 0s - 56ms/epoch - 2ms/step
               FIPS         Cases  Predicted_Cases
count  40846.000000  40846.000000     40846.000000
mean   30383.649268     60.815104        39.472458
std    15160.280886    338.594555       120.817969
min     1001.000000      0.000000         0.000000
25%    18177.000000      0.000000         3.000000
50%    29176.000000      2.000000         9.000000
75%    45081.000000     20.000000        25.000000
max    56045.000000  20618.250000      3635.000000
Target Cases, MAE 40.273, RMSE 267.05, RMSLE 1.4337, SMAPE 1.0537. NNSE 0.61649.

Ended at 2023-01-22 00:21:05.919826. Elapsed time 1:29:26.596903
