{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors and Geoffrey Fox 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "EV1qWhrmI1nF",
    "outputId": "7211154c-731f-4f25-f077-09fcd89f7b16"
   },
   "outputs": [],
   "source": [
    "# Set Runname\n",
    "# RunName = 'CovidA21-TFTTestA'  # Use to access old fit\n",
    "RunName = 'CovidA21-TFT2Extended-JulyCutoff-TFTJupyter'\n",
    "RunComment = ''' \n",
    "              This is the TFT model fit for group number 2 in the rurality based stratification. The data is from beginning until July 2021.\n",
    "              Feature importance is included in this modeling.\n",
    "              '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "startbold = \"\\033[1m\"\n",
    "resetfonts = \"\\033[0m\"\n",
    "startred = '\\033[31m'\n",
    "\n",
    "startpurple = '\\033[35m'\n",
    "startyellowbkg = '\\033[43m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPlUGy2MPRPe"
   },
   "source": [
    "### VERY IMPORTANT LINE -> Set to 1 if TRAINING and 0 if VISUALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "g47lAq4IPRs_",
    "outputId": "d83204db-0071-4a58-d893-00510fb9cd18"
   },
   "outputs": [],
   "source": [
    "TFTMode = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWNb05uZ7V9I"
   },
   "source": [
    "# Initial System Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Transformer model for science data based on original for language understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/transformer\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/transformer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA3Lx2aY1xeg"
   },
   "source": [
    "## Science Data Parameters and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMY9LokXwa9K"
   },
   "source": [
    "-------\n",
    "Here is structure of science time series module. We will need several arrays that will need to be flattened at times. Note Python defaults to row major i.e. final index describes contiguous positions in memory\n",
    "\n",
    "\n",
    "At highest level data is labeled by Time and Location\n",
    "\n",
    "*   Ttot is total number of time steps\n",
    "*   Tseq is length of each sequence in time steps\n",
    "*   Num_Seq is number of sequences in time: Num_Seq = Ttot-Tseq + 1\n",
    "*   Nloc is Number of locations. The locations could be a 1D list or have array structure such as an image.\n",
    "*   Nsample is number of data samples Nloc * Num_Seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Input data is at each location\n",
    "*   Nprop time independent properties describing the location\n",
    "*   Nforcing is number of time dependent forcing features INPUT at each time value\n",
    "\n",
    "\n",
    "Output (predicted) data at each location and for each time sequence is\n",
    "*   Npred predicted time dependent values defined at every time step\n",
    "*   Recorded at Nforecast time values measured wrt final time value of sequence\n",
    "*   ForecastDay is an array of length Nforecast defining how many days into future prediction is. Typically ForecastDay[0] = 1 and Nforecast is often 1\n",
    "*   There is also a class of science problems that are more similar to classic Seq2Seq. Here Nforecast = Tseq and ForecastDay = [-Tseq+1 ... 0]\n",
    "*   We also support Nwishful predictions of events in future such probability of an earthquake of magnitude 6 in next 3 years. These are defined by araays EventType and Timestart, TimeInterval of length Nwishful. EventType is user defined and Timestart, TimeInterval is measured in time steps\n",
    "*   Any missing output values should be set to NaN and Loss function must ensure that these points are ignored in derivative calculation and value calculation\n",
    "\n",
    "We have an input module that supports either LSTM or Transformer (multi-head attention) models\n",
    "\n",
    "Example Problem AICov\n",
    "\n",
    "*   Ttot = 114\n",
    "*   Tseq = 9\n",
    "*   Num_Seq = 106\n",
    "*   Nloc = 110\n",
    "\n",
    "\n",
    "*   Nprop = 35\n",
    "*   Nforcing = 5 including infections, fatalities, plus 3 temporal position variables (last 3 not in current version)\n",
    " \n",
    " \n",
    "*   Npred = 2 (predicted infections and fatalities). Could be 5 if predicted temporal position of output)\n",
    "*   Nforecast= 15\n",
    "*   ForecastDay = [1, 2, .......14, 15]\n",
    "*   Nwishful = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UlOJMJ31SoG"
   },
   "source": [
    "## Science Data Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdszPs9on5gk"
   },
   "source": [
    "Typical Arrays\n",
    "\n",
    "\n",
    "[ time, Location ] as Pandas array with label [name of time-dependent variable] as an array or just name of Pandas array\n",
    "\n",
    "time labels rows indexed by datetime or the difference datetime - start\n",
    "\n",
    "Non windowed data is stored with propert name as row index and location as column index\n",
    "[ static property, Location]\n",
    "\n",
    "Covid Input is\n",
    "[Sequence number 0..Num_Seq-1 ] [ Location 0..Nloc-1 ] [position in time sequence Tseq]  [ Input Features]\n",
    "\n",
    "Covid Output is \n",
    "[Sequence number Num_Seq ] [ Location Nloc ]  [ Output Features] \n",
    "\n",
    "Output Features are [ ipred = 0 ..Npred-1 ] [ iforecast = 0 ..Nforecast-1 ]\n",
    "\n",
    "Input Features are static fields followed by if present by dynamic system fields (cos-theta sin-theta linear) chosen followed by cases, deaths. In fact this is user chosen as they set static and dynamic system properties to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-iizX9OKmI3"
   },
   "source": [
    "We will have various numpy and pandas arrays where we designate label\n",
    "\n",
    "[Ttot] is all time values \n",
    "\n",
    "[Num_Seq]  is all sequences of window size ***Tseq***\n",
    "\n",
    "We can select time values or sequences [Ttot-reason] [Num_Seq-reason] for a given \"reason\"\n",
    "\n",
    "[Num_Seq][Tseq] is all time values in all sequences\n",
    "\n",
    "[Nloc] is all locations while [Nloc-reason] is subset of locations for given \"reason\"\n",
    "\n",
    "[Model1] is initial embedding of each data point\n",
    "\n",
    "[Model1+TrPosEnc] is initial embedding of each data point with Transformer style positional encoding\n",
    "\n",
    "[Nforcing] is time dependent input parameters and [Nprop] static properties while [ExPosEnc] are explicit positional (temporal) encoding.\n",
    "\n",
    "[Nforcing+ExPosEnc+Nprop] are all possible inputs\n",
    "\n",
    "[Npred] is predicted values with [Npred+ExPosEnc] as predictions plus encodings with actually used [Predvals] = [Npred+ExPosEnc-Selout] \n",
    "\n",
    "[Predtimes] = [Forecast time range] are times forecasted with \"time range\" separately defined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "vQND--lq9dCp",
    "outputId": "06bf4086-17f7-47df-97a6-c07a8058fc1e"
   },
   "outputs": [],
   "source": [
    "from numpy.core.numeric import True_\n",
    "AgeDist = (True, ['Age Distribution.csv'])\n",
    "AirPollution = (True, ['Air Pollution.csv'])\n",
    "Comorbidites = (True, ['Comorbidities.csv'])\n",
    "Demographics = (False , ['Demographics.csv'])\n",
    "HealthDisp = (True, ['Health Disparities.csv'])\n",
    "HospitalBed = (False, ['Hospital Beds.csv'])\n",
    "Mobility = (True, ['Mobility.csv'])\n",
    "ResidentialDense = (True , ['Residential Density.csv'])\n",
    "Voting = (False, ['2020votes.csv', 'Alaskavoting2016.csv'])\n",
    "DiseaseSpread = (True , ['Disease Spread.csv'])\n",
    "SocialDist = (True, ['Social Distancing.csv'])\n",
    "Testing = (True, ['Testing.csv'])\n",
    "Transmission = (True, ['Transmissible Cases.csv'])\n",
    "VaccinationOne = (True, ['VaccinationOneDose.csv'])\n",
    "VaccinationFull = (True, ['Vaccination.csv'])\n",
    "\n",
    "Features = [AgeDist, AirPollution, Comorbidites, Demographics, HealthDisp,\n",
    "            HospitalBed, Mobility, ResidentialDense, Voting, DiseaseSpread,\n",
    "            SocialDist, Testing, Transmission, VaccinationOne, VaccinationFull]\n",
    "\n",
    "\n",
    "FeatureFiles = [i[1] if i[0] else None for i in Features]\n",
    "FeatureFiles = list(filter(None.__ne__, FeatureFiles))\n",
    "FeatureFiles = [i[0] if i[0] else None for i in FeatureFiles]\n",
    "\n",
    "UseVoting = True if '2020votes.csv' in FeatureFiles else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "xLhiMqeceFcZ",
    "outputId": "46baa377-e450-463f-fe06-fb168b9b43bd"
   },
   "outputs": [],
   "source": [
    "FeatLen = len(FeatureFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "JjJJyJTZYebt",
    "outputId": "8be06dd1-36fe-484e-9d7b-7ab7e9a020d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudmesh-common in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (4.3.83)\n",
      "Requirement already satisfied: python-hostlist in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (1.21)\n",
      "Requirement already satisfied: pytz in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (2021.3)\n",
      "Requirement already satisfied: psutil in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (5.9.0)\n",
      "Requirement already satisfied: pyfiglet in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (0.8.post1)\n",
      "Requirement already satisfied: colorama in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (0.4.4)\n",
      "Requirement already satisfied: simplejson in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (3.17.6)\n",
      "Requirement already satisfied: oyaml in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (1.0)\n",
      "Requirement already satisfied: tqdm in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (4.62.3)\n",
      "Requirement already satisfied: tabulate in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (0.8.9)\n",
      "Requirement already satisfied: humanize in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (3.14.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (2.8.2)\n",
      "Requirement already satisfied: requests in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from cloudmesh-common) (2.27.1)\n",
      "Requirement already satisfied: pyyaml in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from oyaml->cloudmesh-common) (6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from python-dateutil->cloudmesh-common) (1.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from requests->cloudmesh-common) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from requests->cloudmesh-common) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from requests->cloudmesh-common) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages (from requests->cloudmesh-common) (1.26.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tqdm import tnrange, notebook, tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from csv import reader\n",
    "from csv import writer\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "import pandas as pd\n",
    "import io as io\n",
    "import string\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta,date,datetime\n",
    "\n",
    "!pip install cloudmesh-common -U\n",
    "from cloudmesh.common.StopWatch import StopWatch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdH4W3OJTLyj"
   },
   "source": [
    "### Define Basic Control parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "mZAL5yNsC_UK",
    "outputId": "9d8cf2cc-bd26-4336-9d6e-a3dec1fd91d1"
   },
   "outputs": [],
   "source": [
    "def wraptotext(textinput,size=None):\n",
    "  if size is None:\n",
    "    size = 120\n",
    "  textlist = wrap(textinput,size)\n",
    "  textresult = textlist[0]\n",
    "  for itext in range(1,len(textlist)):\n",
    "    textresult += '\\n'+textlist[itext]\n",
    "  return textresult\n",
    "\n",
    "def timenow():\n",
    "  now = datetime.now()\n",
    "  return now.strftime(\"%m/%d/%Y, %H:%M:%S\") + \" UTC\"\n",
    "\n",
    "def float32fromstrwithNaN(instr):\n",
    "  if instr == 'NaN':\n",
    "    return NaN\n",
    "  return np.float32(instr)\n",
    "\n",
    "def printexit(exitmessage):\n",
    "  print(exitmessage)\n",
    "  sys.exit()\n",
    "\n",
    "def strrnd(value):\n",
    "  return str(round(value,4))\n",
    "\n",
    "NaN = np.float32(\"NaN\")\n",
    "\n",
    "ReadJuly2020Covid = False\n",
    "ReadAugust2020Covid = False\n",
    "ReadJan2021Covid = False\n",
    "ReadApril2021Covid = False\n",
    "ScaleProperties = False\n",
    "ConvertDynamicPredictedQuantity = False\n",
    "ConvertDynamicProperties = True\n",
    "GenerateFutures = False\n",
    "GenerateSequences = False\n",
    "PredictionsfromInputs = False\n",
    "RereadMay2020 = False\n",
    "UseOLDCovariates = False\n",
    "Dropearlydata = 0\n",
    "NIHCovariates = False \n",
    "UseFutures = True\n",
    "Usedaystart = False \n",
    "PopulationNorm = False\n",
    "SymbolicWindows = False\n",
    "Hydrology = False\n",
    "Earthquake = False\n",
    "EarthquakeImagePlots = False\n",
    "AddSpecialstoSummedplots = False\n",
    "UseRealDatesonplots = False\n",
    "Dumpoutkeyplotsaspics = False\n",
    "OutputNetworkPictures = False\n",
    "CDSpecial = False\n",
    "NumpredbasicperTime = 2\n",
    "NumpredFuturedperTime = 2\n",
    "NumTimeSeriesCalculated = 0\n",
    "Dailyunit = 1\n",
    "TimeIntervalUnitName = 'Day'\n",
    "InitialDate = datetime(2000,1,1)\n",
    "NumberofTimeunits = 0\n",
    "Num_Time =0\n",
    "FinalDate = datetime(2000,1,1)\n",
    "GlobalTrainingLoss = 0.0\n",
    "GlobalValidationLoss = 0.0\n",
    "\n",
    "# Type of Testing\n",
    "LocationBasedValidation = False\n",
    "LocationValidationFraction = 0.0\n",
    "LocationTrainingfraction = 1.0\n",
    "RestartLocationBasedValidation = False\n",
    "\n",
    "global SeparateValandTrainingPlots\n",
    "SeparateValandTrainingPlots = True\n",
    "Plotsplitsize = -1 # if > 1 split time in plots\n",
    "\n",
    "GarbageCollect = True\n",
    "GarbageCollectionLimit = 0\n",
    "\n",
    "current_time = timenow()\n",
    "\n",
    "SubName = RunName[0:6]\n",
    "\n",
    "\n",
    "\n",
    "if SubName == 'BEST14' or SubName == 'BEST15' or SubName == 'BEST16':\n",
    "  UseOLDCovariates = False\n",
    "  ReadAugust2020Covid = True\n",
    "  ScaleProperties = True\n",
    "  ConvertDynamicPredictedQuantity = True\n",
    "  GenerateFutures = True\n",
    "  GenerateSequences = True\n",
    "  PredictionsfromInputs = True\n",
    "  NIHCovariates = True\n",
    "  ConvertDynamicProperties = True\n",
    "  Dropearlydata = 37\n",
    "  CDSpecial = True\n",
    "\n",
    "if SubName == 'CovidA':\n",
    "  UseOLDCovariates = False\n",
    "  if 'Extended' in RunName[6:]:\n",
    "    ReadDecember2021 = True\n",
    "    ReadApril2021Covid = False\n",
    "  else:\n",
    "    ReadApril2021Covid = True\n",
    "    ReadDecember2021 = False\n",
    "  ScaleProperties = True\n",
    "  ConvertDynamicPredictedQuantity = True\n",
    "  GenerateFutures = True\n",
    "  UseFutures = True\n",
    "  GenerateSequences = True\n",
    "  PredictionsfromInputs = True\n",
    "  NIHCovariates = True\n",
    "  ConvertDynamicProperties = True\n",
    "  CDSpecial = True\n",
    "\n",
    "if SubName == 'C2021A' or SubName == 'C2021B':\n",
    "  UseOLDCovariates = False\n",
    "  ReadJan2021Covid = True\n",
    "  ScaleProperties = True\n",
    "  ConvertDynamicPredictedQuantity = True\n",
    "  GenerateFutures = True\n",
    "  GenerateSequences = True\n",
    "  PredictionsfromInputs = True\n",
    "  NIHCovariates = True\n",
    "  ConvertDynamicProperties = True\n",
    "  Dropearlydata = 0\n",
    "  CDSpecial = True\n",
    "\n",
    "if SubName == 'Hydrol':\n",
    "  Hydrology = True\n",
    "\n",
    "if SubName == 'EARTHQ':\n",
    "  Earthquake = True\n",
    "\n",
    "if RunName == 'BEST10' or RunName == 'BEST13-10D' or RunName == 'BEST12-10' or RunName == 'BEST12-Test' or RunName == 'BEST13' or RunName == 'BEST13-10' or RunName == 'BEST13-10A' or RunName == 'BEST13-10C':\n",
    "  UseOLDCovariates = False\n",
    "  ReadAugust2020Covid = True\n",
    "  ScaleProperties = True\n",
    "  ConvertDynamicPredictedQuantity = True\n",
    "  GenerateFutures = True\n",
    "  GenerateSequences = True\n",
    "  PredictionsfromInputs = True\n",
    "  CDSpecial = True\n",
    "\n",
    "if RunName == 'BEST11' or RunName == 'BEST11A':\n",
    "  UseOLDCovariates = True\n",
    "  ReadAugust2020Covid = True\n",
    "  ScaleProperties = True\n",
    "  ConvertDynamicPredictedQuantity = True\n",
    "  GenerateFutures = True\n",
    "  GenerateSequences = True\n",
    "  PredictionsfromInputs = True\n",
    "  CDSpecial = True\n",
    "\n",
    "if RunName == 'BEST12':\n",
    "  UseOLDCovariates = True\n",
    "  RereadMay2020 = True\n",
    "  ReadAugust2020Covid = False\n",
    "  ScaleProperties = True\n",
    "  ConvertDynamicPredictedQuantity = True\n",
    "  GenerateFutures = True\n",
    "  GenerateSequences = True\n",
    "  PredictionsfromInputs = True\n",
    "  CDSpecial = True\n",
    "\n",
    "if RunName == 'BEST8' or RunName == 'BEST8A' or RunName == 'BEST12-LSTM-8':\n",
    "  ReadJuly2020Covid = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DefDaYecDhIM"
   },
   "source": [
    "## Define input structure\n",
    "\n",
    "Read in data and set it up for Tensorflow with training and validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kj1DvDTneDZ"
   },
   "source": [
    "Set train_examples, val_examples as science training and validatioon set.\n",
    "\n",
    "The shuffling of Science Data needs some care. We have ***Tseq*** * size of {[Num_Seq][Nloc]} locations in each sample. In simplease case the last is just a decomposition over location; not over time. Let's Nloc-sel be number of locations per sample. It will be helpful if Nloc-sel is divisable by 2. \n",
    "\n",
    "Perhaps Nloc-sel = 2 6 or 10 is reasonable.\n",
    "\n",
    "Then you shuffle locations every epoch and divide them into groups of size Nloc-sel with 50% overlap so you get locations\n",
    "\n",
    "0 1 2 3 4 5; \n",
    "\n",
    "3 4 5 6 7 8; \n",
    "\n",
    "6 7 8 9 10 11 etc.\n",
    "\n",
    "Every locations appears twice in an epoch (for each time value). You need to randomly add locations at end of sequence so it is divisiuble by Nloc-sel e.g add 4 random positions to the end if Nloc=110 and Nloc-sel = 6. Note last group of 6 has members 112 113 114 0 1 2\n",
    "\n",
    "After spatial structure set up, randomly shuffle in Num_Seq where there is an argument to do all locations for a partcular time value together.\n",
    "\n",
    "For validation, it is probably best to select validation location before chopping them into groups of size Nloc-sel\n",
    "\n",
    "How one groups locations for inference is not clear. One idea is to take trained network and use it to find for each location which other locations have the most attention with it. Use those locations in  prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKm_MgRMdcTT"
   },
   "source": [
    "More general input. \n",
    "NaN allowed value\n",
    "\n",
    "* Number time values\n",
    "* Number locations\n",
    "* Number driving values\n",
    "* Number predicted values\n",
    "\n",
    "For COVID driving same as predicted\n",
    "\n",
    "* a) Clean up >=0 daily\n",
    "* b) Normalize\n",
    "* c) Add Futures\n",
    "* d) Add time/location encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KJIxYoMDZOu"
   },
   "source": [
    "### Setup File Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ShbYhXJbKCDT",
    "outputId": "dd3ce5a1-9af9-4d43-e19c-79193a790b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint set up in directory ./independent_study2/COVIDJuly2020/checkpoints/CovidA21-TFT2Extended-JulyCutoff-TFTJupyterdir/\n"
     ]
    }
   ],
   "source": [
    "# read in science data \n",
    "COLABROOTDIR=\"./independent_study2\"\n",
    "os.environ[\"COLABROOTDIR\"] = COLABROOTDIR\n",
    "\n",
    "if Hydrology:\n",
    "  APPLDIR=os.path.join(COLABROOTDIR, \"Hydrology\")\n",
    "elif Earthquake:\n",
    "  APPLDIR=os.path.join(COLABROOTDIR, \"EarthquakeDec2020\")\n",
    "else:\n",
    "  APPLDIR=os.path.join(COLABROOTDIR, \"COVIDJuly2020\")\n",
    "\n",
    "# Set up Checkpoints\n",
    "CHECKPOINTDIR = APPLDIR + \"/checkpoints/\" + RunName + \"dir/\"\n",
    "try: \n",
    "    if not os.path.exists(CHECKPOINTDIR):\n",
    "      os.mkdir(CHECKPOINTDIR) \n",
    "except OSError as error: \n",
    "    print(error)\n",
    "print('Checkpoint set up in directory ' + CHECKPOINTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d_JaiVAV5yk"
   },
   "source": [
    "### Read April 2021 Covid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "uiClFXFWWETE",
    "outputId": "ee095ae8-a6e0-4b07-c870-c1593e17f699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days December 2021 Dataset 488 dropping at start 0\n",
      "Read Cases data locations 3142 Time Steps 488\n",
      "Read Deaths data locations 3142 Time Steps 488\n",
      "548 Missing Votes 15005 Kalawao County Hawaii pop 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/3483247979.py:171: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n"
     ]
    }
   ],
   "source": [
    "if ReadDecember2021:\n",
    "  Dropearlydata = 0 # 3 more than needed by covariates so as to get \"round number of days\"\n",
    "  NIHCovariates = True\n",
    "  UseOLDCovariates = False\n",
    "  LengthFutures = 0\n",
    "\n",
    "  InitialDate = datetime(2020,2,29) + timedelta(days=Dropearlydata)\n",
    "  FinalDate = datetime(2021,6,30)\n",
    "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "  print(\"Total number of days December 2021 Dataset \" + str(NumberofTimeunits) + ' dropping at start ' + str(Dropearlydata))\n",
    "\n",
    "  DATASETDIR = APPLDIR + '/CovidDecember12-2021'\n",
    "\n",
    "  CasesFile = DATASETDIR + '/US_daily_cumulative_cases_Dec12.csv'\n",
    "  DeathsFile = DATASETDIR + '/US_daily_cumulative_deaths_Dec12.csv'\n",
    "  LocationdataFile = DATASETDIR + '/Population.csv'\n",
    "  LocationruralityFile = DATASETDIR + '/Rurality.csv'\n",
    "  VotingdataFile = DATASETDIR + '/2020votes.csv'\n",
    "  AlaskaVotingdataFile = DATASETDIR + '/Alaskavoting2016.csv'\n",
    "\n",
    "  Nloc = 3142\n",
    "  NFIPS = 3142\n",
    "\n",
    "# Set up location information\n",
    "  Num_Time = NumberofTimeunits\n",
    "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs\n",
    "  Locationcolumns = [] # String version of FIPS\n",
    "  FIPSintegerlookup = {}\n",
    "  FIPSstringlookup = {}\n",
    "  BasicInputTimeSeries = np.empty([Num_Time,Nloc,2],dtype = np.float32)\n",
    "\n",
    "# Read in  cases Data into BasicInputTimeSeries\n",
    "  with open(CasesFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type Cases ' + Ftype)\n",
    "\n",
    "      iloc = 0    \n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)< NumberofTimeunits + 1 + Dropearlydata):\n",
    "          printexit('EXIT: Incorrect row length Cases ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "        # skip first entry\n",
    "        localfips = nextrow[0]\n",
    "        Locationcolumns.append(localfips)\n",
    "        Locationfips[iloc] = int(localfips)\n",
    "        FIPSintegerlookup[int(localfips)] = iloc\n",
    "        FIPSstringlookup[localfips] = iloc\n",
    "        for itime in range(0, NumberofTimeunits):\n",
    "          BasicInputTimeSeries[itime,iloc,0] = nextrow[itime + 1 + Dropearlydata]\n",
    "          if Dropearlydata > 0:\n",
    "            floatlast = np.float(nextrow[Dropearlydata])\n",
    "            BasicInputTimeSeries[itime,iloc,0] = BasicInputTimeSeries[itime,iloc,0] - floatlast\n",
    "        iloc += 1\n",
    "# End Reading in cases data\n",
    "\n",
    "  if iloc != Nloc:\n",
    "          printexit('EXIT Inconsistent location lengths Cases ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Cases data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "\n",
    "# Read in deaths Data into BasicInputTimeSeries\n",
    "  with open(DeathsFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type Deaths ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)<NumberofTimeunits + 1 + Dropearlydata):\n",
    "          printexit('EXIT: Incorrect row length Deaths ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "        localfips = nextrow[0]\n",
    "        if (Locationfips[iloc] != int(localfips)):\n",
    "          printexit('EXIT: Unexpected FIPS Deaths ' + localfips + ' ' +str(Locationfips[iloc]))\n",
    "        for itime in range(0, NumberofTimeunits):\n",
    "          BasicInputTimeSeries[itime,iloc,1] = nextrow[itime + 1 + Dropearlydata]\n",
    "          if Dropearlydata > 0:\n",
    "            floatlast = np.float(nextrow[Dropearlydata])\n",
    "            BasicInputTimeSeries[itime,iloc,1] = BasicInputTimeSeries[itime,iloc,1] - floatlast\n",
    "        iloc += 1\n",
    "# End Reading in deaths data\n",
    "\n",
    "  if iloc != Nloc:\n",
    "    printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Deaths data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "\n",
    "  Locationname = ['Empty'] * NFIPS\n",
    "  Locationstate = ['Empty'] * NFIPS\n",
    "  Locationpopulation = np.empty(NFIPS, dtype=int)\n",
    "  with open(LocationdataFile, 'r', encoding='latin1') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type Prop Data ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        localfips = int(nextrow[0])\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          Locationname[jloc] = nextrow[4]\n",
    "          Locationstate[jloc] = nextrow[3]\n",
    "          Locationpopulation[jloc] = int(nextrow[2])\n",
    "          iloc += 1 # just counting lines  \n",
    "        else:\n",
    "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))  \n",
    "# END setting NFIPS location properties\n",
    "\n",
    "  DemVoting = np.full(NFIPS, -1.0, dtype=np.float32)\n",
    "  RepVoting = np.full(NFIPS, -1.0, dtype=np.float32)\n",
    "  with open(VotingdataFile, 'r', encoding='latin1') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'state_name':\n",
    "        printexit('EXIT: Wrong file type Voting Data ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        localfips = int(nextrow[1])\n",
    "        if localfips > 2900 and localfips < 2941: # Alaska not useful\n",
    "          continue\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          if DemVoting[jloc] >= 0.0:\n",
    "             printexit('EXIT Double Setting of FIPS ' +str(iloc) + ' ' + str(localfips))\n",
    "          DemVoting[jloc] = nextrow[8]\n",
    "          RepVoting[jloc] = nextrow[7]\n",
    "          iloc += 1 # just counting lines  \n",
    "        else:\n",
    "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))  \n",
    "\n",
    "  with open(AlaskaVotingdataFile, 'r',encoding='utf-8-sig') as read_obj: # remove ufeff\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'SpecialAlaska':\n",
    "        printexit('EXIT: Wrong file type Alaska Voting Data ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        localfips = int(nextrow[1])\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          if DemVoting[jloc] >= 0.0:\n",
    "             printexit('EXIT Double Setting of FIPS ' +str(iloc) + ' ' + str(localfips))\n",
    "          DemVoting[jloc] = float(nextrow[2]) * 42.77/36.5\n",
    "          RepVoting[jloc] = float(nextrow[3]) * 52.83/51.3\n",
    "          iloc += 1 # just counting lines  \n",
    "        else:\n",
    "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))\n",
    "\n",
    "  for iloc in range(0,NFIPS):\n",
    "    if DemVoting[iloc] >= 0.0:\n",
    "      continue\n",
    "    print(str(iloc) + ' Missing Votes ' + str(Locationfips[iloc]) + ' ' + Locationname[iloc] + ' ' + Locationstate[iloc] + ' pop ' + str( Locationpopulation[iloc]))\n",
    "    DemVoting[iloc] = 0.5\n",
    "    RepVoting[iloc] = 0.5\n",
    "\n",
    "# Set Static Properties of the Nloc studied locations\n",
    "# Order is Static, Dynamic, Cases, Deaths\n",
    "# Voting added as 13th covariate\n",
    "  NpropperTimeDynamic = FeatLen\n",
    "  NpropperTimeStatic = 0\n",
    "\n",
    "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic + 2   \n",
    "  InputPropertyNames = [] * NpropperTime\n",
    "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqwp8FVQC8FD"
   },
   "source": [
    "### Cut by Median Rurality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1423
    },
    "id": "XzcpwRRXLbc7",
    "outputId": "973280bb-7c60-4a39-a7c3-0525428d4841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIPS not in lookup table 92 2270\n",
      "FIPS not in lookup table 2416 46113\n",
      "FIPS not in lookup table 2914 51515\n",
      "FIPS not in lookup table 3140 72001\n",
      "FIPS not in lookup table 3140 72003\n",
      "FIPS not in lookup table 3140 72005\n",
      "FIPS not in lookup table 3140 72007\n",
      "FIPS not in lookup table 3140 72009\n",
      "FIPS not in lookup table 3140 72011\n",
      "FIPS not in lookup table 3140 72013\n",
      "FIPS not in lookup table 3140 72015\n",
      "FIPS not in lookup table 3140 72017\n",
      "FIPS not in lookup table 3140 72019\n",
      "FIPS not in lookup table 3140 72021\n",
      "FIPS not in lookup table 3140 72023\n",
      "FIPS not in lookup table 3140 72025\n",
      "FIPS not in lookup table 3140 72027\n",
      "FIPS not in lookup table 3140 72029\n",
      "FIPS not in lookup table 3140 72031\n",
      "FIPS not in lookup table 3140 72033\n",
      "FIPS not in lookup table 3140 72035\n",
      "FIPS not in lookup table 3140 72037\n",
      "FIPS not in lookup table 3140 72039\n",
      "FIPS not in lookup table 3140 72041\n",
      "FIPS not in lookup table 3140 72043\n",
      "FIPS not in lookup table 3140 72045\n",
      "FIPS not in lookup table 3140 72047\n",
      "FIPS not in lookup table 3140 72049\n",
      "FIPS not in lookup table 3140 72051\n",
      "FIPS not in lookup table 3140 72053\n",
      "FIPS not in lookup table 3140 72054\n",
      "FIPS not in lookup table 3140 72055\n",
      "FIPS not in lookup table 3140 72057\n",
      "FIPS not in lookup table 3140 72059\n",
      "FIPS not in lookup table 3140 72061\n",
      "FIPS not in lookup table 3140 72063\n",
      "FIPS not in lookup table 3140 72065\n",
      "FIPS not in lookup table 3140 72067\n",
      "FIPS not in lookup table 3140 72069\n",
      "FIPS not in lookup table 3140 72071\n",
      "FIPS not in lookup table 3140 72073\n",
      "FIPS not in lookup table 3140 72075\n",
      "FIPS not in lookup table 3140 72077\n",
      "FIPS not in lookup table 3140 72079\n",
      "FIPS not in lookup table 3140 72081\n",
      "FIPS not in lookup table 3140 72083\n",
      "FIPS not in lookup table 3140 72085\n",
      "FIPS not in lookup table 3140 72087\n",
      "FIPS not in lookup table 3140 72089\n",
      "FIPS not in lookup table 3140 72091\n",
      "FIPS not in lookup table 3140 72093\n",
      "FIPS not in lookup table 3140 72095\n",
      "FIPS not in lookup table 3140 72097\n",
      "FIPS not in lookup table 3140 72099\n",
      "FIPS not in lookup table 3140 72101\n",
      "FIPS not in lookup table 3140 72103\n",
      "FIPS not in lookup table 3140 72105\n",
      "FIPS not in lookup table 3140 72107\n",
      "FIPS not in lookup table 3140 72109\n",
      "FIPS not in lookup table 3140 72111\n",
      "FIPS not in lookup table 3140 72113\n",
      "FIPS not in lookup table 3140 72115\n",
      "FIPS not in lookup table 3140 72117\n",
      "FIPS not in lookup table 3140 72119\n",
      "FIPS not in lookup table 3140 72121\n",
      "FIPS not in lookup table 3140 72123\n",
      "FIPS not in lookup table 3140 72125\n",
      "FIPS not in lookup table 3140 72127\n",
      "FIPS not in lookup table 3140 72129\n",
      "FIPS not in lookup table 3140 72131\n",
      "FIPS not in lookup table 3140 72133\n",
      "FIPS not in lookup table 3140 72135\n",
      "FIPS not in lookup table 3140 72137\n",
      "FIPS not in lookup table 3140 72139\n",
      "FIPS not in lookup table 3140 72141\n",
      "FIPS not in lookup table 3140 72143\n",
      "FIPS not in lookup table 3140 72145\n",
      "FIPS not in lookup table 3140 72147\n",
      "FIPS not in lookup table 3140 72149\n",
      "FIPS not in lookup table 3140 72151\n",
      "FIPS not in lookup table 3140 72153\n"
     ]
    }
   ],
   "source": [
    "RuralityFile = DATASETDIR + '/' + 'Rurality.csv'\n",
    "RuralityRange = [4.501,5]\n",
    "RuralityCut=True\n",
    "\n",
    "Locationrurality = np.empty(NFIPS)\n",
    "Locationmad = np.empty(NFIPS)\n",
    "\n",
    "skips = []\n",
    "\n",
    "with open(RuralityFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type Prop Data ' + Ftype)\n",
    "      \n",
    "      iloc = 0\n",
    "      for row in csv_reader:\n",
    "        localfips = int(row[0])\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          Locationrurality[jloc] = row[1]\n",
    "          Locationmad[jloc] = row[2]\n",
    "          iloc +=1\n",
    "        else:\n",
    "          print('FIPS not in lookup table ' +str(iloc) + ' ' + str(localfips))  \n",
    "          skips.append(iloc)\n",
    "\n",
    "\n",
    "Locationrurality[81] = None\n",
    "Locationrurality[2412] = None \n",
    "Locationmad[81] = None \n",
    "Locationmad[2412] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "69-GjMTEK9_w",
    "outputId": "ccf1b684-97f0-4d46-f523-3647917b744c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Age Distribution.csv\n",
      "Missing days at the end -145\n",
      "False\n",
      "Read Age Distribution data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Air Pollution.csv\n",
      "False\n",
      "Read Air Pollution data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Comorbidities.csv\n",
      "False\n",
      "Read Comorbidities data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Health Disparities.csv\n",
      "False\n",
      "Read Health Disparities data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Mobility.csv\n",
      "False\n",
      "Read Mobility data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Residential Density.csv\n",
      "False\n",
      "Read Residential Density data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Disease Spread.csv\n",
      "False\n",
      "Read Disease Spread data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Social Distancing.csv\n",
      "False\n",
      "Read Social Distancing data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Testing.csv\n",
      "False\n",
      "Read Testing data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Transmissible Cases.csv\n",
      "False\n",
      "Read Transmissible Cases data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/VaccinationOneDose.csv\n",
      "False\n",
      "Read VaccinationOneDose data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      "./independent_study2/COVIDJuly2020/CovidDecember12-2021/Vaccination.csv\n",
      "False\n",
      "Read Vaccination data for locations 3142 Time Steps 488 Days dropped at start 1\n",
      " Rurality Cut [4.501, 5] removes 2960 of 3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/1353694957.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  uselocation=np.full(Nloc,False,dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "if NIHCovariates:\n",
    "  if ReadJan2021Covid:\n",
    "    Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Comorbidities.csv\",\"Demographics.csv\", \"Disease Spread.csv\", \n",
    "                     \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Intervention Testing.csv\", \"Mobility.csv\", \n",
    "                     \"Residential Density.csv\", \"Social Distancing.csv\",  \"Transmissible Cases.csv\"]\n",
    "    Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\",  \"Demographics\", \"Disease Spread\", \n",
    "                 \"Health Disparities\", \"Hospital Beds\", \"Intervention Testing\", \"Mobility\", \"Residential Density\", \n",
    "                 \"Social Distancing\", \"Transmissible Cases\"]\n",
    "  \n",
    "  elif ReadApril2021Covid:\n",
    "    Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Comorbidities.csv\",\"Demographics.csv\", \"Disease Spread.csv\", \n",
    "                     \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Mobility.csv\", \n",
    "                     \"Residential Density.csv\", \"Social Distancing.csv\", \"Testing.csv\", \"Transmissible Cases.csv\",\"NOFILE\"]\n",
    "    Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\",  \"Demographics\", \"Disease Spread\", \n",
    "                 \"Health Disparities\", \"Hospital Beds\",  \"Mobility\", \"Residential Density\", \n",
    "                 \"Social Distancing\", \"Testing\",\"Transmissible Cases\",\"voting\"]\n",
    "  elif ReadDecember2021:\n",
    "\n",
    "    Propfilenames = FeatureFiles\n",
    "    Propnames = [i[:-4] for i in Propfilenames]\n",
    "\n",
    "    if UseVoting:\n",
    "      Propfilenames.append(\"NOFILE\")\n",
    "      Propnames.append(\"voting\")\n",
    "\n",
    "  else:\n",
    "    Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Co-morbidities.csv\", \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Pop Demographics.csv\", \"Pop Mobility.csv\", \"Residential Density.csv\", \"Social Distancing.csv\", \"Testing.csv\", \"Transmissible Cases.csv\"]\n",
    "    Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\", \"Health Disparities\", \"Hospital Beds\", \"Pop Demographics\", \"Pop Mobility\", \"Residential Density\", \"Social Distancing\", \"Testing\", \"Transmissible Cases\"]\n",
    "  \n",
    "  NIHDATADIR = DATASETDIR + '/' \n",
    "  numberfiles = len(Propnames)\n",
    "  NpropperTimeStatic = 0\n",
    "  if NpropperTimeDynamic != numberfiles:\n",
    "    printexit('EXIT: Dynamic Properties set wrong ' + str(numberfiles) + ' ' + str(NpropperTimeDynamic))\n",
    "  DynamicPropertyTimeSeries = np.empty([Num_Time,Nloc,numberfiles],dtype = np.float32)\n",
    "  enddifference = NaN\n",
    "\n",
    "  for ifiles in range(0,numberfiles):\n",
    "    InputPropertyNames.append(Propnames[ifiles])\n",
    "    if Propfilenames[ifiles] == 'NOFILE': # Special case of Voting Data\n",
    "      for iloc in range(0,Nloc):\n",
    "        Demsize = DemVoting[iloc]\n",
    "        RepSize = RepVoting[iloc]\n",
    "        Votingcovariate = Demsize/(RepSize+Demsize)\n",
    "        DynamicPropertyTimeSeries[:,iloc,ifiles] = Votingcovariate\n",
    "      continue # over ifile loop\n",
    "\n",
    "    DynamicPropFile = NIHDATADIR + Propfilenames[ifiles]\n",
    "    if not (ReadJan2021Covid or ReadApril2021Covid or ReadDecember2021):\n",
    "      DynamicPropFile = DATASETDIR + '/ThirdCovariates/' + Propfilenames[ifiles]\n",
    "\n",
    "    # Read in  Covariate Data into DynamicPropertyTimeSeries\n",
    "    print(DynamicPropFile)\n",
    "    with open(DynamicPropFile, 'r') as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        header = next(csv_reader)\n",
    "        skip = 1\n",
    "        if ReadJan2021Covid or ReadApril2021Covid or ReadDecember2021:\n",
    "          skip = 2\n",
    "          Ftype = header[0]\n",
    "          if Ftype != 'Name':\n",
    "            printexit('EXIT: Wrong file type ' + Ftype)\n",
    "        Ftype = header[skip-1]\n",
    "        if Ftype != 'FIPS':\n",
    "          printexit('EXIT: Wrong file type ' + Ftype)\n",
    "        # Check Date\n",
    "        hformat = '%m-%d-%Y'\n",
    "        if ReadJan2021Covid or ReadApril2021Covid or ReadDecember2021:\n",
    "          hformat = '%Y-%m-%d'\n",
    "        firstdate = datetime.strptime(header[skip], hformat)\n",
    "        tdelta = (firstdate-InitialDate).days \n",
    "        if tdelta > 0:\n",
    "          printexit('Missing Covariate Data start -- adjust Dropearlydata ' + str(tdelta))\n",
    "        lastdate = datetime.strptime(header[len(header)-1], hformat)\n",
    "        enddifference1 = (FinalDate-lastdate).days\n",
    "        if math.isnan(enddifference):\n",
    "          enddifference = enddifference1\n",
    "          print('Missing days at the end ' + str(enddifference))\n",
    "        else:\n",
    "          if enddifference != enddifference1:\n",
    "            printexit('EXIT: Incorrect time length ' + Propnames[ifiles] + ' expected ' + str(enddifference) + ' actual ' +str(enddifference1))\n",
    "        iloc = 0\n",
    "        \n",
    "        #Test\n",
    "        if lastdate > FinalDate:\n",
    "          tempInt = NumberofTimeunits\n",
    "        else:\n",
    "          tempInt = NumberofTimeunits - enddifference\n",
    "\n",
    "        for nextrow in csv_reader:\n",
    "          if iloc == 0:\n",
    "            print(len(nextrow)!=NumberofTimeunits + skip -enddifference-tdelta)\n",
    "          if (len(nextrow)!=NumberofTimeunits + skip -enddifference-tdelta):\n",
    "            printexit('EXIT: Incorrect row length ' + Propnames[ifiles] + ' Location ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "          localfips = nextrow[skip-1]\n",
    "          jloc = FIPSstringlookup[localfips] \n",
    "          for itime in range(0, tempInt): #NumberofTimeunits - enddifference\n",
    "            DynamicPropertyTimeSeries[itime,jloc,ifiles] = nextrow[itime + skip - tdelta]\n",
    "        # Use previous week value for missing data at the end\n",
    "          if tempInt != NumberofTimeunits:  \n",
    "            for itime in range(tempInt, NumberofTimeunits):# NumberofTimeunits - enddifference\n",
    "              DynamicPropertyTimeSeries[itime,jloc,ifiles] = DynamicPropertyTimeSeries[itime-7,jloc,ifiles]\n",
    "          iloc += 1\n",
    "# End Reading in dynamic property data\n",
    "\n",
    "    if iloc != Nloc:\n",
    "            printexit('EXIT Inconsistent location lengths ' + Propnames[ifiles] + str(iloc) + ' ' + str(Nloc))\n",
    "    print('Read ' + Propnames[ifiles] + ' data for locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time) + ' Days dropped at start ' + str(-tdelta))\n",
    "\n",
    "  if ReadApril2021Covid or ReadDecember2021:\n",
    "\n",
    "    if RuralityCut:\n",
    "      uselocation=np.full(Nloc,False,dtype=np.bool)\n",
    "      if len(RuralityRange) > 0:\n",
    "        for jloc in range(0,Nloc):\n",
    "          if Locationrurality[jloc] >= RuralityRange[0] and Locationrurality[jloc] <= RuralityRange[1]:\n",
    "            uselocation[jloc] = True\n",
    "        TotRuralityCut = uselocation.sum()\n",
    "        NumberCut = Nloc - TotRuralityCut\n",
    "        print(' Rurality Cut ' + str(RuralityRange) + ' removes ' + str(Nloc - TotRuralityCut) + ' of ' + str(Nloc))\n",
    "\n",
    "      else: \n",
    "        printexit('EXIT There are no rurality criteria')\n",
    "      \n",
    "      if TotRuralityCut > 0:\n",
    "        NewNloc = Nloc - NumberCut\n",
    "        NewNFIPS = NewNloc\n",
    "        NewLocationfips = np.empty(NewNFIPS, dtype=int) # integer version of FIPs\n",
    "        NewLocationcolumns = [] # String version of FIPS\n",
    "        NewFIPSintegerlookup = {}\n",
    "        NewFIPSstringlookup = {}\n",
    "        NewBasicInputTimeSeries = np.empty([Num_Time,NewNloc,2],dtype = np.float32)\n",
    "        NewLocationname = ['Empty'] * NewNFIPS\n",
    "        NewLocationstate = ['Empty'] * NewNFIPS\n",
    "        NewLocationpopulation = np.empty(NewNFIPS, dtype=int)\n",
    "        NewLocationrurality = np.empty(NewNFIPS)\n",
    "        NewLocationmad = np.empty(NewNFIPS)\n",
    "        NewDynamicPropertyTimeSeries = np.empty([Num_Time,NewNloc,numberfiles],dtype = np.float32) \n",
    "\n",
    "        Newiloc = 0\n",
    "        for iloc in range(0,Nloc):\n",
    "          if not uselocation[iloc]:\n",
    "            continue\n",
    "          NewBasicInputTimeSeries[:,Newiloc,:] = BasicInputTimeSeries[:,iloc,:]\n",
    "          NewDynamicPropertyTimeSeries[:,Newiloc,:] = DynamicPropertyTimeSeries[:,iloc,:]\n",
    "          localfips = Locationcolumns[iloc]\n",
    "          NewLocationcolumns.append(localfips)\n",
    "          NewLocationfips[Newiloc] = int(localfips)\n",
    "          NewFIPSintegerlookup[int(localfips)] = Newiloc\n",
    "          NewFIPSstringlookup[localfips] = Newiloc \n",
    "          NewLocationpopulation[Newiloc] = Locationpopulation[iloc]\n",
    "          NewLocationstate[Newiloc] = Locationstate[iloc]\n",
    "          NewLocationname[Newiloc] = Locationname[iloc]\n",
    "          NewLocationrurality[Newiloc] = Locationrurality[iloc]\n",
    "          NewLocationmad[Newiloc] = Locationmad[iloc]\n",
    "          Newiloc +=1\n",
    "\n",
    "        BasicInputTimeSeries = NewBasicInputTimeSeries\n",
    "        DynamicPropertyTimeSeries = NewDynamicPropertyTimeSeries\n",
    "        Locationname = NewLocationname\n",
    "        Locationstate = NewLocationstate\n",
    "        Locationpopulation = NewLocationpopulation\n",
    "        FIPSstringlookup = NewFIPSstringlookup\n",
    "        FIPSintegerlookup = NewFIPSintegerlookup\n",
    "        Locationcolumns = NewLocationcolumns\n",
    "        Locationfips = NewLocationfips\n",
    "        Locationrurality = NewLocationrurality\n",
    "        Locationmad = NewLocationmad\n",
    "        NFIPS = NewNFIPS\n",
    "        Nloc = NewNloc\n",
    "\n",
    "\n",
    "    else:\n",
    "      uselocation = np.full(Nloc, True, dtype = np.bool)\n",
    "      if (CovidPopulationCut > 0) or (NumberCut > 0):\n",
    "        if NumberCut >0:\n",
    "          smalllocations = np.argsort(Locationpopulation)\n",
    "          for jloc in range(0,NumberCut):\n",
    "            uselocation[smalllocations[jloc]] = False\n",
    "          CovidPopulationCut = Locationpopulation[smalllocations[NumberCut]]\n",
    "        else:\n",
    "          NumberCut =0\n",
    "          for iloc in range(0,Nloc):\n",
    "            if Locationpopulation[iloc] < CovidPopulationCut:\n",
    "              uselocation[iloc] = False\n",
    "              NumberCut += 1\n",
    "        print(' Population Cut ' + str(CovidPopulationCut) + ' removes ' + str(NumberCut) + ' of ' + str(Nloc))\n",
    "      if(NumberCut > 0):\n",
    "        NewNloc = Nloc - NumberCut\n",
    "        NewNFIPS = NewNloc\n",
    "        NewLocationfips = np.empty(NewNFIPS, dtype=int) # integer version of FIPs\n",
    "        NewLocationcolumns = [] # String version of FIPS\n",
    "        NewFIPSintegerlookup = {}\n",
    "        NewFIPSstringlookup = {}\n",
    "        NewBasicInputTimeSeries = np.empty([Num_Time,NewNloc,2],dtype = np.float32)\n",
    "        NewLocationname = ['Empty'] * NewNFIPS\n",
    "        NewLocationstate = ['Empty'] * NewNFIPS\n",
    "        NewLocationpopulation = np.empty(NewNFIPS, dtype=int)\n",
    "        NewDynamicPropertyTimeSeries = np.empty([Num_Time,NewNloc,numberfiles],dtype = np.float32) \n",
    "\n",
    "        Newiloc = 0\n",
    "        for iloc in range(0,Nloc):\n",
    "          if not uselocation[iloc]:\n",
    "            continue\n",
    "          NewBasicInputTimeSeries[:,Newiloc,:] = BasicInputTimeSeries[:,iloc,:]\n",
    "          NewDynamicPropertyTimeSeries[:,Newiloc,:] = DynamicPropertyTimeSeries[:,iloc,:]\n",
    "          localfips = Locationcolumns[iloc]\n",
    "          NewLocationcolumns.append(localfips)\n",
    "          NewLocationfips[Newiloc] = int(localfips)\n",
    "          NewFIPSintegerlookup[int(localfips)] = Newiloc\n",
    "          NewFIPSstringlookup[localfips] = Newiloc \n",
    "          NewLocationpopulation[Newiloc] = Locationpopulation[iloc]\n",
    "          NewLocationstate[Newiloc] = Locationstate[iloc]\n",
    "          NewLocationname[Newiloc] = Locationname[iloc]\n",
    "          Newiloc +=1\n",
    "\n",
    "        BasicInputTimeSeries = NewBasicInputTimeSeries\n",
    "        DynamicPropertyTimeSeries = NewDynamicPropertyTimeSeries\n",
    "        Locationname = NewLocationname\n",
    "        Locationstate = NewLocationstate\n",
    "        Locationpopulation = NewLocationpopulation\n",
    "        FIPSstringlookup = NewFIPSstringlookup\n",
    "        FIPSintegerlookup = NewFIPSintegerlookup\n",
    "        Locationcolumns = NewLocationcolumns\n",
    "        Locationfips = NewLocationfips\n",
    "        NFIPS = NewNFIPS\n",
    "        Nloc = NewNloc\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6E93pmiMTrG"
   },
   "source": [
    "### Apply Second filter to data to stratify based on MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-53lCQRz4hCm",
    "outputId": "b544e2a9-7d96-42e0-8b1c-dca90836b97d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAD Cut [1, 2] removes 103 of 182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/569544285.py:17: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  uselocation=np.full(Nloc,False,dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "RuralityMADStrataGroups = [0,1,2,3] # Select one group for model training, make sure to update in checkpointing\n",
    "RuralityMADIter = RuralityMADStrataGroups[1]\n",
    "\n",
    "if RuralityMADIter == 0:\n",
    "  MADRange = [0,1]\n",
    "elif RuralityMADIter == 1:\n",
    "  MADRange = [1,2]\n",
    "elif RuralityMADIter == 2:\n",
    "  MADRange = [2,3]\n",
    "elif RuralityMADIter == 3:\n",
    "  MADRange = [3,4]\n",
    "else:\n",
    "  printexit('EXIT Not a valid Rurality MAD Grouping')\n",
    "\n",
    "\n",
    "if RuralityCut:\n",
    "    uselocation=np.full(Nloc,False,dtype=np.bool)\n",
    "    if len(MADRange) > 0:\n",
    "      for jloc in range(0,Nloc):\n",
    "        if (Locationmad[jloc] >= MADRange[0])&(Locationmad[jloc] < MADRange[1]):\n",
    "          uselocation[jloc] = True\n",
    "      TotMADCut = uselocation.sum()\n",
    "      NumberCut = Nloc - TotMADCut\n",
    "      print(' MAD Cut ' + str(MADRange) + ' removes ' + str(Nloc - TotMADCut) + ' of ' + str(Nloc))\n",
    "\n",
    "    else: \n",
    "      printexit('EXIT There are no rurality criteria')\n",
    "    \n",
    "    if TotRuralityCut > 0:\n",
    "      NewNloc = Nloc - NumberCut\n",
    "      NewNFIPS = NewNloc\n",
    "      NewLocationfips = np.empty(NewNFIPS, dtype=int) # integer version of FIPs\n",
    "      NewLocationcolumns = [] # String version of FIPS\n",
    "      NewFIPSintegerlookup = {}\n",
    "      NewFIPSstringlookup = {}\n",
    "      NewBasicInputTimeSeries = np.empty([Num_Time,NewNloc,2],dtype = np.float32)\n",
    "      NewLocationname = ['Empty'] * NewNFIPS\n",
    "      NewLocationstate = ['Empty'] * NewNFIPS\n",
    "      NewLocationpopulation = np.empty(NewNFIPS, dtype=int)\n",
    "      NewDynamicPropertyTimeSeries = np.empty([Num_Time,NewNloc,numberfiles],dtype = np.float32) \n",
    "\n",
    "      Newiloc = 0\n",
    "      for iloc in range(0,Nloc):\n",
    "        if not uselocation[iloc]:\n",
    "          continue\n",
    "        NewBasicInputTimeSeries[:,Newiloc,:] = BasicInputTimeSeries[:,iloc,:]\n",
    "        NewDynamicPropertyTimeSeries[:,Newiloc,:] = DynamicPropertyTimeSeries[:,iloc,:]\n",
    "        localfips = Locationcolumns[iloc]\n",
    "        NewLocationcolumns.append(localfips)\n",
    "        NewLocationfips[Newiloc] = int(localfips)\n",
    "        NewFIPSintegerlookup[int(localfips)] = Newiloc\n",
    "        NewFIPSstringlookup[localfips] = Newiloc \n",
    "        NewLocationpopulation[Newiloc] = Locationpopulation[iloc]\n",
    "        NewLocationstate[Newiloc] = Locationstate[iloc]\n",
    "        NewLocationname[Newiloc] = Locationname[iloc]\n",
    "        Newiloc +=1\n",
    "\n",
    "      BasicInputTimeSeries = NewBasicInputTimeSeries\n",
    "      DynamicPropertyTimeSeries = NewDynamicPropertyTimeSeries\n",
    "      Locationname = NewLocationname\n",
    "      Locationstate = NewLocationstate\n",
    "      Locationpopulation = NewLocationpopulation\n",
    "      FIPSstringlookup = NewFIPSstringlookup\n",
    "      FIPSintegerlookup = NewFIPSintegerlookup\n",
    "      Locationcolumns = NewLocationcolumns\n",
    "      Locationfips = NewLocationfips\n",
    "      NFIPS = NewNFIPS\n",
    "      Nloc = NewNloc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EdsHkx7jLJX"
   },
   "source": [
    "### Read and setup NIH Covariates August 2020 and January, April 2021 Data\n",
    "\n",
    "new collection of time dependent covariates (even if constant).\n",
    "\n",
    "cases and deaths and location property from previous data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awLjz1adEXr3"
   },
   "source": [
    "## Process Input Data  in various ways\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1fLYj-KBAjF"
   },
   "source": [
    "###Set TFT Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "hH6D2TmcBE4u",
    "outputId": "e5075cb8-53f5-47dd-ee66-7ab143b3e56c"
   },
   "outputs": [],
   "source": [
    "TFTConversion = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrugyhFU66md"
   },
   "source": [
    "### Convert Cumulative to Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ipjkf86A6imL",
    "outputId": "63ab21c8-dde2-4bbf-fcd9-8929e764a3cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original summed counts [344652.   6729.] become [349361.   6849.] Cases, Deaths\n"
     ]
    }
   ],
   "source": [
    "# Convert  cumulative to Daily. \n",
    "# Replace negative daily values by zero\n",
    "# remove daily to sqrt(daily)  and Then normalize maximum to 1\n",
    "if ConvertDynamicPredictedQuantity:\n",
    "  NewBasicInputTimeSeries = np.empty_like(BasicInputTimeSeries, dtype=np.float32)\n",
    "  Zeroversion = np.zeros_like(BasicInputTimeSeries, dtype=np.float32)\n",
    "  Rolleddata = np.roll(BasicInputTimeSeries, 1, axis=0)\n",
    "  Rolleddata[0,:,:] = Zeroversion[0,:,:]\n",
    "  NewBasicInputTimeSeries = np.maximum(np.subtract(BasicInputTimeSeries,Rolleddata),Zeroversion)\n",
    "  originalnumber = np.sum(BasicInputTimeSeries[NumberofTimeunits-1,:,:],axis=0)\n",
    "  newnumber = np.sum(NewBasicInputTimeSeries,axis=(0,1))\n",
    "  print('Original summed counts ' + str(originalnumber) + ' become ' + str(newnumber)+ ' Cases, Deaths')\n",
    "\n",
    "  BasicInputTimeSeries = NewBasicInputTimeSeries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZLkGseQpGlr"
   },
   "source": [
    "### Static and Dynamic specials for COVID\n",
    "\n",
    "except case where Romeo data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "1fhi0Ug84ehL",
    "outputId": "ba4f73c3-b217-42ae-b744-a619ca4d6eea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/2891211563.py:7: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  NewProperty_is_Intensive = np.full(NewNpropperTime, True, dtype = np.bool)\n"
     ]
    }
   ],
   "source": [
    "# Remove special status of Cases and Deaths\n",
    "if CDSpecial:\n",
    "    \n",
    "  NewNpropperTimeDynamic = NpropperTimeDynamic + 2\n",
    "  NewNpropperTime = NpropperTimeStatic + NewNpropperTimeDynamic   \n",
    "\n",
    "  NewProperty_is_Intensive = np.full(NewNpropperTime, True, dtype = np.bool)\n",
    "  NewInputPropertyNames = []\n",
    "  NewDynamicPropertyTimeSeries = np.empty([Num_Time,NewNloc,NewNpropperTimeDynamic],dtype = np.float32)\n",
    "\n",
    "  for casesdeaths in range(0,2):\n",
    "    NewDynamicPropertyTimeSeries[:,:,casesdeaths] = BasicInputTimeSeries[:,:,casesdeaths]\n",
    "  BasicInputTimeSeries = None\n",
    "\n",
    "  for iprop in range(0,NpropperTimeStatic):\n",
    "    NewInputPropertyNames.append(InputPropertyNames[iprop])\n",
    "    NewProperty_is_Intensive[iprop] = Property_is_Intensive[iprop]\n",
    "  NewProperty_is_Intensive[NpropperTimeStatic] = False\n",
    "  NewProperty_is_Intensive[NpropperTimeStatic+1] = False\n",
    "  NewInputPropertyNames.append('Cases')\n",
    "  NewInputPropertyNames.append('Deaths')\n",
    "  for ipropdynamic in range(0,NpropperTimeDynamic):\n",
    "    Newiprop = NpropperTimeStatic+2+ipropdynamic\n",
    "    iprop = NpropperTimeStatic+ipropdynamic\n",
    "    NewDynamicPropertyTimeSeries[:,:,Newiprop] = DynamicPropertyTimeSeries[:,:,iprop]\n",
    "    NewInputPropertyNames.append(InputPropertyNames[iprop])\n",
    "    NewProperty_is_Intensive[Newiprop] = Property_is_Intensive[iprop]\n",
    "  \n",
    "  NpropperTimeDynamic = NewNpropperTimeDynamic\n",
    "  NpropperTime = NewNpropperTime\n",
    "  DynamicPropertyTimeSeries = NewDynamicPropertyTimeSeries\n",
    "  InputPropertyNames = NewInputPropertyNames\n",
    "  Property_is_Intensive = NewProperty_is_Intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTNVFciYtS5Y"
   },
   "source": [
    "### Static Property Manipulations for Covid Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "F5BPSUk7i377",
    "outputId": "df80be76-791a-4ba7-a2d2-bff183d2dd9c"
   },
   "outputs": [],
   "source": [
    "# Execute under all COVID circumstances properties generated here\n",
    "if CDSpecial:\n",
    "  if NpropperTimeStatic > 0:\n",
    "    Num_Extensive = 0\n",
    "    for iprop in range(0,NpropperTimeStatic):\n",
    "      if not Property_is_Intensive[iprop]:\n",
    "        Num_Extensive +=1\n",
    "    print(startbold + startred + ' Number of Extensive parameters ' + str(Num_Extensive) + resetfonts)\n",
    "    for iprop in range(0,NpropperTimeStatic):\n",
    "      if not Property_is_Intensive[iprop]:\n",
    "        print(InputPropertyNames[iprop])\n",
    "\n",
    "    # Convert Extensive covariates to SQRT(Population normed)\n",
    "    # Replace negatives by mean of positives and zeroes\n",
    "    positivemean = np.zeros(NpropperTimeStatic, dtype = np.float32)\n",
    "    countvalidentries = np.zeros(NpropperTimeStatic, dtype = np.float32)\n",
    "    for iloc in range(0,Nloc):\n",
    "      for iprop in range(0,NpropperTimeStatic):\n",
    "        if not Property_is_Intensive[iprop]:\n",
    "          BasicInputStaticProps[iloc,iprop] = np.sqrt(BasicInputStaticProps[iloc,iprop]/Locationpopulation[iloc])\n",
    "        else:\n",
    "          if BasicInputStaticProps[iloc,iprop] >= 0:\n",
    "            positivemean[iprop] += BasicInputStaticProps[iloc,iprop]\n",
    "            countvalidentries[iprop] += 1.0\n",
    "\n",
    "    for iprop in range(0,NpropperTimeStatic):\n",
    "        if Property_is_Intensive[iprop]:\n",
    "          positivemean[iprop] /= countvalidentries[iprop]\n",
    "\n",
    "    for iloc in range(0,Nloc):\n",
    "      for iprop in range(0,NpropperTimeStatic):\n",
    "        if Property_is_Intensive[iprop]:\n",
    "          if BasicInputStaticProps[iloc,iprop] < 0:\n",
    "            BasicInputStaticProps[iloc,iprop] = positivemean[iprop]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSDyT65ly4Q-"
   },
   "source": [
    "###Normalize All Static and Dynamic Properties\n",
    "\n",
    "for Static Properties BasicInputStaticProps[Nloc,NpropperTimeStatic] converts to NormedInputStaticProps[Nloc,NpropperTimeStatic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "NGbBzf47zv1m",
    "outputId": "965de01a-2feb-49df-9fe8-885c1ab97587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mProperties scaled\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[35mName   Min    Max    Norm    Mean    Std    Normed Mean    Normed Std    \u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[35m0 Cases\u001b[0m Root 2 0.0 36.263 0.028 2.035 2.218 0.056 0.061\n",
      "\u001b[1m\u001b[35m1 Deaths\u001b[0m Root 2 0.0 6.557 0.152 0.132 0.4 0.02 0.061\n",
      "\u001b[1m\u001b[35m2 Age Distribution\u001b[0m Root 1 0.4 0.696 3.372 0.578 0.061 0.601 0.206\n",
      "\u001b[1m\u001b[35m3 Air Pollution\u001b[0m Root 1 0.0 0.587 1.704 0.383 0.121 0.653 0.206\n",
      "\u001b[1m\u001b[35m4 Comorbidities\u001b[0m Root 1 0.15 0.721 1.751 0.393 0.122 0.425 0.213\n",
      "\u001b[1m\u001b[35m5 Health Disparities\u001b[0m Root 1 0.063 0.77 1.413 0.383 0.188 0.452 0.265\n",
      "\u001b[1m\u001b[35m6 Mobility\u001b[0m Root 1 0.399 0.642 4.114 0.544 0.043 0.596 0.178\n",
      "\u001b[1m\u001b[35m7 Residential Density\u001b[0m Root 1 0.037 0.985 1.055 0.602 0.252 0.596 0.266\n",
      "\u001b[1m\u001b[35m8 Disease Spread\u001b[0m Root 1 0.0 1.0 1.0 0.196 0.226 0.196 0.226\n",
      "\u001b[1m\u001b[35m9 Social Distancing\u001b[0m Root 1 0.0 1.0 1.0 0.823 0.169 0.823 0.169\n",
      "\u001b[1m\u001b[35m10 Testing\u001b[0m Root 1 0.0 1.0 1.0 0.535 0.222 0.535 0.222\n",
      "\u001b[1m\u001b[35m11 Transmissible Cases\u001b[0m Root 1 0.0 0.957 1.044 0.468 0.2 0.488 0.209\n",
      "\u001b[1m\u001b[35m12 VaccinationOneDose\u001b[0m Root 1 0.0 93.6 0.011 7.757 14.064 0.083 0.15\n",
      "\u001b[1m\u001b[35m13 Vaccination\u001b[0m Root 1 0.0 85.5 0.012 5.877 11.725 0.069 0.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/3832495392.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  QuantityTakeroot = np.full(NpropperTimeMAX,1,dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "def SetTakeroot(x,n):\n",
    "    if np.isnan(x):\n",
    "      return NaN   \n",
    "    if n == 3:\n",
    "      return np.cbrt(x)\n",
    "    elif n == 2:\n",
    "      if x <= 0.0:\n",
    "        return 0.0\n",
    "      return np.sqrt(x) \n",
    "    return x \n",
    "\n",
    "def DynamicPropertyScaling(InputTimeSeries):\n",
    "    Results = np.full(7, 0.0,dtype=np.float32)\n",
    "    Results[1] = np.nanmax(InputTimeSeries, axis = (0,1))\n",
    "    Results[0] = np.nanmin(InputTimeSeries, axis = (0,1))\n",
    "    Results[3] = np.nanmean(InputTimeSeries, axis = (0,1))\n",
    "    Results[4] = np.nanstd(InputTimeSeries, axis = (0,1))\n",
    "    Results[2] = np.reciprocal(np.subtract(Results[1],Results[0]))\n",
    "    Results[5] = np.multiply(Results[2],np.subtract(Results[3],Results[0]))\n",
    "    Results[6] = np.multiply(Results[2],Results[4])\n",
    "    return Results\n",
    "\n",
    "NpropperTimeMAX = NpropperTime + NumTimeSeriesCalculated  \n",
    "if ScaleProperties:\n",
    "  QuantityTakeroot = np.full(NpropperTimeMAX,1,dtype=np.int)\n",
    "  if Hydrology:\n",
    "    QuantityTakeroot[27] = 3\n",
    "    QuantityTakeroot[32] = 3\n",
    "  if CDSpecial:\n",
    "    QuantityTakeroot[NpropperTimeStatic] =2\n",
    "    QuantityTakeroot[NpropperTimeStatic+1] =2\n",
    "\n",
    "# Scale data by roots if requested\n",
    "  for iprop in range(0, NpropperTimeMAX):\n",
    "    if QuantityTakeroot[iprop] >= 2:\n",
    "      if iprop < NpropperTimeStatic:\n",
    "        for iloc in range(0,Nloc):\n",
    "          BasicInputStaticProps[iloc,iprop] = SetTakeroot(BasicInputStaticProps[iloc,iprop],QuantityTakeroot[iprop])\n",
    "      elif iprop < NpropperTime:\n",
    "        for itime in range(0,NumberofTimeunits):\n",
    "          for iloc in range(0,Nloc):\n",
    "            DynamicPropertyTimeSeries[itime,iloc,iprop-NpropperTimeStatic] = SetTakeroot(\n",
    "                DynamicPropertyTimeSeries[itime,iloc,iprop-NpropperTimeStatic],QuantityTakeroot[iprop])\n",
    "      else:\n",
    "        for itime in range(0,NumberofTimeunits):\n",
    "          for iloc in range(0,Nloc):\n",
    "            CalculatedTimeSeries[itime,iloc,iprop-NpropperTime] =SetTakeroot(\n",
    "                CalculatedTimeSeries[itime,iloc,iprop-NpropperTime],QuantityTakeroot[iprop])\n",
    "\n",
    "  QuantityStatisticsNames = ['Min','Max','Norm','Mean','Std','Normed Mean','Normed Std']\n",
    "  QuantityStatistics = np.zeros([NpropperTimeMAX,7], dtype=np.float32)\n",
    "  if NpropperTimeStatic > 0:  \n",
    "    print(BasicInputStaticProps.shape)\n",
    "    max_value = np.amax(BasicInputStaticProps, axis = 0)\n",
    "    min_value = np.amin(BasicInputStaticProps, axis = 0)\n",
    "    mean_value = np.mean(BasicInputStaticProps, axis = 0)\n",
    "    std_value = np.std(BasicInputStaticProps, axis = 0)\n",
    "    normval = np.reciprocal(np.subtract(max_value,min_value))\n",
    "    normed_mean = np.multiply(normval,np.subtract(mean_value,min_value))\n",
    "    normed_std = np.multiply(normval,std_value)\n",
    "    QuantityStatistics[0:NpropperTimeStatic,0] = min_value\n",
    "    QuantityStatistics[0:NpropperTimeStatic,1] = max_value\n",
    "    QuantityStatistics[0:NpropperTimeStatic,2] = normval\n",
    "    QuantityStatistics[0:NpropperTimeStatic,3] = mean_value\n",
    "    QuantityStatistics[0:NpropperTimeStatic,4] = std_value\n",
    "    QuantityStatistics[0:NpropperTimeStatic,5] = normed_mean\n",
    "    QuantityStatistics[0:NpropperTimeStatic,6] = normed_std\n",
    "\n",
    "    NormedInputStaticProps =np.empty_like(BasicInputStaticProps)\n",
    "    for iloc in range(0,Nloc):\n",
    "      NormedInputStaticProps[iloc,:] = np.multiply((BasicInputStaticProps[iloc,:] - min_value[:]),normval[:])\n",
    "\n",
    "  if (NpropperTimeDynamic > 0) or (NumTimeSeriesCalculated>0):\n",
    "    for iprop in range(NpropperTimeStatic,NpropperTimeStatic+NpropperTimeDynamic):\n",
    "      QuantityStatistics[iprop,:] = DynamicPropertyScaling(DynamicPropertyTimeSeries[:,:,iprop-NpropperTimeStatic])\n",
    "    for iprop in range(0,NumTimeSeriesCalculated):\n",
    "      QuantityStatistics[iprop+NpropperTime,:] = DynamicPropertyScaling(CalculatedTimeSeries[:,:,iprop]) \n",
    "\n",
    "    NormedDynamicPropertyTimeSeries = np.empty_like(DynamicPropertyTimeSeries)\n",
    "    for iprop in range(NpropperTimeStatic,NpropperTimeStatic+NpropperTimeDynamic):\n",
    "      NormedDynamicPropertyTimeSeries[:,:,iprop - NpropperTimeStatic] = np.multiply((DynamicPropertyTimeSeries[:,:,iprop - NpropperTimeStatic]\n",
    "                                                - QuantityStatistics[iprop,0]),QuantityStatistics[iprop,2])\n",
    "    \n",
    "    if NumTimeSeriesCalculated > 0:\n",
    "      NormedCalculatedTimeSeries = np.empty_like(CalculatedTimeSeries)\n",
    "      for iprop in range(NpropperTime,NpropperTimeMAX):\n",
    "        NormedCalculatedTimeSeries[:,:,iprop - NpropperTime] = np.multiply((CalculatedTimeSeries[:,:,iprop - NpropperTime]\n",
    "                                                - QuantityStatistics[iprop,0]),QuantityStatistics[iprop,2])\n",
    "      CalculatedTimeSeries = None\n",
    "  \n",
    "    BasicInputStaticProps = None\n",
    "    DynamicPropertyTimeSeries = None\n",
    "    print(startbold + \"Properties scaled\" +resetfonts)\n",
    "\n",
    "  line = 'Name   '\n",
    "  for propval in range (0,7):\n",
    "    line += QuantityStatisticsNames[propval] + '    '\n",
    "  print('\\n' + startbold +startpurple + line + resetfonts)\n",
    "  for iprop in range(0,NpropperTimeMAX):\n",
    "    if iprop == NpropperTimeStatic:\n",
    "      print('\\n')\n",
    "    line = startbold + startpurple + str(iprop) + ' ' + InputPropertyNames[iprop] + resetfonts  + ' Root ' + str(QuantityTakeroot[iprop])\n",
    "    for propval in range (0,7):\n",
    "      line += ' ' + str(round(QuantityStatistics[iprop,propval],3))\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW9bPWExf4YK"
   },
   "source": [
    "###Set up Futures \n",
    "\n",
    "-- currently at daily level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "1uwExtALgrsW",
    "outputId": "3e73c689-5eda-4228-d4a6-56cebe3043e1"
   },
   "outputs": [],
   "source": [
    "class Future:\n",
    "    def __init__(self, name, daystart = 0, days =[], wgt=1.0, classweight = 1.0):\n",
    "        self.name = name\n",
    "        self.days = np.array(days)\n",
    "        self.daystart = daystart\n",
    "        self.wgts = np.full_like(self.days,wgt,dtype=float)\n",
    "        self.size = len(self.days)\n",
    "        self.classweight = classweight\n",
    "\n",
    "LengthFutures = 0\n",
    "if GenerateFutures: # daystart overwritten\n",
    "  Secondday = Future('day2',daystart = 23, days=[2],classweight=1./14.)\n",
    "  Thirdday = Future('day3', daystart = 24, days=[3],classweight=1./14.)\n",
    "  Fourthday = Future('day4', daystart = 25, days=[4],classweight=1./14.)\n",
    "  Fifthday = Future('day5', daystart = 26, days=[5],classweight=1./14.)\n",
    "  Sixthday = Future('day6', daystart = 27, days=[6],classweight=1./14.)\n",
    "  Seventhday = Future('day7', daystart = 27, days=[7],classweight=1./14.)\n",
    "  day8 = Future('day8', daystart = 28, days=[8],classweight=1./14.)\n",
    "  day9 =  Future('day9', daystart = 29, days=[9],classweight=1./14.)\n",
    "  day10 =  Future('day10', daystart = 30, days=[10],classweight=1./14.)\n",
    "  day11 =  Future('day11', daystart = 31, days=[11],classweight=1./14.)\n",
    "  day12 =  Future('day12', daystart = 32, days=[12],classweight=1./14.)\n",
    "  day13 =  Future('day13', daystart = 33, days=[13],classweight=1./14.)\n",
    "  day14 =  Future('day14', daystart = 34, days=[14],classweight=1./14.)\n",
    "  day15 =  Future('day15', daystart = 35, days=[15],classweight=1./14.)\n",
    "  # Secondweek = Future('week2', daystart= 19, days=[9,10,11,12,13,14,15],wgt=1./7.,classweight=0.25)\n",
    "  # Thirdweek = Future('week3', daystart= 26, days=[16,17,18,19,20,21,22],wgt=1./7.,classweight=0.25)\n",
    "  # Fourthweek = Future('week4', daystart = 33, days=[23,24,25,26,27,28,29],wgt=1./7.,classweight=0.25)\n",
    "  # Fifthweek = Future('week5', daystart = 40, days=[30,31,32,33,34,35,36],wgt=1./7.,classweight=0.25)\n",
    "  \n",
    "  Futures = [ Secondday,Thirdday,Fourthday,Fifthday,Sixthday,Seventhday,day8,day9,day10,day11,day12,day13,day14,day15]\n",
    "  Futures =[]\n",
    "  for ifuture in range(0,14):\n",
    "    xx = Future(str(ifuture+1), days=[ifuture+2])\n",
    "    Futures.append(xx)\n",
    "  LengthFutures = len(Futures)\n",
    "  Futuresmaxday = 0\n",
    "  Futuresmaxweek = 0\n",
    "  for i in range(0,LengthFutures):\n",
    "      j = len(Futures[i].days)\n",
    "      if j == 1:\n",
    "          Futuresmaxday = max(Futuresmaxday, Futures[i].days[0])\n",
    "      else:\n",
    "          Futuresmaxweek = max(Futuresmaxweek, Futures[i].days[j-1])\n",
    "      Futures[i].daystart -= Dropearlydata\n",
    "      if Futures[i].daystart < 0: Futures[i].daystart = 0\n",
    "      if Earthquake:\n",
    "        Futures[i].daystart = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdm4DDFL92NJ"
   },
   "source": [
    "###Set up mappings of locations\n",
    "\n",
    "In next cell, we map locations for BEFORE location etc added\n",
    "\n",
    "In cell after that we do same for sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "JRZm-x13980a",
    "outputId": "c74af881-480e-40df-97d3-6ec917b450d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/1070767286.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  MappedLocations = np.arange(0,Nloc, dtype=np.int)\n",
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/1070767286.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  LookupLocations = np.arange(0,Nloc, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "OriginalNloc = Nloc\n",
    "\n",
    "MappedLocations = np.arange(0,Nloc, dtype=np.int)\n",
    "LookupLocations = np.arange(0,Nloc, dtype=np.int)\n",
    "MappedNloc = Nloc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTUIpVT3vris"
   },
   "source": [
    "###Property and Prediction  Data Structures\n",
    "\n",
    "Two important Lists Properties and Predictions that are related\n",
    "\n",
    " * Data stored in series is for properties, the calculated value occuring at or ending that day\n",
    " * For predictions, the data is the calculated value from that date or later. \n",
    "\n",
    " * We store data labelled by time so that\n",
    "  * for inputs we use time 0 upto last value - 1 i.e. position [length of array - 1]\n",
    "  * for outputs (predictions) with sequence Tseq, we use array locations [Tseq] to [length of array -1]\n",
    "  * This implies Num_Seq = Num_Time - Tseq\n",
    "\n",
    "\n",
    "**Properties**\n",
    "\n",
    "Everything appears in Property list -- both input and output (predicted)\n",
    "DynamicPropertyTimeSeries holds input property time series where value is value at that time using data before this time for aggregations\n",
    "  * NpropperTimeStatic is the number of static properties -- typically read in or calculated from input information\n",
    "  * NpropperTimeDynamicInput is total number of input time series\n",
    "  * NpropperTimeDynamicCalculated is total number of calculated dynamic quantities  used in Time series analysis as input properties and/or output predictions\n",
    "  * NpropperTimeDynamic = NpropperTimeDynamicInput + NpropperTimeDynamicCalculated ONLY includes input properties\n",
    "  * NpropperTime = NpropperTimeStatic + NpropperTimeDynamic will not include futures and NOT include calculated predictions\n",
    "  * InputPropertyNames is a list of size NpropperTime holding names\n",
    "  * NpropperTimeMAX = NpropperTime + NumTimeSeriesCalculated has calculated predictions following input properties ignoring futures \n",
    "  * QuantityStatistics has 7 statistics used in normalizing for NpropperTimeMAX properties\n",
    "  * Normalization takes NpropperTimeStatic static features in BasicInputStaticProps and stores in NormedInputStaticProps\n",
    "  * Normalization takes NpropperTimeDynamicInput dynamic features in BasicInputTimeSeries and stores in NormedInputTimeSeries\n",
    "  * Normalization takes NpropperTimeDynamicCalculated dynamic features in DynamicPropertyTimeSeries and stores in NormedDynamicPropertyTimeSeries\n",
    "\n",
    "**Predictions**\n",
    "\n",
    " * NumpredbasicperTime can be 1 upto NpropperTimeDynamic and are part of dynamic input series. It includes input values that are to be predicted (these MUST be at start) plus NumTimeSeriesCalculated calculated series\n",
    " * NumpredFuturedperTime is <= NumpredbasicperTime and is the number of input dynamic series that are futured\n",
    " * NumTimeSeriesCalculated is number of calculated (not as futures) time series stored in CalculatedTimeSeries and names in NamespredCalculated\n",
    " * Typically NumpredbasicperTime = NumTimeSeriesCalculated + NumpredFuturedperTime (**Currently this is assumed**)\n",
    " * Normalization takes NumTimeSeriesCalculated calculated series in CalculatedTimeSeries and stores in NormedCalculatedTimeSeries\n",
    " * Predictions per Time are  NpredperTime = NumpredbasicperTime + NumpredFuturedperTime*LengthFutures\n",
    " * Predictions per sequence Npredperseq = NpredperTime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGvEtAj5xHhR"
   },
   "source": [
    "### Set Requested Properties Predictions Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lycrtgBHxQCq",
    "outputId": "a67a9a5a-67b0-4e6c-bfcc-e8c9563ecbaf"
   },
   "outputs": [],
   "source": [
    "# BASIC EARTHQUAKE SET JUST LOG ENERGY AND MULTIPLICITY\n",
    "if Earthquake:\n",
    "  InputSource = ['Static','Static','Static','Static','Dynamic','Dynamic','Dynamic','Dynamic'\n",
    "    ,'Dynamic','Dynamic','Dynamic','Dynamic','Dynamic']\n",
    "  InputSourceNumber = [0,1,2,3,0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "  PredSource = ['Dynamic','Calc','Calc','Calc','Calc','Calc','Calc','Calc','Calc','Calc']\n",
    "  PredSourceNumber = [0,0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "  FuturedPred = [-1]*len(PredSource)\n",
    "\n",
    "  # Encodings\n",
    "\n",
    "  # Earthquake Space-Time\n",
    "  PropTypes = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','BottomUp','BottomUp','BottomUp','BottomUp']\n",
    "  PropValues = [0, 0, 1, 2, 3,4, 8,16,32,64]\n",
    "\n",
    "  PredTypes = Types = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','BottomUp','BottomUp','BottomUp','BottomUp']\n",
    "  PredValues = [0, 0, 1, 2, 3,4, 8,16,32,64]\n",
    "\n",
    "if ReadApril2021Covid:\n",
    "  InputSource = ['Dynamic']*15\n",
    "  InputSourceNumber = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "\n",
    "  PredSource = ['Dynamic','Dynamic']\n",
    "  PredSourceNumber = [0,1]\n",
    "  FuturedPred = [1,1]\n",
    "\n",
    "  # Encodings\n",
    "  PropTypes = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','Weekly']\n",
    "  PropValues = [0, 0, 1, 2, 3,4, 0]\n",
    "\n",
    "  PredTypes = Types = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','Weekly']\n",
    "  PredValues = [0, 0, 1, 2, 3,4, 0]\n",
    "  if TFTConversion:\n",
    "    PredTypes =[]\n",
    "    PredValues = []\n",
    "\n",
    "if ReadDecember2021:\n",
    "  InputSource = ['Dynamic'] * 14\n",
    "  InputSourceNumber = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "  PredSource = ['Dynamic','Dynamic']\n",
    "  PredSourceNumber = [0,1]\n",
    "  FuturedPred = [1,1]\n",
    "\n",
    "  # Encodings\n",
    "  PropTypes = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','Weekly']\n",
    "  PropValues = [0, 0, 1, 2, 3,4, 0]\n",
    "\n",
    "  PredTypes = Types = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','Weekly']\n",
    "  PredValues = [0, 0, 1, 2, 3,4, 0]\n",
    "  if TFTConversion:\n",
    "    PredTypes =[]\n",
    "    PredValues = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZbYR4a2lGCe"
   },
   "source": [
    "### Choose Input and Predicted Quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tXz5CLaOlOnn",
    "outputId": "3b66cd68-de60-42cb-f4b6-f5338865c4b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/939742535.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  FuturedPointer = np.full(NumpredbasicperTime,-1,dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "if len(InputSource) != len(InputSourceNumber):\n",
    "  printexit(' Inconsistent Source Lengths ' + str(len(InputSource)) + str(len(InputSourceNumber)) )\n",
    "if len(PredSource) != len(PredSourceNumber):\n",
    "  printexit(' Inconsistent Rediction Lengths ' + str(len(PredSource)) + str(len(PredSourceNumber)) )\n",
    "\n",
    "# Executed by all even if GenerateFutures false except for direct Romeo data\n",
    "if (not ReadJuly2020Covid) and not (ReadJan2021Covid) and not (ReadApril2021Covid) and not (ReadDecember2021):\n",
    "  if not UseFutures:\n",
    "      LengthFutures = 0\n",
    "  print(startbold + \"Number of Futures -- separate for each regular prediction \" +str(LengthFutures) + resetfonts)\n",
    "  Usedaystart = False\n",
    "\n",
    "if len(PredSource) > 0: # set up Predictions\n",
    "  NumpredbasicperTime = len(PredSource)\n",
    "  FuturedPointer = np.full(NumpredbasicperTime,-1,dtype=np.int)\n",
    "  NumpredFuturedperTime = 0\n",
    "  NumpredfromInputsperTime = 0\n",
    "  for ipred in range(0,len(PredSource)):\n",
    "    if PredSource[ipred] == 'Dynamic':\n",
    "      NumpredfromInputsperTime += 1\n",
    "  countinputs = 0\n",
    "  countcalcs = 0\n",
    "  for ipred in range(0,len(PredSource)):\n",
    "    if not(PredSource[ipred] == 'Dynamic' or PredSource[ipred] == 'Calc'):\n",
    "      printexit('Illegal Prediction ' + str(ipred) + ' ' + PredSource[ipred])\n",
    "    if PredSource[ipred] == 'Dynamic':\n",
    "      countinputs += 1 \n",
    "    else:\n",
    "      countcalcs += 1\n",
    "    if FuturedPred[ipred] >= 0:\n",
    "      if LengthFutures > 0:\n",
    "        FuturedPred[ipred] = NumpredFuturedperTime\n",
    "        FuturedPointer[ipred] = NumpredFuturedperTime\n",
    "        NumpredFuturedperTime += 1\n",
    "      else:\n",
    "        FuturedPred[ipred] = -1\n",
    "\n",
    "else: # Set defaults\n",
    "  NumpredfromInputsperTime = NumpredFuturedperTime\n",
    "  FuturedPointer = np.full(NumpredbasicperTime,-1,dtype=np.int)\n",
    "  PredSource =[]\n",
    "  PredSourceNumber = []\n",
    "  FuturedPred =[]\n",
    "  futurepos = 0\n",
    "  for ipred in range(0,NumpredFuturedperTime): \n",
    "    PredSource.append('Dynamic')\n",
    "    PredSourceNumber.append(ipred)\n",
    "    futured = -1\n",
    "    if LengthFutures > 0:\n",
    "      futured = futurepos\n",
    "      FuturedPointer[ipred] = futurepos\n",
    "      futurepos += 1\n",
    "    FuturedPred.append(futured)\n",
    "  for ipred in range(0,NumTimeSeriesCalculated):\n",
    "    PredSource.append('Calc')\n",
    "    PredSourceNumber.append(ipred)\n",
    "    FuturedPred.append(-1) \n",
    "  print('Number of Predictions ' + str(len(PredSource)))   \n",
    "\n",
    "\n",
    "PropertyNameIndex = np.empty(NpropperTime, dtype = np.int32)\n",
    "PropertyAverageValuesPointer = np.empty(NpropperTime, dtype = np.int32)\n",
    "for iprop in range(0,NpropperTime):\n",
    "  PropertyNameIndex[iprop] = iprop # names\n",
    "  PropertyAverageValuesPointer[iprop] = iprop # normalizations\n",
    "\n",
    "# Reset Source -- if OK as read don't set InputSource InputSourceNumber\n",
    "# Reset NormedDynamicPropertyTimeSeries and NormedInputStaticProps\n",
    "# Reset NpropperTime = NpropperTimeStatic + NpropperTimeDynamic\n",
    "if len(InputSource) > 0: # Reset Input Source\n",
    "  NewNpropperTimeStatic = 0\n",
    "  NewNpropperTimeDynamic = 0\n",
    "  for isource in range(0,len(InputSource)):\n",
    "    if InputSource[isource] == 'Static':\n",
    "      NewNpropperTimeStatic += 1\n",
    "    if InputSource[isource] == 'Dynamic':\n",
    "      NewNpropperTimeDynamic += 1\n",
    "  NewNormedDynamicPropertyTimeSeries = np.empty([Num_Time,Nloc,NewNpropperTimeDynamic],dtype = np.float32)  \n",
    "  NewNormedInputStaticProps = np.empty([Nloc,NewNpropperTimeStatic],dtype = np.float32)\n",
    "  NewNpropperTime = NewNpropperTimeStatic + NewNpropperTimeDynamic\n",
    "  NewPropertyNameIndex = np.empty(NewNpropperTime, dtype = np.int32)\n",
    "  NewPropertyAverageValuesPointer = np.empty(NewNpropperTime, dtype = np.int32)\n",
    "  countstatic = 0\n",
    "  countdynamic = 0\n",
    "  for isource in range(0,len(InputSource)):\n",
    "    if InputSource[isource] == 'Static':\n",
    "      OldstaticNumber = InputSourceNumber[isource]\n",
    "      NewNormedInputStaticProps[:,countstatic] = NormedInputStaticProps[:,OldstaticNumber]\n",
    "      NewPropertyNameIndex[countstatic] = PropertyNameIndex[OldstaticNumber]\n",
    "      NewPropertyAverageValuesPointer[countstatic] = PropertyAverageValuesPointer[OldstaticNumber]\n",
    "      countstatic += 1\n",
    "\n",
    "    elif InputSource[isource] == 'Dynamic':\n",
    "      OlddynamicNumber =InputSourceNumber[isource]\n",
    "      NewNormedDynamicPropertyTimeSeries[:,:,countdynamic] = NormedDynamicPropertyTimeSeries[:,:,OlddynamicNumber]\n",
    "      NewPropertyNameIndex[countdynamic+NewNpropperTimeStatic] = PropertyNameIndex[OlddynamicNumber+NpropperTimeStatic]\n",
    "      NewPropertyAverageValuesPointer[countdynamic+NewNpropperTimeStatic] = PropertyAverageValuesPointer[OlddynamicNumber+NpropperTimeStatic]\n",
    "      countdynamic += 1\n",
    "    \n",
    "    else:\n",
    "     printexit('Illegal Property ' + str(isource) + ' ' + InputSource[isource]) \n",
    "\n",
    "else: # pretend data altered\n",
    "  NewPropertyNameIndex = PropertyNameIndex\n",
    "  NewPropertyAverageValuesPointer = PropertyAverageValuesPointer\n",
    "  NewNpropperTime = NpropperTime\n",
    "  NewNpropperTimeStatic = NpropperTimeStatic\n",
    "  NewNpropperTimeDynamic = NpropperTimeDynamic\n",
    "\n",
    "  NewNormedInputStaticProps = NormedInputStaticProps\n",
    "  NewNormedDynamicPropertyTimeSeries = NormedDynamicPropertyTimeSeries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb8-aCUg3Ry5"
   },
   "source": [
    "###Calculate Futures\n",
    "\n",
    "Start Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1111
    },
    "id": "Mx4PkF7nkLu_",
    "outputId": "4323655f-af39-4d68-bf49-5cd3fd008cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPredictions Total 30 Basic 2 Of which futured are 2 Giving number explicit futures 28\u001b[0m\n",
      "0 Internal Property # 0 Next Cases Weight 1.0 Day 1 Explicit Futures Added \n",
      "1 Internal Property # 1 Next Deaths Weight 1.0 Day 1 Explicit Futures Added \n",
      "13 Internal Property # 0 Cases1 Weight 1.0 Day 2 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths1 Weight 1.0 Day 2 This is Explicit Future\n",
      "13 Internal Property # 0 Cases2 Weight 1.0 Day 3 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths2 Weight 1.0 Day 3 This is Explicit Future\n",
      "13 Internal Property # 0 Cases3 Weight 1.0 Day 4 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths3 Weight 1.0 Day 4 This is Explicit Future\n",
      "13 Internal Property # 0 Cases4 Weight 1.0 Day 5 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths4 Weight 1.0 Day 5 This is Explicit Future\n",
      "13 Internal Property # 0 Cases5 Weight 1.0 Day 6 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths5 Weight 1.0 Day 6 This is Explicit Future\n",
      "13 Internal Property # 0 Cases6 Weight 1.0 Day 7 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths6 Weight 1.0 Day 7 This is Explicit Future\n",
      "13 Internal Property # 0 Cases7 Weight 1.0 Day 8 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths7 Weight 1.0 Day 8 This is Explicit Future\n",
      "13 Internal Property # 0 Cases8 Weight 1.0 Day 9 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths8 Weight 1.0 Day 9 This is Explicit Future\n",
      "13 Internal Property # 0 Cases9 Weight 1.0 Day 10 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths9 Weight 1.0 Day 10 This is Explicit Future\n",
      "13 Internal Property # 0 Cases10 Weight 1.0 Day 11 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths10 Weight 1.0 Day 11 This is Explicit Future\n",
      "13 Internal Property # 0 Cases11 Weight 1.0 Day 12 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths11 Weight 1.0 Day 12 This is Explicit Future\n",
      "13 Internal Property # 0 Cases12 Weight 1.0 Day 13 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths12 Weight 1.0 Day 13 This is Explicit Future\n",
      "13 Internal Property # 0 Cases13 Weight 1.0 Day 14 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths13 Weight 1.0 Day 14 This is Explicit Future\n",
      "13 Internal Property # 0 Cases14 Weight 1.0 Day 15 This is Explicit Future\n",
      "13 Internal Property # 1 Deaths14 Weight 1.0 Day 15 This is Explicit Future\n",
      "\u001b[1m\n",
      "Basic Predicted Quantities\u001b[0m\n",
      "Cases Weight 1.0 Day= 1 Name Next Cases\n",
      "Deaths Weight 1.0 Day= 1 Name Next Deaths\n",
      "Cases Weight 1.0 Day= 2 Name Cases1\n",
      "Deaths Weight 1.0 Day= 2 Name Deaths1\n",
      "Cases Weight 1.0 Day= 3 Name Cases2\n",
      "Deaths Weight 1.0 Day= 3 Name Deaths2\n",
      "Cases Weight 1.0 Day= 4 Name Cases3\n",
      "Deaths Weight 1.0 Day= 4 Name Deaths3\n",
      "Cases Weight 1.0 Day= 5 Name Cases4\n",
      "Deaths Weight 1.0 Day= 5 Name Deaths4\n",
      "Cases Weight 1.0 Day= 6 Name Cases5\n",
      "Deaths Weight 1.0 Day= 6 Name Deaths5\n",
      "Cases Weight 1.0 Day= 7 Name Cases6\n",
      "Deaths Weight 1.0 Day= 7 Name Deaths6\n",
      "Cases Weight 1.0 Day= 8 Name Cases7\n",
      "Deaths Weight 1.0 Day= 8 Name Deaths7\n",
      "Cases Weight 1.0 Day= 9 Name Cases8\n",
      "Deaths Weight 1.0 Day= 9 Name Deaths8\n",
      "Cases Weight 1.0 Day= 10 Name Cases9\n",
      "Deaths Weight 1.0 Day= 10 Name Deaths9\n",
      "Cases Weight 1.0 Day= 11 Name Cases10\n",
      "Deaths Weight 1.0 Day= 11 Name Deaths10\n",
      "Cases Weight 1.0 Day= 12 Name Cases11\n",
      "Deaths Weight 1.0 Day= 12 Name Deaths11\n",
      "Cases Weight 1.0 Day= 13 Name Cases12\n",
      "Deaths Weight 1.0 Day= 13 Name Deaths12\n",
      "Cases Weight 1.0 Day= 14 Name Cases13\n",
      "Deaths Weight 1.0 Day= 14 Name Deaths13\n",
      "Cases Weight 1.0 Day= 15 Name Cases14\n",
      "Deaths Weight 1.0 Day= 15 Name Deaths14\n"
     ]
    }
   ],
   "source": [
    "# Order of Predictions *****************************\n",
    "# Basic \"futured\" Predictions from property dynamic arrays\n",
    "# Additional predictions without futures and NOT in property arrays including Calculated time series\n",
    "# LengthFutures predictions for first NumpredFuturedperTime predictions\n",
    "# Special predictions (temporal, positional) added later\n",
    "NpredperTime = NumpredbasicperTime + NumpredFuturedperTime*LengthFutures\n",
    "Npredperseq = NpredperTime\n",
    "Predictionbasicname = [' '] * NumpredbasicperTime\n",
    "for ipred in range(0,NumpredbasicperTime):\n",
    "  if PredSource[ipred] == 'Dynamic':\n",
    "    Predictionbasicname[ipred] = InputPropertyNames[PredSourceNumber[ipred]+NpropperTimeStatic]\n",
    "  else:\n",
    "    Predictionbasicname[ipred]= NamespredCalculated[PredSourceNumber[ipred]]\n",
    "\n",
    "TotalFutures = 0\n",
    "if NumpredFuturedperTime <= 0:\n",
    "  GenerateFutures = False\n",
    "if GenerateFutures:\n",
    "  TotalFutures = NumpredFuturedperTime * LengthFutures\n",
    "print(startbold + 'Predictions Total ' + str(Npredperseq) + ' Basic ' + str(NumpredbasicperTime) + ' Of which futured are '\n",
    "  + str(NumpredFuturedperTime) + ' Giving number explicit futures ' + str(TotalFutures) + resetfonts )\n",
    "Predictionname = [' '] * Npredperseq\n",
    "Predictionnametype = [' '] * Npredperseq\n",
    "Predictionoldvalue = np.empty(Npredperseq, dtype=int)\n",
    "Predictionnewvalue = np.empty(Npredperseq, dtype=int)\n",
    "Predictionday = np.empty(Npredperseq, dtype=int)\n",
    "PredictionAverageValuesPointer = np.empty(Npredperseq, dtype=int)\n",
    "Predictionwgt = [1.0] * Npredperseq\n",
    "for ipred in range(0,NumpredbasicperTime):\n",
    "  Predictionnametype[ipred] = PredSource[ipred]\n",
    "  Predictionoldvalue[ipred] = PredSourceNumber[ipred]\n",
    "  Predictionnewvalue[ipred] = ipred\n",
    "  if PredSource[ipred] == 'Dynamic':\n",
    "    PredictionAverageValuesPointer[ipred] = NpropperTimeStatic + Predictionoldvalue[ipred]\n",
    "  else:\n",
    "    PredictionAverageValuesPointer[ipred] = NpropperTime + PredSourceNumber[ipred]\n",
    "  Predictionwgt[ipred] = 1.0\n",
    "  Predictionday[ipred] = 1\n",
    "  extrastring =''\n",
    "  Predictionname[ipred] = 'Next ' + Predictionbasicname[ipred]\n",
    "  if FuturedPred[ipred] >= 0:\n",
    "    extrastring = ' Explicit Futures Added '   \n",
    "  print(str(ipred)+  ' Internal Property # ' + str(PredictionAverageValuesPointer[ipred]) + ' ' + Predictionname[ipred]\n",
    "      + ' Weight ' + str(round(Predictionwgt[ipred],3)) + ' Day ' + str(Predictionday[ipred]) + extrastring )\n",
    "\n",
    "for ifuture in range(0,LengthFutures):\n",
    "  for ipred in range(0,NumpredbasicperTime):\n",
    "    if FuturedPred[ipred] >= 0:\n",
    "      FuturedPosition = NumpredbasicperTime + NumpredFuturedperTime*ifuture + FuturedPred[ipred]\n",
    "      Predictionname[FuturedPosition] = Predictionbasicname[ipred] + Futures[ifuture].name\n",
    "      Predictionday[FuturedPosition] = Futures[ifuture].days[0]\n",
    "      Predictionwgt[FuturedPosition] = Futures[ifuture].classweight\n",
    "      Predictionnametype[FuturedPosition] = Predictionnametype[ipred]\n",
    "      Predictionoldvalue[FuturedPosition] = Predictionoldvalue[ipred]\n",
    "      Predictionnewvalue[FuturedPosition] = Predictionnewvalue[ipred]\n",
    "      PredictionAverageValuesPointer[FuturedPosition] = NpropperTimeStatic + PredictionAverageValuesPointer[ipred]\n",
    "      print(str(iprop)+  ' Internal Property # ' + str(PredictionAverageValuesPointer[FuturedPosition]) + ' ' + \n",
    "        Predictionname[FuturedPosition] + ' Weight ' + str(round(Predictionwgt[FuturedPosition],3))\n",
    "         + ' Day ' + str(Predictionday[FuturedPosition]) + ' This is Explicit Future' )\n",
    "\n",
    "Predictionnamelookup = {}\n",
    "print(startbold + '\\nBasic Predicted Quantities' + resetfonts)\n",
    "for i in range(0,Npredperseq):\n",
    "  Predictionnamelookup[Predictionname[i]] = i\n",
    "\n",
    "  iprop = Predictionnewvalue[i]\n",
    "  line = Predictionbasicname[iprop]\n",
    "  line += ' Weight ' + str(round(Predictionwgt[i],4))\n",
    "  if (iprop < NumpredFuturedperTime) or (iprop >= NumpredbasicperTime):\n",
    "    line += ' Day= ' + str(Predictionday[i])\n",
    "    line += ' Name ' + Predictionname[i]\n",
    "  print(line)\n",
    "\n",
    "  # Note that only Predictionwgt and Predictionname defined for later addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4V0SXGd-nVfX"
   },
   "source": [
    "### Set up Predictions \n",
    "\n",
    "first for time arrays; we will extend to sequences next. Sequences include the predictions for final time in sequence.\n",
    "\n",
    "This is prediction for sequence ending one day before the labelling time index. So sequence must end one unit before last time value\n",
    "\n",
    "Note this is  \"pure forecast\" which are of quantities used in driving data allowing us to iitialize prediction to input\n",
    "\n",
    "NaN represents non existent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kXMefJVkkFL7",
    "outputId": "a021570b-2499-4835-c3dd-661d607dc732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFutures Added: Predictions set from inputs OK 13456 Veto at end 105 Veto at start 0 Times number of locations\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if PredictionsfromInputs:\n",
    "  InputPredictionsbyTime = np.zeros([Num_Time, Nloc, Npredperseq], dtype = np.float32)\n",
    "  for ipred in range (0,NumpredbasicperTime):\n",
    "    if Predictionnametype[ipred] == 'Dynamic':\n",
    "      InputPredictionsbyTime[:,:,ipred] = NormedDynamicPropertyTimeSeries[:,:,Predictionoldvalue[ipred]]\n",
    "    else:\n",
    "      InputPredictionsbyTime[:,:,ipred] = NormedCalculatedTimeSeries[:,:,Predictionoldvalue[ipred]]\n",
    "\n",
    "  # Add Futures based on Futured properties\n",
    "  if LengthFutures > 0:\n",
    "    NaNall = np.full([Nloc],NaN,dtype = np.float32)\n",
    "    daystartveto = 0\n",
    "    atendveto = 0\n",
    "    allok = NumpredbasicperTime \n",
    "    for ifuture in range(0,LengthFutures):\n",
    "      for itime in range(0,Num_Time):\n",
    "        ActualTime = itime+Futures[ifuture].days[0]-1\n",
    "        if ActualTime >= Num_Time:\n",
    "          for ipred in range (0,NumpredbasicperTime):\n",
    "            Putithere = FuturedPred[ipred]\n",
    "            if Putithere >=0:\n",
    "              InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] = NaNall\n",
    "          atendveto +=1\n",
    "        elif Usedaystart and (itime < Futures[ifuture].daystart):\n",
    "          for ipred in range (0,NumpredbasicperTime):\n",
    "            Putithere = FuturedPred[ipred]\n",
    "            if Putithere >=0:\n",
    "              InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] = NaNall \n",
    "          daystartveto +=1     \n",
    "        else:\n",
    "          for ipred in range (0,NumpredbasicperTime):\n",
    "            Putithere = FuturedPred[ipred]\n",
    "            if Putithere >=0:\n",
    "              if Predictionnametype[ipred] == 'Dynamic':\n",
    "                InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] \\\n",
    "                  = NormedDynamicPropertyTimeSeries[ActualTime,:,Predictionoldvalue[ipred]]\n",
    "              else:\n",
    "                InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] \\\n",
    "                  = NormedCalculatedTimeSeries[ActualTime,:,Predictionoldvalue[ipred]]\n",
    "          allok += NumpredFuturedperTime\n",
    "    print(startbold + 'Futures Added: Predictions set from inputs OK ' +str(allok) + \n",
    "          ' Veto at end ' + str(atendveto) +  ' Veto at start ' + str(daystartveto) + ' Times number of locations' + resetfonts)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlGIiaIWIrYm"
   },
   "source": [
    "### Clean-up Input quantities#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "0Gq6G5JjIw_g",
    "outputId": "4cd803be-e7fe-42a7-c722-30133777ca61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static Properties\n",
      " None Defined\n",
      "Dynamic Properties\n",
      " is NaN  0  percent  0.0  not NaN  539728\n"
     ]
    }
   ],
   "source": [
    "def checkNaN(y):\n",
    "  countNaN = 0\n",
    "  countnotNaN = 0\n",
    "  ctprt = 0\n",
    "  if y is None:\n",
    "    return\n",
    "  if len(y.shape) == 2:\n",
    "    for i in range(0,y.shape[0]):\n",
    "        for j in range(0,y.shape[1]):\n",
    "            if(np.math.isnan(y[i,j])):\n",
    "                countNaN += 1\n",
    "            else:\n",
    "                countnotNaN += 1\n",
    "  else:\n",
    "    for i in range(0,y.shape[0]):\n",
    "      for j in range(0,y.shape[1]):\n",
    "        for k in range(0,y.shape[2]):\n",
    "          if(np.math.isnan(y[i,j,k])):\n",
    "              countNaN += 1\n",
    "              ctprt += 1\n",
    "              print(str(i) + ' ' + str(j) + ' ' + str(k))\n",
    "              if ctprt > 10:\n",
    "                sys.exit(0)\n",
    "          else:\n",
    "              countnotNaN += 1\n",
    "\n",
    "  percent = (100.0*countNaN)/(countNaN + countnotNaN)\n",
    "  print(' is NaN ',str(countNaN),' percent ',str(round(percent,2)),' not NaN ', str(countnotNaN))\n",
    "\n",
    "# Clean-up Input Source\n",
    "if len(InputSource) > 0: \n",
    "  PropertyNameIndex = NewPropertyNameIndex\n",
    "  NewPropertyNameIndex = None\n",
    "  PropertyAverageValuesPointer = NewPropertyAverageValuesPointer\n",
    "  NewPropertyAverageValuesPointer = None\n",
    "\n",
    "  NormedInputStaticProps = NewNormedInputStaticProps\n",
    "  NewNormedInputStaticProps = None\n",
    "  NormedDynamicPropertyTimeSeries = NewNormedDynamicPropertyTimeSeries\n",
    "  NewNormedDynamicPropertyTimeSeries = None\n",
    "\n",
    "  NpropperTime = NewNpropperTime\n",
    "  NpropperTimeStatic = NewNpropperTimeStatic\n",
    "  NpropperTimeDynamic = NewNpropperTimeDynamic\n",
    "\n",
    "print('Static Properties')\n",
    "if NpropperTimeStatic > 0 :\n",
    "  checkNaN(NormedInputStaticProps)\n",
    "else:\n",
    "  print(' None Defined')\n",
    "print('Dynamic Properties')\n",
    "\n",
    "checkNaN(NormedDynamicPropertyTimeSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eRJTbE7ypBX"
   },
   "source": [
    "###Covid Data: Agree on Tseq Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "xJLbkWv6xSoV",
    "outputId": "91cf8c84-6d11-4f32-efcc-5ec9ece5cd3e"
   },
   "outputs": [],
   "source": [
    "if ReadAugust2020Covid or RereadMay2020:\n",
    "  Tseq = 9\n",
    "if ReadJan2021Covid or ReadAugust2020Covid or ReadApril2021Covid or ReadDecember2021:\n",
    "  Tseq = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZyZD9mEio0z"
   },
   "source": [
    "###Setup Sequences and TFTConversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "kTQVBsqmix8O",
    "outputId": "ff6d8e01-3af7-4679-d3ed-8bbd291fb5ee"
   },
   "outputs": [],
   "source": [
    "Num_SeqExtraUsed = Tseq-1\n",
    "Num_Seq = Num_Time - Tseq\n",
    "Num_SeqPred = Num_Seq\n",
    "TSeqPred = Tseq\n",
    "TFTExtraTimes = 0\n",
    "Num_TimeTFT = Num_Time\n",
    "if TFTConversion:\n",
    "  TFTExtraTimes = 1 + LengthFutures\n",
    "  SymbolicWindows = True\n",
    "  Num_SeqExtraUsed = Tseq # as last position needed in input\n",
    "  Num_TimeTFT = Num_Time +TFTExtraTimes\n",
    "  Num_SeqPred = Num_Seq\n",
    "  TseqPred = Tseq\n",
    "\n",
    "# If SymbolicWindows, sequences are not made but we use same array with that dimension (RawInputSeqDimension) set to 1\n",
    "# reshape can get rid of this irrelevant dimension\n",
    "# Predictions and Input Properties are associated with sequence number which is first time value used in sequence\n",
    "# if SymbolicWindows false then sequences are labelled by sequence # and contain time values from sequence # to sequence# + Tseq-1\n",
    "# if SymbolicWindows True then sequences are labelled by time # and contain one value. They are displaced by Tseq\n",
    "# If TFT Inputs and Predictions do NOT differ by Tseq\n",
    "# Num_SeqExtra extra positions in RawInputSequencesTOT for Symbolic windows True as need to store full window\n",
    "# TFTExtraTimes are extra times\n",
    "RawInputSeqDimension = Tseq\n",
    "Num_SeqExtra = 0\n",
    "if SymbolicWindows:\n",
    "  RawInputSeqDimension = 1\n",
    "  Num_SeqExtra =  Num_SeqExtraUsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYgeVR4S11pc"
   },
   "source": [
    "###Generate Sequences from Time labelled data \n",
    "given Tseq set above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "KUnmDWwS3Iai",
    "outputId": "646990e5-25db-44d1-9e2c-34c16f627c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSequences set from Time values Num Seq 475 Time 488\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/1337979039.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  UseProperties = np.full(NpropperTime, True, dtype=np.bool)\n",
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/1337979039.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  IndexintoPropertyArrays = np.empty(NpropperTime, dtype = np.int)\n"
     ]
    }
   ],
   "source": [
    "if GenerateSequences:\n",
    "  UseProperties = np.full(NpropperTime, True, dtype=np.bool)\n",
    "  if Hydrology:\n",
    "    UseProperties[NpropperTime-1] = False\n",
    "  Npropperseq = 0\n",
    "  IndexintoPropertyArrays = np.empty(NpropperTime, dtype = np.int)\n",
    "  for iprop in range(0,NpropperTime):\n",
    "    if UseProperties[iprop]:\n",
    "      IndexintoPropertyArrays[Npropperseq] = iprop\n",
    "      Npropperseq +=1\n",
    "  RawInputSequences = np.zeros([Num_Seq + Num_SeqExtra, Nloc, RawInputSeqDimension, Npropperseq], dtype =np.float32)\n",
    "  RawInputPredictions = np.zeros([Num_SeqPred, Nloc, Npredperseq], dtype =np.float32)\n",
    "\n",
    "  locationarray = np.empty(Nloc, dtype=np.float32)\n",
    "  for iseq in range(0,Num_Seq  + Num_SeqExtra):\n",
    "    for windowposition in range(0,RawInputSeqDimension):\n",
    "      itime = iseq + windowposition\n",
    "      for usedproperty  in range (0,Npropperseq):\n",
    "        iprop = IndexintoPropertyArrays[usedproperty]\n",
    "        if iprop>=NpropperTimeStatic:\n",
    "          jprop =iprop-NpropperTimeStatic\n",
    "          locationarray = NormedDynamicPropertyTimeSeries[itime,:,jprop]\n",
    "        else:\n",
    "          locationarray = NormedInputStaticProps[:,iprop]\n",
    "        RawInputSequences[iseq,:,windowposition,usedproperty] = locationarray\n",
    "    if iseq < Num_SeqPred:\n",
    "      RawInputPredictions[iseq,:,:] = InputPredictionsbyTime[iseq+TseqPred,:,:]\n",
    "  print(startbold + 'Sequences set from Time values Num Seq ' + str(Num_SeqPred) + ' Time ' +str(Num_Time) + resetfonts)  \n",
    "\n",
    "NormedInputTimeSeries = None\n",
    "NormedDynamicPropertyTimeSeries = None\n",
    "if GarbageCollect:\n",
    "  gc.collect()\n",
    "\n",
    "GlobalTimeMask = np.empty([1,1,1,Tseq,Tseq],dtype =np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lprQwdZFby5Y"
   },
   "source": [
    "### Define Possible Temporal and Spatial Positional Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Tu9Oy46Nb4LO",
    "outputId": "2fc82c80-052b-4541-de53-206818dfd780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Time Units 488 Day\n"
     ]
    }
   ],
   "source": [
    "def LinearLocationEncoding(TotalLoc):\n",
    "  linear = np.empty(TotalLoc, dtype=float)\n",
    "  for i in range(0,TotalLoc):\n",
    "    linear[i] = float(i)/float(TotalLoc)\n",
    "  return linear\n",
    "\n",
    "def LinearTimeEncoding(Dateslisted):\n",
    "  Firstdate = Dateslisted[0]\n",
    "  numtofind = len(Dateslisted)\n",
    "  dayrange = (Dateslisted[numtofind-1]-Firstdate).days + 1\n",
    "  linear = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    linear[i] = float((Dateslisted[i]-Firstdate).days)/float(dayrange)\n",
    "  return linear\n",
    "\n",
    "def P2TimeEncoding(numtofind):\n",
    "  P2 = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    x =  -1 + 2.0*i/(numtofind-1)\n",
    "    P2[i] = 0.5*(3*x*x-1)\n",
    "  return P2\n",
    "\n",
    "def P3TimeEncoding(numtofind):\n",
    "  P3 = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    x =  -1 + 2.0*i/(numtofind-1)\n",
    "    P3[i] = 0.5*(5*x*x-3)*x\n",
    "  return P3\n",
    "\n",
    "def P4TimeEncoding(numtofind):\n",
    "  P4 = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    x =  -1 + 2.0*i/(numtofind-1)\n",
    "    P4[i] = 0.125*(35*x*x*x*x - 30*x*x + 3)\n",
    "  return P4\n",
    "\n",
    "def WeeklyTimeEncoding(Dateslisted):\n",
    "  numtofind = len(Dateslisted)\n",
    "  costheta = np.empty(numtofind, dtype=float)\n",
    "  sintheta = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    j = Dateslisted[i].date().weekday()\n",
    "    theta = float(j)*2.0*math.pi/7.0\n",
    "    costheta[i] = math.cos(theta)\n",
    "    sintheta[i] = math.sin(theta)\n",
    "  return costheta, sintheta\n",
    "\n",
    "def AnnualTimeEncoding(Dateslisted): \n",
    "  numtofind = len(Dateslisted)\n",
    "  costheta = np.empty(numtofind, dtype=float)\n",
    "  sintheta = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    runningdate = Dateslisted[i]\n",
    "    year = runningdate.year\n",
    "    datebeginyear = datetime(year, 1, 1)\n",
    "    displacement = (runningdate-datebeginyear).days\n",
    "    daysinyear = (datetime(year,12,31)-datebeginyear).days+1\n",
    "    if displacement >= daysinyear:\n",
    "      printexit(\"EXIT Bad Date \", runningdate)\n",
    "    theta = float(displacement)*2.0*math.pi/float(daysinyear)\n",
    "    costheta[i] = math.cos(theta)\n",
    "    sintheta[i] = math.sin(theta)\n",
    "  return costheta, sintheta\n",
    "\n",
    "def ReturnEncoding(numtofind,Typeindex, Typevalue):\n",
    "  Dummy = costheta = np.empty(0, dtype=float)\n",
    "  if Typeindex == 1:\n",
    "    return LinearoverLocationEncoding, Dummy, ('LinearSpace',0.,1.0,0.5,0.2887), ('Dummy',0.,0.,0.,0.)\n",
    "  if Typeindex == 2:\n",
    "    if Dailyunit == 1:\n",
    "      return CosWeeklytimeEncoding, SinWeeklytimeEncoding, ('CosWeekly',-1.0, 1.0, 0.,0.7071), ('SinWeekly',-1.0, 1.0, 0.,0.7071)\n",
    "    else:\n",
    "      return Dummy, Dummy, ('Dummy',0.,0.,0.,0.), ('Dummy',0.,0.,0.,0.)\n",
    "  if Typeindex == 3:\n",
    "    return CosAnnualtimeEncoding, SinAnnualtimeEncoding, ('CosAnnual',-1.0, 1.0, 0.,0.7071), ('SinAnnual',-1.0, 1.0, 0.,0.7071)\n",
    "  if Typeindex == 4:\n",
    "    if Typevalue == 0:\n",
    "      ConstArray = np.full(numtofind,0.5, dtype = float)\n",
    "      return ConstArray, Dummy, ('Constant',0.5,0.5,0.5,0.0), ('Dummy',0.,0.,0.,0.)\n",
    "    if Typevalue == 1:\n",
    "      return LinearovertimeEncoding, Dummy, ('LinearTime',0., 1.0, 0.5,0.2887), ('Dummy',0.,0.,0.,0.)\n",
    "    if Typevalue == 2:\n",
    "      return P2TimeEncoding(numtofind), Dummy, ('P2-Time',-1.0, 1.0, 0.,0.4472), ('Dummy',0.,0.,0.,0.)\n",
    "    if Typevalue == 3:\n",
    "      return P3TimeEncoding(numtofind), Dummy, ('P3-Time',-1.0, 1.0, 0.,0.3780), ('Dummy',0.,0.,0.,0.)\n",
    "    if Typevalue == 4:\n",
    "      return P4TimeEncoding(numtofind), Dummy, ('P4-Time',-1.0, 1.0, 0.,0.3333), ('Dummy',0.,0.,0.,0.)\n",
    "  if Typeindex == 5:\n",
    "      costheta = np.empty(numtofind, dtype=float)\n",
    "      sintheta = np.empty(numtofind, dtype=float)\n",
    "      j = 0\n",
    "      for i in range(0,numtofind):\n",
    "        theta = float(j)*2.0*math.pi/Typevalue\n",
    "        costheta[i] = math.cos(theta)\n",
    "        sintheta[i] = math.sin(theta)\n",
    "        j += 1\n",
    "        if j >= Typevalue:\n",
    "          j = 0\n",
    "      return costheta, sintheta,('Cos '+str(Typevalue)+ ' Len',-1.0, 1.0,0.,0.7071), ('Sin '+str(Typevalue)+ ' Len',-1.0, 1.0,0.,0.7071)\n",
    "\n",
    "# Dates set up in Python datetime format as Python LISTS\n",
    "# All encodings are Numpy arrays\n",
    "print(\"Total number of Time Units \" + str(NumberofTimeunits) + ' ' + TimeIntervalUnitName)\n",
    "if NumberofTimeunits != (Num_Seq + Tseq):\n",
    "  printexit(\"EXIT Wrong Number of Time Units \" + str(Num_Seq + Tseq))\n",
    "\n",
    "Dateslist = []\n",
    "for i in range(0,NumberofTimeunits + TFTExtraTimes):\n",
    "  Dateslist.append(InitialDate+timedelta(days=i*Dailyunit))\n",
    "\n",
    "LinearoverLocationEncoding = LinearLocationEncoding(Nloc)\n",
    "LinearovertimeEncoding = LinearTimeEncoding(Dateslist)\n",
    "\n",
    "if Dailyunit == 1:\n",
    "  CosWeeklytimeEncoding, SinWeeklytimeEncoding = WeeklyTimeEncoding(Dateslist)\n",
    "CosAnnualtimeEncoding, SinAnnualtimeEncoding = AnnualTimeEncoding(Dateslist)\n",
    "\n",
    "\n",
    "# Encodings\n",
    "\n",
    "# linearlocationposition\n",
    "# Supported Time Dependent Probes that can be in properties and/or predictions\n",
    "# Special\n",
    "# Annual\n",
    "# Weekly\n",
    "# \n",
    "# Top Down\n",
    "# TD0 Constant at 0.5\n",
    "# TD1 Linear from 0 to 1\n",
    "# TD2 P2(x) where x goes from -1 to 1 as time goes from start to end\n",
    "# \n",
    "# Bottom Up\n",
    "# n-way Cos and sin theta where n = 4 7 8 16 24 32\n",
    "\n",
    "EncodingTypes = {'Spatial':1, 'Weekly':2,'Annual':3,'TopDown':4,'BottomUp':5}\n",
    "\n",
    "PropIndex =[]\n",
    "PropNameMeanStd = []\n",
    "PropMeanStd = []\n",
    "PropArray = []\n",
    "PropPosition = []\n",
    "\n",
    "PredIndex =[]\n",
    "PredNameMeanStd = []\n",
    "PredArray = []\n",
    "PredPosition = []\n",
    "\n",
    "Numberpropaddons = 0\n",
    "propposition = Npropperseq\n",
    "Numberpredaddons = 0\n",
    "predposition = Npredperseq\n",
    "\n",
    "numprop = len(PropTypes)\n",
    "if numprop != len(PropValues):\n",
    "  printexit('Error in property addons ' + str(numprop) + ' ' + str(len(PropValues)))\n",
    "for newpropinlist in range(0,numprop):\n",
    "  Typeindex = EncodingTypes[PropTypes[newpropinlist]]\n",
    "  a,b,c,d = ReturnEncoding(Num_Time + TFTExtraTimes,Typeindex, PropValues[newpropinlist])\n",
    "  if c[0] != 'Dummy':\n",
    "    PropIndex.append(Typeindex)\n",
    "    PropNameMeanStd.append(c)\n",
    "    InputPropertyNames.append(c[0])\n",
    "    PropArray.append(a)\n",
    "    PropPosition.append(propposition)\n",
    "    propposition += 1\n",
    "    Numberpropaddons += 1\n",
    "    line = ' '\n",
    "    for ipr in range(0,20):\n",
    "      line += str(round(a[ipr],4)) + ' '\n",
    "#    print('c'+line)\n",
    "  if d[0] != 'Dummy':\n",
    "    PropIndex.append(Typeindex)\n",
    "    PropNameMeanStd.append(d)\n",
    "    InputPropertyNames.append(d[0])\n",
    "    PropArray.append(b)\n",
    "    PropPosition.append(propposition)\n",
    "    propposition += 1\n",
    "    Numberpropaddons += 1\n",
    "    line = ' '\n",
    "    for ipr in range(0,20):\n",
    "      line += str(round(b[ipr],4)) + ' '\n",
    "#    print('d'+line)\n",
    "\n",
    "numpred = len(PredTypes)\n",
    "if numpred != len(PredValues):\n",
    "  printexit('Error in prediction addons ' + str(numpred) + ' ' + str(len(PredValues)))\n",
    "for newpredinlist in range(0,numpred):\n",
    "  Typeindex = EncodingTypes[PredTypes[newpredinlist]]\n",
    "  a,b,c,d = ReturnEncoding(Num_Time + TFTExtraTimes,Typeindex, PredValues[newpredinlist])\n",
    "  if c[0] != 'Dummy':\n",
    "    PredIndex.append(Typeindex)\n",
    "    PredNameMeanStd.append(c)\n",
    "    PredArray.append(a)\n",
    "    Predictionname.append(c[0])\n",
    "    Predictionnamelookup[c] = predposition\n",
    "    PredPosition.append(predposition)\n",
    "    predposition += 1\n",
    "    Numberpredaddons += 1\n",
    "    Predictionwgt.append(0.25)\n",
    "  if d[0] != 'Dummy':\n",
    "    PredIndex.append(Typeindex)\n",
    "    PredNameMeanStd.append(d)\n",
    "    PredArray.append(b)\n",
    "    Predictionname.append(d[0])\n",
    "    Predictionnamelookup[d[0]] = predposition\n",
    "    PredPosition.append(predposition)\n",
    "    predposition += 1\n",
    "    Numberpredaddons += 1\n",
    "    Predictionwgt.append(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANMrg0vjoPxS"
   },
   "source": [
    "### Add in Temporal and Spatial Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1009
    },
    "id": "I977Ffv_obEC",
    "outputId": "35d04578-0f0c-4d8d-e70a-5ec2a7bd7af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total storage 3392576.0 Bytes\n",
      "Time and Space encoding added to input and predictions\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n",
      "['Cases', 'Deaths', 'Age Distribution', 'Air Pollution', 'Comorbidities', 'Health Disparities', 'Mobility', 'Residential Density', 'Disease Spread', 'Social Distancing', 'Testing', 'Transmissible Cases', 'VaccinationOneDose', 'Vaccination', 'LinearSpace', 'Constant', 'LinearTime', 'P2-Time', 'P3-Time', 'P4-Time', 'CosWeekly', 'SinWeekly']\n",
      "Property 0 Cases\n",
      "Property 1 Deaths\n",
      "Property 2 Age Distribution\n",
      "Property 3 Air Pollution\n",
      "Property 4 Comorbidities\n",
      "Property 5 Health Disparities\n",
      "Property 6 Mobility\n",
      "Property 7 Residential Density\n",
      "Property 8 Disease Spread\n",
      "Property 9 Social Distancing\n",
      "Property 10 Testing\n",
      "Property 11 Transmissible Cases\n",
      "Property 12 VaccinationOneDose\n",
      "Property 13 Vaccination\n",
      "Property 14 LinearSpace\n",
      "Property 15 Constant\n",
      "Property 16 LinearTime\n",
      "Property 17 P2-Time\n",
      "Property 18 P3-Time\n",
      "Property 19 P4-Time\n",
      "Property 20 CosWeekly\n",
      "Property 21 SinWeekly\n",
      "Prediction 0 Next Cases 1.0\n",
      "Prediction 1 Next Deaths 1.0\n",
      "Prediction 2 Cases1 1.0\n",
      "Prediction 3 Deaths1 1.0\n",
      "Prediction 4 Cases2 1.0\n",
      "Prediction 5 Deaths2 1.0\n",
      "Prediction 6 Cases3 1.0\n",
      "Prediction 7 Deaths3 1.0\n",
      "Prediction 8 Cases4 1.0\n",
      "Prediction 9 Deaths4 1.0\n",
      "Prediction 10 Cases5 1.0\n",
      "Prediction 11 Deaths5 1.0\n",
      "Prediction 12 Cases6 1.0\n",
      "Prediction 13 Deaths6 1.0\n",
      "Prediction 14 Cases7 1.0\n",
      "Prediction 15 Deaths7 1.0\n",
      "Prediction 16 Cases8 1.0\n",
      "Prediction 17 Deaths8 1.0\n",
      "Prediction 18 Cases9 1.0\n",
      "Prediction 19 Deaths9 1.0\n",
      "Prediction 20 Cases10 1.0\n",
      "Prediction 21 Deaths10 1.0\n",
      "Prediction 22 Cases11 1.0\n",
      "Prediction 23 Deaths11 1.0\n",
      "Prediction 24 Cases12 1.0\n",
      "Prediction 25 Deaths12 1.0\n",
      "Prediction 26 Cases13 1.0\n",
      "Prediction 27 Deaths13 1.0\n",
      "Prediction 28 Cases14 1.0\n",
      "Prediction 29 Deaths14 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/2244957357.py:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  NewQuantityTakeroot = np.full(NewNpropperTimeMAX,1,dtype=np.int) # All new ones aare 1 and are set here\n",
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/2244957357.py:47: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  flsize = np.float(Num_Seq  + Num_SeqExtra)*np.float(Nloc)*np.float(RawInputSeqDimension)* np.float(NpropperseqTOT)* 4.0\n"
     ]
    }
   ],
   "source": [
    "def SetNewAverages(InputList): # name min max mean std\n",
    "  results = np.empty(7, dtype = np.float32)\n",
    "  results[0] = InputList[1]\n",
    "  results[1] = InputList[2]\n",
    "  results[2] = 1.0\n",
    "  results[3] = InputList[3]\n",
    "  results[4] = InputList[4]\n",
    "  results[5] = InputList[3]\n",
    "  results[6] = InputList[4]\n",
    "  return results\n",
    "\n",
    "\n",
    "NpropperseqTOT = Npropperseq + Numberpropaddons\n",
    "\n",
    "# These include both Property and Prediction Variables\n",
    "NpropperTimeMAX =len(QuantityTakeroot)\n",
    "NewNpropperTimeMAX = NpropperTimeMAX + Numberpropaddons + Numberpredaddons  \n",
    "NewQuantityStatistics = np.zeros([NewNpropperTimeMAX,7], dtype=np.float32)\n",
    "NewQuantityTakeroot = np.full(NewNpropperTimeMAX,1,dtype=np.int) # All new ones aare 1 and are set here\n",
    "NewQuantityStatistics[0:NpropperTimeMAX,:] = QuantityStatistics[0:NpropperTimeMAX,:]\n",
    "NewQuantityTakeroot[0:NpropperTimeMAX] = QuantityTakeroot[0:NpropperTimeMAX]\n",
    "\n",
    "# Lookup for property names\n",
    "NewPropertyNameIndex = np.empty(NpropperseqTOT, dtype = np.int32)\n",
    "NumberofNames = len(InputPropertyNames)-Numberpropaddons\n",
    "NewPropertyNameIndex[0:Npropperseq] = PropertyNameIndex[0:Npropperseq]\n",
    "\n",
    "NewPropertyAverageValuesPointer = np.empty(NpropperseqTOT, dtype = np.int32)\n",
    "NewPropertyAverageValuesPointer[0:Npropperseq] = PropertyAverageValuesPointer[0:Npropperseq]\n",
    "\n",
    "for propaddons in range(0,Numberpropaddons):\n",
    "  NewPropertyNameIndex[Npropperseq+propaddons] = NumberofNames + propaddons\n",
    "  NewPropertyAverageValuesPointer[Npropperseq+propaddons] = NpropperTimeMAX + propaddons\n",
    "  NewQuantityStatistics[NpropperTimeMAX + propaddons,:] = SetNewAverages(PropNameMeanStd[propaddons])\n",
    "\n",
    "# Set extra Predictions metadata for Sequences\n",
    "NpredperseqTOT = Npredperseq + Numberpredaddons\n",
    "\n",
    "NewPredictionAverageValuesPointer = np.empty(NpredperseqTOT, dtype = np.int32)\n",
    "NewPredictionAverageValuesPointer[0:Npredperseq] = PredictionAverageValuesPointer[0:Npredperseq]\n",
    "\n",
    "for predaddons in range(0,Numberpredaddons):\n",
    "  NewPredictionAverageValuesPointer[Npredperseq +predaddons] = NpropperTimeMAX + +Numberpropaddons + predaddons\n",
    "  NewQuantityStatistics[NpropperTimeMAX + Numberpropaddons + predaddons,:] = SetNewAverages(PredNameMeanStd[predaddons])\n",
    "\n",
    "RawInputSequencesTOT = np.empty([Num_Seq  + Num_SeqExtra + TFTExtraTimes, Nloc, RawInputSeqDimension, NpropperseqTOT], dtype =np.float32)\n",
    "flsize = np.float(Num_Seq  + Num_SeqExtra)*np.float(Nloc)*np.float(RawInputSeqDimension)* np.float(NpropperseqTOT)* 4.0\n",
    "print('Total storage ' +str(round(flsize,0)) + ' Bytes')\n",
    "\n",
    "for i in range(0,Num_Seq  + Num_SeqExtra):\n",
    "  for iprop in range(0,Npropperseq):\n",
    "    RawInputSequencesTOT[i,:,:,iprop] = RawInputSequences[i,:,:,iprop]\n",
    "for i in range(Num_Seq  + Num_SeqExtra,Num_Seq  + Num_SeqExtra + TFTExtraTimes):\n",
    "  for iprop in range(0,Npropperseq):\n",
    "    RawInputSequencesTOT[i,:,:,iprop] = NaN\n",
    "\n",
    "for i in range(0,Num_Seq  + Num_SeqExtra + TFTExtraTimes):\n",
    "    for k in range(0,RawInputSeqDimension):\n",
    "      for iprop in range(0, Numberpropaddons):\n",
    "        if PropIndex[iprop] == 1:\n",
    "          continue\n",
    "        RawInputSequencesTOT[i,:,k,PropPosition[iprop]] = PropArray[iprop][i+k]\n",
    "\n",
    "for iprop in range(0, Numberpropaddons):\n",
    "  if PropIndex[iprop] == 1:\n",
    "    for j in range(0,Nloc):       \n",
    "        RawInputSequencesTOT[:,j,:,PropPosition[iprop]] = PropArray[iprop][j]\n",
    "\n",
    "# Set extra Predictions for Sequences\n",
    "RawInputPredictionsTOT = np.empty([Num_SeqPred + TFTExtraTimes, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "\n",
    "for i in range(0,Num_SeqPred):\n",
    "  for ipred in range(0,Npredperseq):\n",
    "    RawInputPredictionsTOT[i,:,ipred] = RawInputPredictions[i,:,ipred]\n",
    "for i in range(Num_SeqPred, Num_SeqPred + TFTExtraTimes):\n",
    "  for ipred in range(0,Npredperseq):\n",
    "    RawInputPredictionsTOT[i,:,ipred] = NaN\n",
    "\n",
    "for i in range(0,Num_SeqPred + TFTExtraTimes):\n",
    "  for ipred in range(0, Numberpredaddons):\n",
    "    if PredIndex[ipred] == 1:\n",
    "      continue\n",
    "    actualarray = PredArray[ipred]\n",
    "    RawInputPredictionsTOT[i,:,PredPosition[ipred]] = actualarray[i+TseqPred]\n",
    "\n",
    "for ipred in range(0, Numberpredaddons):\n",
    "  if PredIndex[ipred] == 1:\n",
    "    for j in range(0,Nloc):\n",
    "      RawInputPredictionsTOT[:,j,PredPosition[ipred]] = PredArray[ipred][j]\n",
    "\n",
    "PropertyNameIndex  = None\n",
    "PropertyNameIndex = NewPropertyNameIndex\n",
    "QuantityStatistics = None\n",
    "QuantityStatistics = NewQuantityStatistics\n",
    "QuantityTakeroot = None\n",
    "QuantityTakeroot = NewQuantityTakeroot\n",
    "PropertyAverageValuesPointer = None\n",
    "PropertyAverageValuesPointer = NewPropertyAverageValuesPointer\n",
    "PredictionAverageValuesPointer = None\n",
    "PredictionAverageValuesPointer = NewPredictionAverageValuesPointer\n",
    "\n",
    "print('Time and Space encoding added to input and predictions')\n",
    "\n",
    "if SymbolicWindows:\n",
    "  SymbolicInputSequencesTOT = np.empty([Num_Seq, Nloc], dtype =np.int32) # This is sequences\n",
    "  for iseq in range(0,Num_Seq):\n",
    "    for iloc in range(0,Nloc):\n",
    "      SymbolicInputSequencesTOT[iseq,iloc] = np.left_shift(iseq,16) + iloc\n",
    "  ReshapedSequencesTOT = np.transpose(RawInputSequencesTOT,(1,0,3,2))\n",
    "  ReshapedSequencesTOT = np.reshape(ReshapedSequencesTOT,(Nloc,Num_Seq  + Num_SeqExtra + TFTExtraTimes,NpropperseqTOT))\n",
    "\n",
    "# To calculate masks (identical to Symbolic windows)\n",
    "SpacetimeforMask = np.empty([Num_Seq, Nloc], dtype =np.int32)\n",
    "for iseq in range(0,Num_Seq):\n",
    "  for iloc in range(0,Nloc):\n",
    "    SpacetimeforMask[iseq,iloc] = np.left_shift(iseq,16) + iloc\n",
    "    \n",
    "print(PropertyNameIndex)\n",
    "print(InputPropertyNames)\n",
    "for iprop in range(0,NpropperseqTOT):\n",
    "  print('Property ' + str(iprop) + ' ' + InputPropertyNames[PropertyNameIndex[iprop]])\n",
    "for ipred in range(0,NpredperseqTOT):\n",
    "  print('Prediction ' + str(ipred) + ' ' + Predictionname[ipred] + ' ' + str(round(Predictionwgt[ipred],3)))\n",
    "\n",
    "\n",
    "\n",
    "RawInputPredictions = None\n",
    "RawInputSequences = None\n",
    "if SymbolicWindows:\n",
    "  RawInputSequencesTOT = None\n",
    "if GarbageCollect:\n",
    "  gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0FxRdZa81_Z"
   },
   "source": [
    "### Set up NNSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tdFW7f6l3Uo-",
    "outputId": "07579161-4b6e-44de-d610-7ef1766ec554"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/2500394300.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  CalculateNNSE = np.full(NpredperseqTOT, False, dtype = np.bool)\n"
     ]
    }
   ],
   "source": [
    "#Set up NNSE Normalized Nash Sutcliffe Efficiency\n",
    "CalculateNNSE = np.full(NpredperseqTOT, False, dtype = np.bool)\n",
    "for ipred in range(0,NumpredbasicperTime):\n",
    "  CalculateNNSE[ipred] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hytLQj7QW3gx"
   },
   "source": [
    "## Location Based Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "s2g_-MHEhyGr",
    "outputId": "5dc0bff9-12c5-46c4-d013-22055192f252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Locations 79 Validation Locations 0\n"
     ]
    }
   ],
   "source": [
    "LocationBasedValidation = True\n",
    "LocationValidationFraction = 0.0\n",
    "RestartLocationBasedValidation = False\n",
    "RestartRunName = RunName\n",
    "RestartRunName = 'EARTHQN-Transformer3'\n",
    "FullSetValidation = False\n",
    "\n",
    "global SeparateValandTrainingPlots\n",
    "SeparateValandTrainingPlots = True\n",
    "if not LocationBasedValidation:\n",
    "  SeparateValandTrainingPlots = False\n",
    "  LocationValidationFraction = 0.0\n",
    "NlocValplusTraining = Nloc\n",
    "ListofTrainingLocs = np.arange(Nloc, dtype = np.int32)\n",
    "ListofValidationLocs = np.full(Nloc, -1, dtype = np.int32)\n",
    "MappingtoTraining = np.arange(Nloc, dtype = np.int32)\n",
    "MappingtoValidation = np.full(Nloc, -1, dtype = np.int32)\n",
    "TrainingNloc = Nloc\n",
    "ValidationNloc = 0\n",
    "if LocationBasedValidation:\n",
    "  if RestartLocationBasedValidation:\n",
    "      InputFileName = APPLDIR + '/Validation' + RestartRunName\n",
    "      with open(InputFileName, 'r', newline='') as inputfile:\n",
    "        Myreader = reader(inputfile, delimiter=',')\n",
    "        header = next(Myreader)\n",
    "        LocationValidationFraction = np.float32(header[0])\n",
    "        TrainingNloc = np.int32(header[1])\n",
    "        ValidationNloc = np.int32(header[2])     \n",
    "\n",
    "        ListofTrainingLocs = np.empty(TrainingNloc, dtype = np.int32)\n",
    "        ListofValidationLocs = np.empty(ValidationNloc,  dtype = np.int32)\n",
    "        nextrow = next(Myreader)\n",
    "        for iloc in range(0, TrainingNloc):\n",
    "          ListofTrainingLocs[iloc] = np.int32(nextrow[iloc])\n",
    "        nextrow = next(Myreader)\n",
    "        for iloc in range(0, ValidationNloc):\n",
    "          ListofValidationLocs[iloc] = np.int32(nextrow[iloc])\n",
    "\n",
    "      LocationTrainingfraction = 1.0 - LocationValidationFraction\n",
    "      if TrainingNloc + ValidationNloc != Nloc:\n",
    "        printexit('EXIT: Inconsistent location counts for Location Validation ' +str(Nloc)\n",
    "          + ' ' + str(TrainingNloc) + ' ' + str(ValidationNloc))\n",
    "      print(' Validation restarted Fraction ' +str(round(LocationValidationFraction,4)) + ' ' + RestartRunName)\n",
    "\n",
    "  else:\n",
    "    LocationTrainingfraction = 1.0 - LocationValidationFraction\n",
    "    TrainingNloc = math.ceil(LocationTrainingfraction*Nloc)\n",
    "    ValidationNloc = Nloc - TrainingNloc\n",
    "    np.random.shuffle(ListofTrainingLocs)\n",
    "    ListofValidationLocs = ListofTrainingLocs[TrainingNloc:Nloc]\n",
    "    ListofTrainingLocs = ListofTrainingLocs[0:TrainingNloc]\n",
    "\n",
    "  for iloc in range(0,TrainingNloc):\n",
    "    jloc = ListofTrainingLocs[iloc]\n",
    "    MappingtoTraining[jloc] = iloc\n",
    "    MappingtoValidation[jloc] = -1\n",
    "  for iloc in range(0,ValidationNloc):\n",
    "    jloc = ListofValidationLocs[iloc]\n",
    "    MappingtoValidation[jloc] = iloc\n",
    "    MappingtoTraining[jloc] = -1\n",
    "  if ValidationNloc <= 0:\n",
    "    SeparateValandTrainingPlots = False\n",
    "\n",
    "  if not RestartLocationBasedValidation:\n",
    "    OutputFileName = APPLDIR + '/Validation' + RunName\n",
    "    with open(OutputFileName, 'w', newline='') as outputfile:\n",
    "      Mywriter = writer(outputfile, delimiter=',')\n",
    "      Mywriter.writerow([LocationValidationFraction, TrainingNloc, ValidationNloc] )\n",
    "      Mywriter.writerow(ListofTrainingLocs)\n",
    "      Mywriter.writerow(ListofValidationLocs)\n",
    "\n",
    "  print('Training Locations ' + str(TrainingNloc) + ' Validation Locations ' + str(ValidationNloc))\n",
    "  if ValidationNloc <=0:\n",
    "    LocationBasedValidation = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33FLmGmcilz5"
   },
   "source": [
    "## LSTM Control Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Ds28euHRi5vt",
    "outputId": "d6a3aca9-8154-4f4f-d5a1-58eea24c015c"
   },
   "outputs": [],
   "source": [
    "CustomLoss = 1         # Can be 0 1 4\n",
    "UseClassweights = True\n",
    "\n",
    "PredictionTraining = False\n",
    "\n",
    "if (not Hydrology) and (not Earthquake) and (NpredperseqTOT <=2):\n",
    "  useFutures = False\n",
    "  CustomLoss = 0\n",
    "  UseClassweights = False\n",
    "\n",
    "number_of_LSTMworkers = 1\n",
    "LSTMepochs = 100\n",
    "LSTMbatch_size = TrainingNloc\n",
    "LSTMbatch_size = min(LSTMbatch_size, TrainingNloc)\n",
    "\n",
    "LSTMactivationvalue = \"selu\"\n",
    "LSTMrecurrent_activation = \"sigmoid\"\n",
    "LSTMoptimizer = 'adam'\n",
    "LSTMdropout1=0.2\n",
    "LSTMrecurrent_dropout1 = 0.2\n",
    "LSTMdropout2=0.2\n",
    "LSTMrecurrent_dropout2 = 0.2\n",
    "number_LSTMnodes= 16\n",
    "LSTMFinalMLP = 64\n",
    "LSTMInitialMLP = 32\n",
    "LSTMThirdLayer = False\n",
    "\n",
    "LSTMSkipInitial = False\n",
    "LSTMverbose = 0\n",
    "LSTMvalidationfrac = 0.0\n",
    "UsedLSTMvalidationfrac = LSTMvalidationfrac\n",
    "if LocationBasedValidation:\n",
    "  UsedLSTMvalidationfrac = LocationBasedValidation\n",
    "  LSTMvalidationfrac = UsedLSTMvalidationfrac\n",
    "bestmethod = 2\n",
    "if UsedLSTMvalidationfrac < 0.001:\n",
    "    bestmethod = 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJFhD-nq0fO0"
   },
   "source": [
    "## Important Parameters defining Transformer project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "oOxm7gWkyjIj",
    "outputId": "d431c805-205b-4a77-ac34-8d8103c709de"
   },
   "outputs": [],
   "source": [
    "ActivateAttention = False\n",
    "DoubleQKV = False\n",
    "TimeShufflingOnly = False\n",
    "Transformerbatch_size = 1\n",
    "Transformervalidationfrac = 0.0\n",
    "UsedTransformervalidationfrac = 0.0\n",
    "Transformerepochs = 200\n",
    "Transformeroptimizer ='adam'\n",
    "Transformerverbose = 0\n",
    "TransformerOnlyFullAttention = True\n",
    "d_model =64\n",
    "d_Attention = 2 * d_model\n",
    "if TransformerOnlyFullAttention:\n",
    "  d_Attention = d_model\n",
    "d_qk = d_model\n",
    "d_intermediateqk = 2 * d_model\n",
    "num_heads = 2\n",
    "num_Encoderlayers = 2\n",
    "EncoderDropout= 0.1\n",
    "EncoderActivation = 'selu'\n",
    "d_EncoderLayer = d_Attention\n",
    "d_merge = d_model\n",
    "d_ffn = 4*d_model\n",
    "MaskingOption = 0\n",
    "PeriodicInputTemporalEncoding = 7 # natural for COVID\n",
    "LinearInputTemporalEncoding = -1 # natural for COVID\n",
    "TransformerInputTemporalEncoding = 10000\n",
    "UseTransformerInputTemporalEncoding = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CdCNdQ_yGWV"
   },
   "source": [
    "## General Control Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "fwkXnZZGgJ_1",
    "outputId": "39e2ea81-164b-4f32-8de0-40bdc6c9d84e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 2020-03-13 Cutoff 2020-11-05 sequence index 237\n"
     ]
    }
   ],
   "source": [
    "OuterBatchDimension = Num_Seq * TrainingNloc\n",
    "IndividualPlots = True\n",
    "Plotrealnumbers = True\n",
    "PlotsOnlyinTestFIPS = True\n",
    "ListofTestFIPS = map(str,list(FIPSintegerlookup.keys())[::1])\n",
    "if Hydrology:\n",
    "  ListofTestFIPS = ['6224000','6622700']\n",
    "  ListofTestFIPS = ['','']\n",
    "if Earthquake:\n",
    "  ListofTestFIPS = ['','']\n",
    "  Plotrealnumbers = True\n",
    "\n",
    "StartDate = np.datetime64(InitialDate).astype('datetime64[D]') + np.timedelta64(Tseq*Dailyunit + int(Dailyunit/2),'D')\n",
    "if Earthquake: \n",
    "  dayrange = np.timedelta64(Dailyunit,'D')\n",
    "  CutoffDate = np.datetime64('1989-01-01')\n",
    "  NumericalCutoff = math.floor((CutoffDate - StartDate)/dayrange)\n",
    "else:\n",
    "  NumericalCutoff = int(Num_Seq/2)\n",
    "  CutoffDate = StartDate + np.timedelta64(NumericalCutoff*Dailyunit,'D')\n",
    "print('Start ' + str(StartDate) + ' Cutoff ' + str(CutoffDate) + \" sequence index \" + str(NumericalCutoff))\n",
    "\n",
    "TimeCutLabel = [' All Time ',' Start ',' End ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "4V88mmqms1pq",
    "outputId": "1cbf240f-d2f0-43c1-f1a0-8aea3cae05d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of sequence window Tseq  13\n",
      "Number of Sequences in time Num_Seq  475\n",
      "Number of locations Nloc  79\n",
      "Number of Training Sequences in Location and Time  37525\n",
      "Number of internal properties per sequence including static or dynamic Npropperseq  14\n",
      "Number of internal properties per sequence adding in explicit space-time encoding  22\n",
      "Total number of predictions per sequence NpredperseqTOT  30\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of sequence window Tseq \", str(Tseq))\n",
    "print(\"Number of Sequences in time Num_Seq \", str(Num_Seq))\n",
    "print(\"Number of locations Nloc \", str(Nloc))\n",
    "print(\"Number of Training Sequences in Location and Time \", str(OuterBatchDimension))\n",
    "print(\"Number of internal properties per sequence including static or dynamic Npropperseq \", str(Npropperseq))\n",
    "print(\"Number of internal properties per sequence adding in explicit space-time encoding \", str(NpropperseqTOT))\n",
    "print(\"Total number of predictions per sequence NpredperseqTOT \", str(NpredperseqTOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikdmffIpA6AC"
   },
   "source": [
    "## Useful Time series utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2QTzC0vnSGP"
   },
   "source": [
    "### DLprediction\n",
    "\n",
    "Prediction and Visualization LSTM+Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "aarkiMHirB1S",
    "outputId": "d1b42a08-2043-4447-e4bd-da15724fedc1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def DLprediction(Xin, yin, DLmodel, modelflag, TFTWeights, LabelFit =''):\n",
    "  # modelflag = 0 LSTM = 1 Transformer\n",
    "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
    "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
    "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
    "    current_time = timenow()\n",
    "    print(startbold + startred + current_time + ' ' + RunName + \" DLPrediction \" +RunComment + resetfonts)\n",
    "\n",
    "    FitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "    # Compare to RawInputPredictionsTOT\n",
    "\n",
    "    RMSEbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSETRAINbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSEVALbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSVbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    AbsEbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    AbsVbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    ObsVbytimeandclass = np.zeros([Num_Seq, NpredperseqTOT], dtype=np.float64)\n",
    "    Predbytimeandclass = np.zeros([Num_Seq, NpredperseqTOT], dtype=np.float64)\n",
    "    countbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    countVALbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    countTRAINbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    totalcount = 0\n",
    "    overcount = 0\n",
    "    weightedcount = 0.0\n",
    "    weightedovercount = 0.0\n",
    "    weightedrmse1 = 0.0\n",
    "    weightedrmse1TRAIN = 0.0\n",
    "    weightedrmse1VAL = 0.0\n",
    "\n",
    "    closs = 0.0\n",
    "    dloss = 0.0\n",
    "    eloss = 0.0\n",
    "    floss = 0.0\n",
    "    sw = np.empty([Nloc,NpredperseqTOT],dtype = np.float32)\n",
    "    for iloc in range(0,Nloc):\n",
    "      for k in range(0,NpredperseqTOT):\n",
    "        sw[iloc,k] = Predictionwgt[k]\n",
    "\n",
    "    global tensorsw\n",
    "    tensorsw = tf.convert_to_tensor(sw, np.float32)\n",
    "    Ctime1 = 0.0\n",
    "    Ctime2 = 0.0\n",
    "    Ctime3 = 0.0\n",
    "    samplebar = notebook.trange(Num_Seq,  desc='Predict loop', unit  = 'sequences')\n",
    "    countingcalls = 0\n",
    "\n",
    "    for iseq in range(0, Num_Seq):\n",
    "      StopWatch.start('label1')\n",
    "      if SymbolicWindows:\n",
    "        if modelflag == 2:\n",
    "          InputVector = np.empty((Nloc,2), dtype = int)\n",
    "          for iloc in range (0,Nloc):\n",
    "            InputVector[iloc,0] = iloc\n",
    "            InputVector[iloc,1] = iseq\n",
    "        else:\n",
    "          InputVector = Xin[:,iseq:iseq+Tseq,:]\n",
    "      else:\n",
    "        InputVector = Xin[iseq]\n",
    "      Time = None\n",
    "      if modelflag == 0:\n",
    "        InputVector = np.reshape(InputVector,(-1,Tseq,NpropperseqTOT))\n",
    "      elif modelflag == 1:\n",
    "        InputVector = np.reshape(InputVector,(1,Tseq*Nloc,NpropperseqTOT))\n",
    "        BasicTimes = np.full(Nloc,iseq, dtype=np.int32)\n",
    "        Time = SetSpacetime(np.reshape(BasicTimes,(1,-1)))\n",
    "      StopWatch.stop('label1')\n",
    "      Ctime1 += StopWatch.get('label1', digits=4)\n",
    "      \n",
    "      StopWatch.start('label2')\n",
    "      PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time)\n",
    "      StopWatch.stop('label2')\n",
    "      Ctime2 += StopWatch.get('label2', digits=4)\n",
    "      StopWatch.start('label3')\n",
    "      PredictedVector = np.reshape(PredictedVector,(Nloc,NpredperseqTOT))\n",
    "      TrueVector = yin[iseq]\n",
    "      functionval = numpycustom_lossGCF1(TrueVector,PredictedVector,sw)\n",
    "      closs += functionval\n",
    "      PredictedVector_t = tf.convert_to_tensor(PredictedVector)\n",
    "      yin_t = tf.convert_to_tensor(TrueVector)\n",
    "      dloss += weightedcustom_lossGCF1(yin_t,PredictedVector_t,tensorsw)\n",
    "      eloss += custom_lossGCF1spec(yin_t,PredictedVector_t) \n",
    "      OutputLoss = 0.0\n",
    "      FitPredictions[iseq] = PredictedVector\n",
    "      for iloc in range(0,Nloc):\n",
    "        yy = yin[iseq,iloc]\n",
    "        yyhat = PredictedVector[iloc]\n",
    "\n",
    "        sum1 = 0.0\n",
    "        for i in range(0,NpredperseqTOT):\n",
    "          overcount += 1\n",
    "          weightedovercount += Predictionwgt[i]\n",
    "\n",
    "          if(math.isnan(yy[i])):\n",
    "            continue\n",
    "          weightedcount += Predictionwgt[i]\n",
    "          totalcount += 1\n",
    "          mse1 = ((yy[i]-yyhat[i])**2)\n",
    "          mse = mse1*sw[iloc,i]\n",
    "          if i < Npredperseq:\n",
    "            floss += mse\n",
    "          sum1 += mse\n",
    "          AbsEbyclass[i] += abs(yy[i] - yyhat[i])\n",
    "          RMSVbyclass[i] += yy[i]**2\n",
    "          AbsVbyclass[i] += abs(yy[i])\n",
    "          RMSEbyclass[i,0] += mse\n",
    "          countbyclass[i,0] += 1.0\n",
    "          if iseq < NumericalCutoff:\n",
    "            countbyclass[i,1] += 1.0\n",
    "            RMSEbyclass[i,1] += mse\n",
    "          else:\n",
    "            countbyclass[i,2] += 1.0\n",
    "            RMSEbyclass[i,2] += mse\n",
    "          if LocationBasedValidation:\n",
    "            if MappingtoTraining[iloc] >= 0:\n",
    "              RMSETRAINbyclass[i,0] += mse\n",
    "              countTRAINbyclass[i,0] += 1.0\n",
    "              if iseq < NumericalCutoff:\n",
    "                RMSETRAINbyclass[i,1] += mse\n",
    "                countTRAINbyclass[i,1] += 1.0\n",
    "              else:\n",
    "                RMSETRAINbyclass[i,2] += mse\n",
    "                countTRAINbyclass[i,2] += 1.0\n",
    "            if MappingtoValidation[iloc] >= 0:\n",
    "              RMSEVALbyclass[i,0] += mse\n",
    "              countVALbyclass[i,0] += 1.0\n",
    "              if iseq < NumericalCutoff:\n",
    "                RMSEVALbyclass[i,1] += mse\n",
    "                countVALbyclass[i,1] += 1.0\n",
    "              else:\n",
    "                RMSEVALbyclass[i,2] += mse\n",
    "                countVALbyclass[i,2] += 1.0\n",
    "          ObsVbytimeandclass [iseq,i] += abs(yy[i])\n",
    "          Predbytimeandclass [iseq,i] += abs(yyhat[i])\n",
    "        weightedrmse1 += sum1\n",
    "        if LocationBasedValidation:\n",
    "          if MappingtoTraining[iloc] >= 0:\n",
    "            weightedrmse1TRAIN += sum1\n",
    "          if MappingtoValidation[iloc] >= 0:\n",
    "            weightedrmse1VAL += sum1\n",
    "        OutputLoss += sum1\n",
    "      StopWatch.stop('label3')\n",
    "      Ctime3 += StopWatch.get('label3', digits=4)\n",
    "      OutputLoss /= Nloc\n",
    "      countingcalls += 1\n",
    "      samplebar.update(1)\n",
    "      samplebar.set_postfix( Call = countingcalls, TotalLoss = OutputLoss)\n",
    "\n",
    "    print('Times ' + str(round(Ctime1,5))  + ' ' + str(round(Ctime3,5)) + ' TF ' + str(round(Ctime2,5)))\n",
    "    weightedrmse1 /= (Num_Seq * Nloc)\n",
    "    floss /= (Num_Seq * Nloc)\n",
    "    if LocationBasedValidation:\n",
    "      weightedrmse1TRAIN /= (Num_Seq * TrainingNloc)\n",
    "      if ValidationNloc>0:\n",
    "        weightedrmse1VAL /= (Num_Seq * ValidationNloc)\n",
    "    dloss = dloss.numpy()\n",
    "    eloss = eloss.numpy()\n",
    "    closs /= Num_Seq\n",
    "    dloss /= Num_Seq\n",
    "    eloss /= Num_Seq\n",
    "\n",
    "    current_time = timenow()\n",
    "    line1 = ''\n",
    "    global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
    "    GlobalLoss = weightedrmse1\n",
    "    if LocationBasedValidation:\n",
    "      line1 = ' Training ' + str(round(weightedrmse1TRAIN,6)) + ' Validation ' + str(round(weightedrmse1VAL,6))\n",
    "      GlobalTrainingLoss = weightedrmse1TRAIN\n",
    "      GlobalValidationLoss = weightedrmse1VAL\n",
    "    print( startbold + startred + current_time + ' DLPrediction Averages' + ' ' + RunName + ' ' + RunComment +  resetfonts)\n",
    "    line = LabelFit + ' ' + RunName + ' Weighted sum over predicted values ' + str(round(weightedrmse1,6))\n",
    "    line += ' No Encoding Preds ' + str(round(floss,6)) + line1\n",
    "    line += ' from loss function ' + str(round(closs,6)) + ' TF version ' + str(round(dloss,6)) + ' TFspec version ' + str(round(eloss,6))  \n",
    "    print(wraptotext(line))\n",
    "    print('Count ignoring NaN ' +str(round(weightedcount,4))+ ' Counting NaN ' + str(round(weightedovercount,4)), 70 )\n",
    "    print(' Unwgt Count no NaN ',totalcount, ' Unwgt Count with NaN ',overcount, ' Number Sequences ', Nloc*Num_Seq)\n",
    "\n",
    "    ObsvPred = np.sum( np.abs(ObsVbytimeandclass-Predbytimeandclass) , axis=0)\n",
    "    TotalObs = np.sum( ObsVbytimeandclass , axis=0)\n",
    "    SummedEbyclass = np.divide(ObsvPred,TotalObs)\n",
    "    RMSEbyclass1 = np.divide(RMSEbyclass,countbyclass) # NO SQRT\n",
    "    RMSEbyclass2 = np.sqrt(np.divide(RMSEbyclass[:,0],RMSVbyclass))\n",
    "    RelEbyclass = np.divide(AbsEbyclass, AbsVbyclass)\n",
    "    extracomments = []\n",
    "\n",
    "    line1 = '\\nErrors by Prediction Components -- class weights not included except in final Loss components\\n Name Count without NaN, '\n",
    "    line2 = 'Loss wgt * sum errors**2/count, sqrt(sum errors**2/sum target**2), sum(abs(error)/sum(abs(value), abs(sum(abs(value)-abs(pred)))/sum(abs(pred)'\n",
    "    print(wraptotext(startbold + startred + line1 + line2 + resetfonts))\n",
    "    \n",
    "    for i in range(0,NpredperseqTOT):\n",
    "      line = startbold + startred + ' Loss Coeffs '\n",
    "      for timecut in range(0,3):\n",
    "        line += TimeCutLabel[timecut] + 'Full ' + str(round(RMSEbyclass1[i,timecut],6)) + resetfonts\n",
    "      if LocationBasedValidation:\n",
    "        RTRAIN = np.divide(RMSETRAINbyclass[i],countTRAINbyclass[i])\n",
    "        RVAL = np.full(3,0.0, dtype =np.float32)\n",
    "        if countVALbyclass[i,0] > 0:\n",
    "          RVAL = np.divide(RMSEVALbyclass[i],countVALbyclass[i])\n",
    "        for timecut in range(0,3):\n",
    "          line += startbold + startpurple + TimeCutLabel[timecut] + 'TRAIN ' + resetfonts + str(round(RTRAIN[timecut],6))\n",
    "          line += startbold + ' VAL ' + resetfonts + str(round(RVAL[timecut],6))\n",
    "      else:\n",
    "        RTRAIN = RMSEbyclass1[i]\n",
    "        RVAL = np.full(3,0.0, dtype =np.float32)\n",
    "        for timecut in range(0,3):\n",
    "          line += TimeCutLabel[timecut] + 'FULL ' + str(round(RTRAIN[timecut],6))   \n",
    "      print(wraptotext(str(i) + ' ' + startbold + Predictionname[i] + resetfonts + ' All Counts ' + str(round(countbyclass[i,0],0)) + ' '\n",
    "       + str(round(100.0*RMSEbyclass2[i],4)) + ' ' + str(round(100.0*RelEbyclass[i],4)) + ' ' + str(round(100.0*SummedEbyclass[i],4)) +line ))\n",
    "      extracomments.append(['Loss Coeffs F=' + str(round(RTRAIN[0],5)) + ' S=' + str(round(RTRAIN[1],5))+ ' E=' + str(round(RTRAIN[2],5)),\n",
    "                            'Loss Coeffs F=' + str(round(RVAL[0],5)) + ' S=' + str(round(RVAL[1],5))+ ' E=' + str(round(RVAL[2],5))])\n",
    "\n",
    "# Don't use DLPrediction for Transformer Plots. Wait for DL2B,D,E\n",
    "    if modelflag == 1:\n",
    "      return FitPredictions\n",
    "    \n",
    "\n",
    "    print('\\n Next plots come from DLPrediction')\n",
    "    Location_summed_plot(yin, FitPredictions, extracomments = extracomments, Dumpplot = True)  \n",
    "    ##inserting average feature plots\n",
    "    AverageFeatureContributionPlot(TFTWeights)\n",
    "    AverageTemporalImportance(TFTWeights)\n",
    "    \n",
    "    FindNNSE(yin, FitPredictions)\n",
    "    for iplot in range(1,1+LengthFutures):\n",
    "      plot_by_futureindex(iplot,yin,FitPredictions, fill=True, extrastring='')\n",
    "      \n",
    "    if IndividualPlots:\n",
    "      ProduceIndividualPlots(yin, FitPredictions)\n",
    "\n",
    "\n",
    "    return FitPredictions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "BSr9vtXkGE5p",
    "outputId": "89bad9be-efc6-43b2-89c8-cd66c42727b0"
   },
   "outputs": [],
   "source": [
    "IncreaseNloc_sample = 1\n",
    "DecreaseNloc_sample = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "mclQf3BxB3sQ",
    "outputId": "99ef1e11-edf1-467c-cff5-2f085621fcbc"
   },
   "outputs": [],
   "source": [
    "def DLprediction2F(Xin, yin, DLmodel, modelflag):\n",
    "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
    "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
    "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
    "  # Label Array is always [Num_Seq][Nloc] [0=Window(first sequence)#, 1=Location]\n",
    "\n",
    "  # if SkipDL2F:\n",
    "  #   return\n",
    "  if GarbageCollect:\n",
    "    gc.collect()\n",
    "  global  OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
    "\n",
    "  SensitivityAnalyze = np.full((NpropperseqTOT), False, dtype = np.bool)\n",
    "  SensitivityChange = np.zeros ((NpropperseqTOT), dtype = np.float32)\n",
    "  SensitvitybyPrediction = False\n",
    "  if ReadApril2021Covid or ReadDecember2021:\n",
    "    for iprop in range(0,NpropperseqTOT):\n",
    "      SensitivityAnalyze[iprop] = True\n",
    "\n",
    "  something = 0\n",
    "  SensitivityList = []\n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    if SensitivityAnalyze[iprop]:\n",
    "      something +=1\n",
    "      SensitivityList.append(iprop)\n",
    "  if something == 0:\n",
    "    return\n",
    "  ScaleProperty = 0.99\n",
    "  SampleSize = 1\n",
    "\n",
    "\n",
    "  SensitivityFitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT, 1 + something], dtype =np.float32)\n",
    "  FRanges = np.full((NpredperseqTOT), 1.0, dtype = np.float32)\n",
    "  current_time = timenow()\n",
    "  print(wraptotext(startbold+startred+ 'DLPrediction2F ' +current_time + ' ' + RunName + RunComment +  resetfonts))\n",
    "\n",
    "  sw = np.empty_like(yin, dtype=np.float32)\n",
    "  for i in range(0,sw.shape[0]):\n",
    "    for j in range(0,sw.shape[1]):\n",
    "      for k in range(0,NpredperseqTOT):\n",
    "        sw[i,j,k] = Predictionwgt[k] \n",
    "  labelarray =np.empty([Num_Seq, Nloc, 2], dtype = np.int32)\n",
    "  for iseq in range(0, Num_Seq):\n",
    "    for iloc in range(0,Nloc):\n",
    "      labelarray[iseq,iloc,0] = iseq\n",
    "      labelarray[iseq,iloc,1] = iloc\n",
    "\n",
    "  Totaltodo = Num_Seq*Nloc\n",
    "  Nloc_sample = Nloc # default\n",
    "\n",
    "  if IncreaseNloc_sample > 1:\n",
    "    Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
    "  elif DecreaseNloc_sample > 1:\n",
    "    Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
    "\n",
    "  if Totaltodo%Nloc_sample != 0:\n",
    "    printexit('Invalid Nloc_sample ' + str(Nloc_sample) + \" \" + str(Totaltodo))\n",
    "  d_sample = Tseq * Nloc_sample        \n",
    "  max_d_sample = d_sample\n",
    "  OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
    "  print(' Predict with ' +str(Nloc_sample) + ' sequences per sample and batch size ' + str(OuterBatchDimension))\n",
    "\n",
    "  print(startbold+startred+ 'Sensitivity using Property ScaleFactor ' + str(round(ScaleProperty,3)) + resetfonts)\n",
    "  for Sensitivities in range(0,1+something):\n",
    "    if Sensitivities == 0:\n",
    "      iprop = -1\n",
    "      print(startbold+startred+ 'Basic Predictions' + resetfonts)\n",
    "      if SymbolicWindows:\n",
    "        ReshapedSequencesTOTmodified = ReshapedSequencesTOT\n",
    "      else:\n",
    "        Xinmodified = Xin\n",
    "    else:\n",
    "      iprop = SensitivityList[Sensitivities-1]\n",
    "      maxminplace = PropertyNameIndex[iprop]\n",
    "      lastline = ''\n",
    "      if iprop < Npropperseq:\n",
    "        lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
    "      print(startbold+startred+ 'Property ' + str(iprop) + ' ' + InputPropertyNames[maxminplace] + resetfonts + lastline)\n",
    "      if SymbolicWindows:\n",
    "        ReshapedSequencesTOTmodified = np.copy(ReshapedSequencesTOT)\n",
    "        ReshapedSequencesTOTmodified[:,:,iprop] = ScaleProperty * ReshapedSequencesTOTmodified[:,:,iprop]\n",
    "      else:\n",
    "        Xinmodified = np.copy(Xin)\n",
    "        Xinmodified[:,:,:,iprop] = ScaleProperty*Xinmodified[:,:,:,iprop]\n",
    "    CountFitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "    meanvalue2 = 0.0\n",
    "    meanvalue3 = 0.0\n",
    "    meanvalue4 = 0.0\n",
    "    variance2= 0.0\n",
    "    variance3= 0.0\n",
    "    variance4= 0.0\n",
    "\n",
    "    samplebar = notebook.trange(SampleSize,  desc='Full Samples', unit  = 'sample')\n",
    "    bbar = notebook.trange(OuterBatchDimension,  desc='Batch    loop', unit  = 'sample')\n",
    "    for shuffling in range (0,SampleSize):\n",
    "      if GarbageCollect:\n",
    "        gc.collect()\n",
    "      yuse = yin\n",
    "      labeluse = labelarray\n",
    "      y2= np.reshape(yuse, (-1, NpredperseqTOT)).copy()\n",
    "      labelarray2 = np.reshape(labeluse, (-1,2))\n",
    "\n",
    "      if SymbolicWindows:\n",
    "        # Xin X2 X3 not used rather ReshapedSequencesTOT\n",
    "        labelarray2, y2 = shuffleDLinput(labelarray2, y2)\n",
    "        ReshapedSequencesTOTuse = ReshapedSequencesTOTmodified\n",
    "      else:\n",
    "        Xuse = Xinmodified\n",
    "        X2 = np.reshape(Xuse, (-1, Tseq, NpropperseqTOT)).copy()\n",
    "        X2, y2, labelarray2 = shuffleDLinput(X2, y2,labelarray2)\n",
    "        X3 = np.reshape(X2, (-1, d_sample, NpropperseqTOT))\n",
    "        \n",
    "      y3 = np.reshape(y2, (-1, Nloc_sample, NpredperseqTOT))\n",
    "      sw = np.reshape(sw, (-1, Nloc_sample, NpredperseqTOT))\n",
    "      labelarray3 = np.reshape(labelarray2, (-1, Nloc_sample, 2))\n",
    "\n",
    "      quan2 = 0.0\n",
    "      quan3 = 0.0\n",
    "      quan4 = 0.0\n",
    "      for Batchindex in range(0, OuterBatchDimension):\n",
    "        if GarbageCollect:\n",
    "          gc.collect()\n",
    "\n",
    "        if SymbolicWindows:\n",
    "          X3local = list()\n",
    "          for iloc_sample in range(0,Nloc_sample):\n",
    "            LocLocal = labelarray3[Batchindex, iloc_sample,1]\n",
    "            SeqLocal = labelarray3[Batchindex, iloc_sample,0]\n",
    "            X3local.append(ReshapedSequencesTOTuse[LocLocal,SeqLocal:SeqLocal+Tseq])\n",
    "          InputVector = np.array(X3local)\n",
    "        else:\n",
    "          InputVector = X3[Batchindex]\n",
    "\n",
    "        Labelsused = labelarray3[Batchindex]\n",
    "        Time = None\n",
    "        # if modelflag == 0:\n",
    "        #   InputVector = np.reshape(InputVector,(-1,Tseq,NpropperseqTOT))\n",
    "        # else:\n",
    "        #   Time = SetSpacetime(np.reshape(Labelsused[:,0],(1,-1)))\n",
    "        #   InputVector = np.reshape(InputVector,(1,Tseq*Nloc_sample,NpropperseqTOT))\n",
    "        for iseq in range(0, Num_Seq):\n",
    "          StopWatch.start('label1')\n",
    "          if SymbolicWindows:\n",
    "            if modelflag == 2:\n",
    "              InputVector = np.empty((Nloc,2), dtype = int)\n",
    "              for iloc in range (0,Nloc):\n",
    "                InputVector[iloc,0] = iloc\n",
    "                InputVector[iloc,1] = iseq\n",
    "            else:\n",
    "              InputVector = Xin[:,iseq:iseq+Tseq,:]\n",
    "          else:\n",
    "            InputVector = Xin[iseq]\n",
    "          Time = None\n",
    "          if modelflag == 0:\n",
    "            InputVector = np.reshape(InputVector,(-1,Tseq,NpropperseqTOT))\n",
    "          elif modelflag == 1:\n",
    "            InputVector = np.reshape(InputVector,(1,Tseq*Nloc,NpropperseqTOT))\n",
    "            BasicTimes = np.full(Nloc,iseq, dtype=np.int32)\n",
    "            Time = SetSpacetime(np.reshape(BasicTimes,(1,-1)))\n",
    "        # print(InputVector)\n",
    "        # print(PredictionTraining)\n",
    "        # print('TIME')\n",
    "        # print(Time)\n",
    "        PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time )\n",
    "        PredictedVector = np.reshape(PredictedVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "        swbatched = sw[Batchindex,:,:]\n",
    "        if LocationBasedValidation:\n",
    "          swT = np.zeros([1,Nloc_sample,NpredperseqTOT],dtype = np.float32)\n",
    "          swV = np.zeros([1,Nloc_sample,NpredperseqTOT],dtype = np.float32)\n",
    "          for iloc_sample in range(0,Nloc_sample):\n",
    "            fudgeT = Nloc/TrainingNloc\n",
    "            fudgeV = Nloc/ValidationNloc\n",
    "            iloc = Labelsused[iloc_sample,1]\n",
    "            if MappingtoTraining[iloc] >= 0:\n",
    "              swT[0,iloc_sample,:] = swbatched[iloc_sample,:]*fudgeT\n",
    "            else:\n",
    "              swV[0,iloc_sample,:] = swbatched[iloc_sample,:]*fudgeV\n",
    "        TrueVector = y3[Batchindex]\n",
    "        TrueVector = np.reshape(TrueVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "        swbatched = np.reshape(swbatched,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "        losspercall = numpycustom_lossGCF1(TrueVector,PredictedVector,swbatched)\n",
    "        quan2 += losspercall\n",
    "        bbar.update(1)\n",
    "        if LocationBasedValidation:\n",
    "          losspercallTr = numpycustom_lossGCF1(TrueVector,PredictedVector,swT)\n",
    "          quan3 += losspercallTr\n",
    "          losspercallVl = numpycustom_lossGCF1(TrueVector,PredictedVector,swV)\n",
    "          quan4 += losspercallVl\n",
    "        \n",
    "        for iloc_sample in range(0,Nloc_sample):\n",
    "          LocLocal = Labelsused[iloc_sample,1]\n",
    "          SeqLocal = Labelsused[iloc_sample,0]\n",
    "          yyhat = PredictedVector[0,iloc_sample]\n",
    "          CountFitPredictions [SeqLocal,LocLocal,:] += FRanges\n",
    "          SensitivityFitPredictions [SeqLocal,LocLocal,:,Sensitivities] += yyhat\n",
    "\n",
    "        fudge = 1.0/(1.0 + Batchindex)\n",
    "        mean2 = quan2 * fudge \n",
    "        if LocationBasedValidation:\n",
    "          mean3 = quan3 * fudge\n",
    "          mean4 = quan4 * fudge\n",
    "          bbar.set_postfix(AvLoss = mean2, AvTr = mean3, AvVl = mean4, Loss = losspercall, Tr = losspercallTr, Vl = losspercallVl)\n",
    "        else:\n",
    "          bbar.set_postfix(Loss = losspercall, AvLoss = mean2 ) \n",
    "\n",
    "  # Processing at the end of Sampling Loop\n",
    "      fudge = 1.0/OuterBatchDimension\n",
    "      quan2 *= fudge\n",
    "      quan3 *= fudge\n",
    "      quan4 *= fudge\n",
    "      meanvalue2 += quan2\n",
    "      variance2 += quan2**2\n",
    "      variance3 += quan3**2\n",
    "      variance4 += quan4**2\n",
    "      if LocationBasedValidation:\n",
    "        meanvalue3 += quan3\n",
    "        meanvalue4 += quan4        \n",
    "      samplebar.update(1)\n",
    "      if LocationBasedValidation:\n",
    "        samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Tr = quan3, Val = quan4)\n",
    "      else:\n",
    "        samplebar.set_postfix(Shuffle=shuffling, Loss = quan2)\n",
    "      bbar.reset()\n",
    "  # End Shuffling loop\n",
    "\n",
    "    if Sensitivities == 0:\n",
    "      iprop = -1\n",
    "      lineend = startbold+startred+ 'Basic Predictions' + resetfonts\n",
    "    else:\n",
    "      iprop = SensitivityList[Sensitivities-1]\n",
    "      nameplace = PropertyNameIndex[iprop]\n",
    "      maxminplace = PropertyAverageValuesPointer[iprop]\n",
    "      lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
    "      lineend= startbold+startred + 'Property ' + str(iprop) + ' ' + InputPropertyNames[nameplace] + resetfonts + lastline\n",
    "\n",
    "    meanvalue2 /= SampleSize \n",
    "\n",
    "    global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
    "    # printloss(' Full Loss ',meanvalue2,variance2,SampleSize, lineend = lineend)\n",
    "    meanvalue2 /= SampleSize\n",
    "    GlobalLoss = meanvalue2\n",
    "    GlobalTrainingLoss = 0.0\n",
    "    GlobalValidationLoss = 0.0\n",
    "    \n",
    "    if LocationBasedValidation:\n",
    "      # printloss(' Training Loss ',meanvalue3,variance3,SampleSize, lineend = lineend)\n",
    "      # printloss(' Validation Loss ',meanvalue4,variance4,SampleSize, lineend = lineend)\n",
    "      meanvalue3 /= SampleSize\n",
    "      meanvalue4 /= SampleSize\n",
    "      GlobalTrainingLoss = meanvalue3\n",
    "      GlobalValidationLoss = meanvalue4\n",
    "\n",
    "# Sequence Location Predictions\n",
    "    SensitivityFitPredictions[:,:,:,Sensitivities] = np.divide(SensitivityFitPredictions[:,:,:,Sensitivities],CountFitPredictions[:,:,:])\n",
    "    if Sensitivities == 0:\n",
    "      Goldstandard = np.sum(np.abs(SensitivityFitPredictions[:,:,:,Sensitivities]), axis =(0,1))\n",
    "      TotalGS = np.sum(Goldstandard)\n",
    "      continue\n",
    "    Change = np.sum(np.abs(np.subtract(SensitivityFitPredictions[:,:,:,Sensitivities],SensitivityFitPredictions[:,:,:,0])), axis =(0,1))\n",
    "    TotalChange = np.sum(Change)\n",
    "    SensitivityChange[iprop] = TotalChange\n",
    "    print(str(round(TotalChange,5)) +  ' GS ' + str(round(TotalGS,5)) + ' ' +lineend)\n",
    "    if SensitvitybyPrediction:\n",
    "      for ipred in range(0,NpredperseqTOT):\n",
    "        print(str(round(Change[ipred],5)) +  ' GS ' + str(round(Goldstandard[ipred],5)) \n",
    "        + ' ' + str(ipred) + ' ' + Predictionname[ipred] + ' wgt ' + str(round(Predictionwgt[ipred],3)))\n",
    "    \n",
    "  print(startbold+startred+ '\\nSummarize Changes Total ' + str(round(TotalGS,5))+ ' Property ScaleFactor ' + str(round(ScaleProperty,3)) + resetfonts )\n",
    "  for Sensitivities in range(1,1+something):\n",
    "    iprop = SensitivityList[Sensitivities-1]\n",
    "    nameplace = PropertyNameIndex[iprop]\n",
    "    maxminplace = PropertyAverageValuesPointer[iprop]\n",
    "    \n",
    " \n",
    "    lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
    "    lastline += ' Normed Std ' +str(round(QuantityStatistics[maxminplace,6],4))\n",
    "    TotalChange = SensitivityChange[iprop] \n",
    "    NormedChange = TotalChange/((1-ScaleProperty)*TotalGS)\n",
    "    stdmeanratio = 0.0\n",
    "    stdchangeratio = 0.0   \n",
    "    if np.abs(QuantityStatistics[maxminplace,5]) > 0.0001:\n",
    "      stdmeanratio = QuantityStatistics[maxminplace,6]/QuantityStatistics[maxminplace,5]\n",
    "    if np.abs(QuantityStatistics[maxminplace,6]) > 0.0001:\n",
    "      stdchangeratio = NormedChange/QuantityStatistics[maxminplace,6]\n",
    "\n",
    "    lratios =  ' Normed Change '+ str(round(NormedChange,5)) + ' /std ' + str(round(stdchangeratio,5))\n",
    "    lratios += ' Std/Mean ' + str(round(stdmeanratio,5))\n",
    "    print(str(iprop) + ' Change '+ str(round(TotalChange,2)) + startbold + lratios\n",
    "          + ' ' + InputPropertyNames[nameplace] + resetfonts + lastline)\n",
    "\n",
    "  current_time = timenow()  \n",
    "  print(startbold+startred+ '\\nEND DLPrediction2F ' + current_time + ' ' + RunName + RunComment +resetfonts) \n",
    "  return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrFjfW6ICXWd"
   },
   "source": [
    "### Feature Importance Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "CjYbUAEAO-nR",
    "outputId": "b6725fe9-f6a8-44ee-840f-44c2a53f2ed1"
   },
   "outputs": [],
   "source": [
    "col_list =['black','blue','red','grey','yellow','green','purple','magenta','cyan','brown','aquamarine','plum','chartreuse','coral','yellowgreen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "bZkHuRYZBbaf",
    "outputId": "71decec5-0d57-4d6a-9494-6e8d67b7d4c2"
   },
   "outputs": [],
   "source": [
    "def AverageFeatureContributionPlot(TFTWeights):\n",
    "  '''\n",
    "  This function is for the summed plots to be viewed after training each model iteration\n",
    "  '''\n",
    "  # current_time = timenow()\n",
    "  # print(wraptotext(startbold + startred + current_time + ' feature importance from Future Attention weights ' + RunName + ' ' + RunComment + resetfonts))\n",
    "  # otherlen = len(otherlabs)\n",
    "\n",
    "  plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "  figure, (ax1) = plt.subplots(nrows=1, ncols=1)\n",
    "  ax1.set_prop_cycle('color',col_list)\n",
    "  FutureWeights = TFTWeights.getFutureWeights()\n",
    "  avgFutWeights = FutureWeights.mean(axis=0)\n",
    "\n",
    "  if len(TFTWeights.known) != avgFutWeights.shape[1]:\n",
    "    printexit('Incorrect future feature lengths')\n",
    "  else:\n",
    "    for index, Feature in enumerate(TFTWeights.known):\n",
    "      \n",
    "      FeatureString = Feature[0].split()[1]\n",
    "      titleString = 'Future Known Inputs Attention weights for feature importance'\n",
    "      ax1.set_title('\\n'.join(wrap(titleString,70)))\n",
    "      ax1.plot(avgFutWeights[:,index], label=FeatureString)\n",
    "      ylab = FeatureString + ' average attention weights'\n",
    "      ax1.set_ylabel(ylab)\n",
    "      ax1.grid(False)\n",
    "      ax1.legend()\n",
    "  figure.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  ## End Future Plot\n",
    "\n",
    "  ## Begin Historical Plot\n",
    "\n",
    "  plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "  figure, ax1 = plt.subplots(nrows=1, ncols=1)\n",
    "  ax1.set_prop_cycle('color',col_list)\n",
    "\n",
    "  HistoricalWeights = TFTWeights.getHistoricalWeights()\n",
    "  skip_unknown = True\n",
    "  if skip_unknown:\n",
    "    unknown_indices = []\n",
    "    new_historical = []\n",
    "  if len(TFTWeights.historical) != HistoricalWeights.shape[2]:\n",
    "    printexit('Incorrect Feature Lengths')\n",
    "  else:\n",
    "    for index, Feature in enumerate(TFTWeights.historical):\n",
    "      if skip_unknown:\n",
    "        if (Feature[2] != 2) and (Feature[2] != 1):\n",
    "          unknown_indices.append(index)\n",
    "        else:\n",
    "          new_historical.append(Feature)\n",
    "  HistoricalWeights = np.delete(HistoricalWeights, unknown_indices, axis=2)\n",
    "  avgHist = HistoricalWeights.mean(axis=0)\n",
    "  if skip_unknown:\n",
    "    if len(new_historical) != avgHist.shape[1]:\n",
    "      printexit('Mismatch due to the unknown inputs')\n",
    "    else:\n",
    "      for index, Feature in enumerate(new_historical):\n",
    "        FeatureString = Feature[0].split()[1]\n",
    "        titleString = 'Historical (known future + obserbed) inputs attention weights for feature importance'\n",
    "        ax1.set_title('\\n'.join(wrap(titleString,70)))\n",
    "        ax1.plot(avgHist[:,index], label=FeatureString)\n",
    "        ylab = FeatureString + ' average attention weights'\n",
    "        ax1.set_ylabel(ylab)\n",
    "        ax1.grid(False)\n",
    "        ax1.legend()\n",
    "  figure.tight_layout()\n",
    "  plt.show()\n",
    "          \n",
    "  ## End Historical Plot \n",
    "\n",
    "  ## Begin Static Plot \n",
    "\n",
    "  plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "  figure, ax1 = plt.subplots(nrows=1, ncols=1)\n",
    "  ax1.set_prop_cycle('color',col_list)\n",
    "\n",
    "  StaticWeights = TFTWeights.getStaticWeights()\n",
    "  avgStatic = StaticWeights.mean(axis=0)\n",
    "\n",
    "  if len(TFTWeights.static) != avgStatic.shape[1]:\n",
    "    printexit('Incorrect static feature lengths')\n",
    "  else:\n",
    "    for index, Feature in enumerate(TFTWeights.static):\n",
    "      \n",
    "      FeatureString = Feature[0].split()[1]\n",
    "      titleString = 'Static Inputs Attention weights for feature importance'\n",
    "      ax1.set_title('\\n'.join(wrap(titleString,70)))\n",
    "      ax1.plot(avgStatic[:,index], label=FeatureString)\n",
    "      ylab = FeatureString + ' average attention weights'\n",
    "      ax1.set_ylabel(ylab)\n",
    "      ax1.grid(False)\n",
    "      ax1.legend()\n",
    "\n",
    "  figure.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def AverageTemporalImportance(TFTWeights):\n",
    "\n",
    "  TemporalMatrix = TFTWeights.getTemporalImportance()\n",
    "  for idx,i in enumerate(TemporalMatrix):\n",
    "  \n",
    "    titleStr = 'Temporal attention weights for day ' + str(idx+1)\n",
    "    if idx % 2 == 0:\n",
    "      figure, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n",
    "    \n",
    "    ax.flatten()[idx%2].plot(TemporalMatrix[idx])\n",
    "    minm = min(TemporalMatrix[idx])\n",
    "    maxm = max(TemporalMatrix[idx])\n",
    "    ax.flatten()[idx%2].vlines(idx, minm, maxm,color='black',alpha=.5,label='Weights for Day')\n",
    "    if idx-7 > 0:\n",
    "      ax.flatten()[idx%2].vlines(idx-7, minm, maxm,color='green',alpha=.5, label='Current Timestep - 7(days)')\n",
    "    if idx-14 > 0:\n",
    "      ax.flatten()[idx%2].vlines(idx-14, minm, maxm,color='green',alpha=.5)\n",
    "    if idx-21 > 0:\n",
    "      ax.flatten()[idx%2].vlines(idx-21, minm, maxm,color='green',alpha=.5)\n",
    "\n",
    "    ax.flatten()[idx%2].set_title(titleStr)\n",
    "    ax.flatten()[idx%2].set_ylabel('Decoder Attention Weight')\n",
    "    ax.flatten()[idx%2].set_xlabel('Importance of point X in the 28 day sequence')\n",
    "    ax.flatten()[idx%2].legend(loc='upper right')\n",
    "    if idx%2==1:\n",
    "      figure.tight_layout()\n",
    "      plt.show()\n",
    "      print()\n",
    "\n",
    "\n",
    "def FuturewQuantiles(TFTWeights):\n",
    "\n",
    "  qs = TFTWeights.quantiles\n",
    "  FutureQMatrix = np.quantile(TFTWeights.weights['future_flags'],qs,axis=0)\n",
    "\n",
    "  for Feature in range(FutureQMatrix.shape[2]):\n",
    "    plot_title = TFTWeights.known[Feature][0]\n",
    "    for Quantile in range(len(qs)):\n",
    "      if qs[Quantile] == .5:\n",
    "        plt.plot(FutureQMatrix[Quantile,:,Feature],label='.5 Quantile')\n",
    "      else:\n",
    "        qlabel = str(qs[Quantile]) + ' Quantile'\n",
    "        plt.plot(FutureQMatrix[Quantile,:,Feature],linestyle='--',color='black',alpha=.5,label=qlabel)\n",
    "    plt.title(plot_title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "def HistwQuantiles(TFTWeights):\n",
    "\n",
    "  qs = TFTWeights.quantiles\n",
    "  HistoryQMatrix = np.quantile(TFTWeights.weights['historical_flags'],qs,axis=0)\n",
    "\n",
    "  for Feature in range(HistoryQMatrix.shape[2]):\n",
    "    plot_title = TFTWeights.historical[Feature][0]\n",
    "    for Quantile in range(len(qs)):\n",
    "      if qs[Quantile] == .5:\n",
    "        plt.plot(HistoryQMatrix[Quantile,:,Feature],label='.5 Quantile')\n",
    "      else:\n",
    "        qlabel = str(qs[Quantile]) + ' Quantile'\n",
    "        plt.plot(HistoryQMatrix[Quantile,:,Feature],linestyle='--',color='black',alpha=.5,label=qlabel)\n",
    "    plt.title(plot_title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def HistwQuantiles(TFTWeights):\n",
    "\n",
    "  qs = TFTWeights.quantiles\n",
    "  HistoryQMatrix = np.quantile(TFTWeights.weights['historical_flags'],qs,axis=0)\n",
    "\n",
    "  for Feature in range(HistoryQMatrix.shape[2]):\n",
    "    plot_title = TFTWeights.historical[Feature][0]\n",
    "    for Quantile in range(len(qs)):\n",
    "      if qs[Quantile] == .5:\n",
    "        plt.plot(HistoryQMatrix[Quantile,:,Feature],label='.5 Quantile')\n",
    "      else:\n",
    "        qlabel = str(qs[Quantile]) + ' Quantile'\n",
    "        plt.plot(HistoryQMatrix[Quantile,:,Feature],linestyle='--',color='black',alpha=.5,label=qlabel)\n",
    "    plt.title(plot_title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "def StaticwQuantiles(TFTWeights):\n",
    "\n",
    "  qs = TFTWeights.quantiles\n",
    "  StaticQMatrix = np.quantile(TFTWeights.weights['static_flags'],qs,axis=0)\n",
    "  StaticNames = TFTWeights.static\n",
    "  xloc = np.arange(StaticQMatrix.shape[1])\n",
    "  width = .6\n",
    "\n",
    "  xt = [i[0].split()[1] for i in StaticNames]\n",
    "  print(xt)\n",
    "  for Feature in range(StaticQMatrix.shape[1]):\n",
    "    plot_title = 'Static Feature'\n",
    "    for Quantile in range(len(qs)):\n",
    "      if qs[Quantile] == .5:\n",
    "        plt.bar(Feature,StaticQMatrix[Quantile,Feature],.2,color='blue')\n",
    "      elif qs[Quantile] < .5:\n",
    "        qlabel = str(qs[Quantile]) + ' Quantile'\n",
    "        plt.bar(Feature-width/3,StaticQMatrix[Quantile,Feature],.2,color='orange',alpha=.5)#,label=qlabel)\n",
    "      elif qs[Quantile] > .5:\n",
    "        qlabel = str(qs[Quantile]) + ' Quantile'\n",
    "        plt.bar(Feature+width/3,StaticQMatrix[Quantile,Feature],.2,color='green',alpha=.5)#,label=qlabel)\n",
    "  plt.xticks(xloc,xt)\n",
    "  plt.title(plot_title)\n",
    "  leg = [str(i) + ' quantile' for i in qs]\n",
    "  plt.legend(leg,loc='upper right')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIHPso_LrPJy"
   },
   "source": [
    "###Organize Location v Time Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "1WsspqAef_yR",
    "outputId": "e8ca5da0-4738-4948-8787-14bc2bc72e23"
   },
   "outputs": [],
   "source": [
    "def ProduceIndividualPlots(Observations, FitPredictions):\n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + current_time + ' Produce Individual Plots ' + RunName + ' ' + RunComment + resetfonts)\n",
    "# Find Best and Worst Locations\n",
    "  fips_b, fips_w = bestandworst(Observations, FitPredictions)\n",
    "  if Hydrology or Earthquake:\n",
    "    plot_by_fips(fips_b, Observations, FitPredictions)\n",
    "    plot_by_fips(fips_w, Observations, FitPredictions)\n",
    "  else:\n",
    "    for i in list(map(str,list(FIPSintegerlookup.keys())[::10])):\n",
    "      plot_by_fips(int(i),Observations,FitPredictions)\n",
    "\n",
    "    # plot_by_fips(6037, Observations, FitPredictions)\n",
    "    # plot_by_fips(41017, Observations, FitPredictions)\n",
    "    # plot_by_fips(48167, Observations, FitPredictions)\n",
    "    # plot_by_fips(1003, Observations, FitPredictions)\n",
    "    # plot_by_fips(42019, Observations, FitPredictions)\n",
    "    # plot_by_fips(10005, Observations, FitPredictions)\n",
    "    # plot_by_fips(15001, Observations, FitPredictions)\n",
    "    if (fips_b!=6037) and (fips_b!=36061) and (fips_b!=17031) and (fips_b!=53033):\n",
    "        plot_by_fips(fips_b, Observations, FitPredictions)\n",
    "    if (fips_w!=6037) and (fips_w!=36061) and (fips_w!=17031) and (fips_w!=53033):\n",
    "        plot_by_fips(fips_w, Observations, FitPredictions)\n",
    "\n",
    "  # Plot top 10 largest cities\n",
    "    sortedcities = np.flip(np.argsort(Locationpopulation))\n",
    "    for pickout in range (0,10):\n",
    "      Locationindex = sortedcities[pickout]\n",
    "      fips = Locationfips[Locationindex]\n",
    "      if not(Hydrology or Earthquake):\n",
    "        if (fips == 6037 or fips == 36061 or fips == 17031 or fips == 53033):\n",
    "          continue\n",
    "      if (fips == fips_b or fips == fips_w):\n",
    "        continue\n",
    "      plot_by_fips(fips, Observations, FitPredictions)\n",
    "      \n",
    "  if LengthFutures > 1:\n",
    "      plot_by_futureindex(2, Observations, FitPredictions)\n",
    "  if LengthFutures > 6:\n",
    "      plot_by_futureindex(7, Observations, FitPredictions)\n",
    "  if LengthFutures > 11:\n",
    "      plot_by_futureindex(12, Observations, FitPredictions)\n",
    "  return\n",
    "\n",
    "def bestandworst(Observations, FitPredictions):\n",
    "    current_time = timenow()\n",
    "    print(startbold +  startred + current_time + ' ' + RunName + \" Best and Worst \" +RunComment + resetfonts)\n",
    "\n",
    "    keepabserrorvalues = np.zeros([Nloc,NumpredbasicperTime], dtype=np.float64)\n",
    "    keepRMSEvalues = np.zeros([Nloc,NumpredbasicperTime], dtype=np.float64)\n",
    "    testabserrorvalues = np.zeros(Nloc, dtype=np.float64)\n",
    "    testRMSEvalues = np.zeros(Nloc, dtype=np.float64)\n",
    "\n",
    "    real = np.zeros([NumpredbasicperTime,Num_Seq], dtype=np.float64)\n",
    "    predictsmall = np.zeros([NumpredbasicperTime,Num_Seq], dtype=np.float64) \n",
    "    c_error_props = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
    "    c_error_props = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
    " \n",
    "  \n",
    "    for icity in range(0,Nloc):\n",
    "      validcounts = np.zeros([NumpredbasicperTime], dtype=np.float64) \n",
    "      RMSE = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        for itime in range (0,Num_Seq):\n",
    "          if not math.isnan(Observations[itime, icity, PredictedQuantity]):\n",
    "            real[PredictedQuantity,itime] = Observations[itime, icity, PredictedQuantity]\n",
    "            predictsmall[PredictedQuantity,itime] = FitPredictions[itime, icity, PredictedQuantity]\n",
    "            validcounts[PredictedQuantity] += 1.0\n",
    "            RMSE[PredictedQuantity] += (Observations[itime, icity, PredictedQuantity]-FitPredictions[itime, icity, PredictedQuantity])**2\n",
    "        c_error_props[PredictedQuantity] = cumulative_error(predictsmall[PredictedQuantity], real[PredictedQuantity]) # abs(error) as percentage\n",
    "        keepabserrorvalues[icity,PredictedQuantity] = c_error_props[PredictedQuantity]\n",
    "        keepRMSEvalues[icity,PredictedQuantity] = RMSE[PredictedQuantity] *100. / validcounts[PredictedQuantity]\n",
    "\n",
    "      testabserror = 0.0\n",
    "      testRMSE = 0.0\n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "         testabserror += c_error_props[PredictedQuantity]\n",
    "         testRMSE += keepRMSEvalues[icity,PredictedQuantity]\n",
    "      testabserrorvalues[icity] = testabserror\n",
    "      testRMSEvalues[icity] = testRMSE\n",
    "    \n",
    "    sortingindex = np.argsort(testabserrorvalues)\n",
    "    bestindex = sortingindex[0]\n",
    "    worstindex = sortingindex[Nloc-1]\n",
    "    fips_b = Locationfips[bestindex]\n",
    "    fips_w = Locationfips[worstindex]\n",
    "\n",
    "    current_time = timenow()\n",
    "    print( startbold + \"\\n\" + current_time + \" Best \" + str(fips_b) + \" \" + Locationname[bestindex] + \" \" + Locationstate[bestindex] + ' ABS(error) ' + \n",
    "          str(round(testabserrorvalues[bestindex],2)) + ' RMSE ' + str(round(testRMSEvalues[bestindex],2)) + resetfonts)\n",
    "     \n",
    "    for topcities in range(0,10):\n",
    "      localindex = sortingindex[topcities]\n",
    "      printstring = str(topcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] + \" ABS(error) Total \" + str(round(testabserrorvalues[localindex],4)) + \" Components \" \n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        printstring += ' ' + str(round(keepabserrorvalues[localindex,PredictedQuantity],2))\n",
    "      print(printstring)\n",
    "    print(\"\\nlist RMSE\")\n",
    "    for topcities in range(0,9):\n",
    "      localindex = sortingindex[topcities]\n",
    "      printstring = str(topcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" RMSE Total \" + str(round(testRMSEvalues[localindex],4)) + \" Components \" \n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        printstring += ' ' + str(round(keepRMSEvalues[localindex,PredictedQuantity],2))\n",
    "      print(printstring)\n",
    "\n",
    "    print( startbold + \"\\n\" + current_time + \" Worst \" + str(fips_w) + \" \" + Locationname[worstindex] + \" \" + Locationstate[worstindex] + ' ABS(error) ' + \n",
    "          str(round(testabserrorvalues[worstindex],2)) + ' RMSE ' + str(round(testRMSEvalues[worstindex],2)) + resetfonts)\n",
    " \n",
    "    for badcities in range(Nloc-1,Nloc-11,-1):\n",
    "      localindex = sortingindex[badcities]\n",
    "      printstring = str(badcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" ABS(error) Total \" + str(round(testabserrorvalues[localindex],4)) + \" Components \" \n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        printstring += ' ' + str(round(keepabserrorvalues[localindex,PredictedQuantity],2))\n",
    "      print(printstring)\n",
    "    print(\"\\nlist RMSE\")\n",
    "    for badcities in range(0,9):\n",
    "      localindex = sortingindex[badcities]\n",
    "      printstring = str(badcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" RMSE Total \" + str(round(testRMSEvalues[localindex],4)) + \" Components \" \n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        printstring += ' ' + str(round(keepRMSEvalues[localindex,PredictedQuantity],2))\n",
    "      print(printstring)\n",
    " \n",
    "    return fips_b,fips_w  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S2QaUybnLTb"
   },
   "source": [
    "### Summed & By Location Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GrWzXpoTa18c",
    "outputId": "454d48ba-04ac-43e7-9b68-b22a71f72d07"
   },
   "outputs": [],
   "source": [
    "def setValTrainlabel(iValTrain):\n",
    "\n",
    "  if SeparateValandTrainingPlots:\n",
    "    if iValTrain == 0:\n",
    "      Overalllabel = 'Training ' \n",
    "      if GlobalTrainingLoss > 0.0001:\n",
    "        Overalllabel += str(round(GlobalTrainingLoss,5)) + ' '\n",
    "    if iValTrain == 1:\n",
    "      Overalllabel = 'Validation '\n",
    "      if GlobalValidationLoss > 0.0001:\n",
    "        Overalllabel += str(round(GlobalValidationLoss,5)) + ' '\n",
    "  else:\n",
    "    Overalllabel = 'Full ' + str(round(GlobalLoss,5)) + ' '\n",
    "  Overalllabel += RunName + ' '\n",
    "  return Overalllabel\n",
    "\n",
    "def Location_summed_plot(Observations, FitPredictions,  fill=True, otherlabs= [], otherfits=[], extracomments = None, Dumpplot = False):\n",
    "    # Only deal with futures as days; plot sum over locations\n",
    "    current_time = timenow()\n",
    "    print(wraptotext(startbold + startred + current_time + ' Location_summed_plot ' + RunName + ' ' + RunComment + resetfonts))\n",
    "    otherlen = len(otherlabs)\n",
    "    basiclength = Num_Seq\n",
    "    predictlength = LengthFutures\n",
    "    if not UseFutures:\n",
    "        predictlength = 0\n",
    "    totallength = basiclength + predictlength\n",
    "    if extracomments is None:\n",
    "      extracomments = []\n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        extracomments.append([' ',''])\n",
    "\n",
    "    NumberValTrainLoops = 1\n",
    "    if SeparateValandTrainingPlots:\n",
    "      NumberValTrainLoops = 2\n",
    "        \n",
    "    real = np.zeros([NumpredbasicperTime,NumberValTrainLoops,basiclength])\n",
    "    predictsmall = np.zeros([NumpredbasicperTime,NumberValTrainLoops,basiclength])\n",
    "    predict = np.zeros([NumpredbasicperTime,NumberValTrainLoops,totallength])\n",
    "    if otherlen!=0:\n",
    "      otherpredict = np.zeros([otherlen,NumpredbasicperTime,NumberValTrainLoops, totallength])  \n",
    "\n",
    "      \n",
    "    for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "      for iValTrain in range(0,NumberValTrainLoops):\n",
    "\n",
    "        for iloc in range(0,Nloc):\n",
    "          if SeparateValandTrainingPlots:\n",
    "            if iValTrain == 0:\n",
    "              if MappingtoTraining[iloc] < 0:\n",
    "                continue\n",
    "            if iValTrain == 1:\n",
    "              if MappingtoTraining[iloc] >= 0:\n",
    "                continue\n",
    "          for itime in range (0,Num_Seq):\n",
    "            if np.math.isnan(Observations[itime, iloc, PredictedQuantity]):\n",
    "              real[PredictedQuantity,iValTrain,itime] += FitPredictions[itime, iloc, PredictedQuantity]\n",
    "            else:\n",
    "              real[PredictedQuantity,iValTrain,itime] += Observations[itime, iloc, PredictedQuantity]\n",
    "            predict[PredictedQuantity,iValTrain,itime] += FitPredictions[itime, iloc, PredictedQuantity]\n",
    "            for others in range (0,otherlen):\n",
    "              otherpredict[others,PredictedQuantity,iValTrain,itime] += FitPredictions[itime, iloc, PredictedQuantity] + otherfits[others,itime, iloc, PredictedQuantity]\n",
    "          if FuturedPointer[PredictedQuantity] >= 0:\n",
    "            for ifuture in range(0,LengthFutures):\n",
    "              jfuture = NumpredbasicperTime + NumpredFuturedperTime*ifuture\n",
    "              predict[PredictedQuantity,iValTrain,Num_Seq+ifuture] += FitPredictions[itime, iloc, \n",
    "                                      FuturedPointer[PredictedQuantity] + jfuture] \n",
    "              for others in range (0,otherlen):\n",
    "                otherpredict[others,PredictedQuantity,iValTrain,Num_Seq+ifuture] += FitPredictions[itime, iloc, PredictedQuantity + jfuture] + otherfits[others, itime, iloc, PredictedQuantity + jfuture]\n",
    "        for itime in range(0,basiclength):\n",
    "            predictsmall[PredictedQuantity,iValTrain,itime] = predict[PredictedQuantity,iValTrain,itime]\n",
    "          \n",
    "    error = np.absolute(real - predictsmall)\n",
    "    xsmall = np.arange(0,Num_Seq)\n",
    "\n",
    "    neededrows = math.floor((NumpredbasicperTime*NumberValTrainLoops +1.1)/2)\n",
    "    iValTrain = -1\n",
    "    PredictedQuantity = -1\n",
    "    for rowloop in range(0,neededrows):\n",
    "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "      for kplot in range (0,2):\n",
    "        if NumberValTrainLoops == 2:\n",
    "          iValTrain = kplot\n",
    "        else:\n",
    "          iValTrain = 0\n",
    "        if iValTrain == 0:\n",
    "          PredictedQuantity +=1\n",
    "          if PredictedQuantity > (NumpredbasicperTime-1):\n",
    "            PredictedQuantity = NumpredbasicperTime-1          \n",
    "        Overalllabel = setValTrainlabel(iValTrain)\n",
    "        \n",
    "        eachplt = ax1\n",
    "        if kplot == 1:\n",
    "          eachplt = ax2\n",
    "        \n",
    "        Overalllabel = 'Full '\n",
    "        if SeparateValandTrainingPlots:\n",
    "          if iValTrain == 0:\n",
    "            Overalllabel = 'Training ' \n",
    "            if GlobalTrainingLoss > 0.0001:\n",
    "              Overalllabel += str(round(GlobalTrainingLoss,5)) + ' '\n",
    "          if iValTrain == 1:\n",
    "            Overalllabel = 'Validation '\n",
    "            if GlobalValidationLoss > 0.0001:\n",
    "              Overalllabel += str(round(GlobalValidationLoss,5)) + ' '\n",
    "        else:\n",
    "          Overalllabel += RunName + ' ' + str(round(GlobalLoss,5)) + ' '\n",
    "\n",
    "        maxplot = np.float32(totallength)\n",
    "        if UseRealDatesonplots:\n",
    "          StartDate = np.datetime64(InitialDate).astype('datetime64[D]') + np.timedelta64(Tseq*Dailyunit + math.floor(Dailyunit/2),'D')\n",
    "          EndDate = StartDate + np.timedelta64(totallength*Dailyunit)\n",
    "          datemin, datemax = makeadateplot(figure,eachplt, datemin=StartDate, datemax=EndDate)\n",
    "          Dateplot = True\n",
    "          Dateaxis = np.empty(totallength, dtype = 'datetime64[D]')\n",
    "          Dateaxis[0] = StartDate\n",
    "          for idate in range(1,totallength):\n",
    "            Dateaxis[idate] = Dateaxis[idate-1] + np.timedelta64(Dailyunit,'D')\n",
    "        else:\n",
    "          Dateplot = False\n",
    "          datemin = 0.0\n",
    "          datemax = maxplot\n",
    "\n",
    "        sumreal = 0.0\n",
    "        sumerror = 0.0\n",
    "        for itime in range(0,Num_Seq):\n",
    "          sumreal += abs(real[PredictedQuantity,iValTrain,itime])\n",
    "          sumerror += error[PredictedQuantity,iValTrain,itime]\n",
    "        c_error = round(100.0*sumerror/sumreal,2)\n",
    "\n",
    "        if UseRealDatesonplots:\n",
    "          eachplt.plot(Dateaxis[0:real.shape[-1]],real[PredictedQuantity,iValTrain,:], label=f'real')\n",
    "          eachplt.plot(Dateaxis,predict[PredictedQuantity,iValTrain,:], label='prediction')            \n",
    "          eachplt.plot(Dateaxis[0:error.shape[-1]],error[PredictedQuantity,iValTrain,:], label=f'error', color=\"red\")\n",
    "          for others in range (0,otherlen):\n",
    "            eachplt.plot(Dateaxis[0:otherpredict.shape[-1]],otherpredict[others,PredictedQuantity,iValTrain,:], label=otherlabs[others])\n",
    "\n",
    "          if fill:\n",
    "            eachplt.fill_between(Dateaxis[0:predictsmall.shape[-1]], predictsmall[PredictedQuantity,iValTrain,:], \n",
    "                                 real[PredictedQuantity,iValTrain,:], alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(Dateaxis[0:error.shape[-1]], error[PredictedQuantity,iValTrain,:], alpha=0.05, color=\"red\")\n",
    "\n",
    "        else:\n",
    "\n",
    "          ## Andrej Testing\n",
    "          print('ANDREJ IN THE LOOP')\n",
    "          # real[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, Locationindex, Observations[itime, Locationindex, PredictedQuantity])\n",
    "          # predict[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, Locationindex, PredictedQuantity])\n",
    "\n",
    "          print(real.shape,predict.shape,error.shape)\n",
    "\n",
    "          new_real = np.zeros(real.shape)\n",
    "          new_real = np.squeeze(new_real)\n",
    "          new_error = np.zeros(error.shape)\n",
    "          new_error = np.squeeze(new_error)\n",
    "          new_predict = np.zeros(predict.shape)\n",
    "          new_predict = np.squeeze(new_predict)\n",
    "\n",
    "          for itime in range(new_real.shape[1]):\n",
    "            new_real[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, 0, real[PredictedQuantity,iValTrain,itime])\n",
    "\n",
    "          for itime in range(new_predict.shape[1]):\n",
    "            new_predict[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, 0, predict[PredictedQuantity,iValTrain,itime])\n",
    "\n",
    "          for itime in range(new_error.shape[1]):\n",
    "            new_error[PredictedQuantity,itime] = np.absolute(new_real[PredictedQuantity, itime] - new_predict[PredictedQuantity, itime])\n",
    "\n",
    "          \n",
    "          real[PredictedQuantity,iValTrain,:] = new_real[PredictedQuantity,:]\n",
    "          predict[PredictedQuantity,iValTrain,:] = new_predict[PredictedQuantity,:]\n",
    "          error[PredictedQuantity,iValTrain,:] = new_error[PredictedQuantity,:]\n",
    "          \n",
    "\n",
    "          ## Andrej Testing End\n",
    "          eachplt.plot(real[PredictedQuantity,iValTrain,:], label=f'real')\n",
    "          eachplt.plot(predict[PredictedQuantity,iValTrain,:], label='prediction')\n",
    "          eachplt.plot(error[PredictedQuantity,iValTrain,:], label=f'error', color=\"red\")\n",
    "          for others in range (0,otherlen):\n",
    "            eachplt.plot(otherpredict[others,PredictedQuantity,iValTrain,:], label=otherlabs[others])\n",
    "\n",
    "          if fill:\n",
    "            eachplt.fill_between(xsmall, predictsmall[PredictedQuantity,iValTrain,:], real[PredictedQuantity,iValTrain,:], \n",
    "                                 alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(xsmall, error[PredictedQuantity,iValTrain,:], alpha=0.05, color=\"red\")\n",
    "\n",
    "        \n",
    "        if Earthquake and AddSpecialstoSummedplots:\n",
    "          if NumberValTrainLoops == 2:\n",
    "            if iValTrain == 0:\n",
    "              Addfixedearthquakes(eachplt, datemin, datemax, quakecolor = 'black', Dateplot = Dateplot, \n",
    "                                  vetoquake = PrimaryTrainingvetoquake)\n",
    "              Addfixedearthquakes(eachplt, datemin, datemax,  quakecolor = 'purple', Dateplot = Dateplot, \n",
    "                                  vetoquake = SecondaryTrainingvetoquake)\n",
    "            else:\n",
    "              Addfixedearthquakes(eachplt, datemin, datemax,  quakecolor = 'black', Dateplot = Dateplot, \n",
    "                                  vetoquake = PrimaryValidationvetoquake)\n",
    "              Addfixedearthquakes(eachplt, datemin, datemax, quakecolor = 'purple', Dateplot = Dateplot, \n",
    "                                  vetoquake = SecondaryValidationvetoquake)\n",
    "          else:\n",
    "            vetoquake = np.full(numberspecialeqs,False, dtype = np.bool)\n",
    "            Addfixedearthquakes(eachplt, datemin, datemax,  quakecolor = 'black', Dateplot = Dateplot, \n",
    "                                vetoquake = vetoquake)\n",
    "          \n",
    "        extrastring = Overalllabel + current_time + ' ' + RunName + \" \" \n",
    "        extrastring += f\"Length={Num_Seq}, Location Summed Results {Predictionbasicname[PredictedQuantity]}, \"\n",
    "        extrastring += extracomments[PredictedQuantity][iValTrain]\n",
    "        eachplt.set_title('\\n'.join(wrap(extrastring,70)))\n",
    "        if Dateplot:\n",
    "          eachplt.set_xlabel('Years')\n",
    "        else:\n",
    "          eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
    "        eachplt.set_ylabel(Predictionbasicname[PredictedQuantity])\n",
    "        eachplt.grid(False)\n",
    "        eachplt.legend()\n",
    "      figure.tight_layout()\n",
    "      if Dumpplot and Dumpoutkeyplotsaspics:\n",
    "        VT = 'Both'\n",
    "        if NumberValTrainLoops == 1:\n",
    "          VT='Full'\n",
    "        plt.savefig(APPLDIR +'/Outputs/DLResults' + VT + str(PredictedQuantity) +RunName + '.png ',format='png')\n",
    "      plt.show()\n",
    "\n",
    "# Produce more detailed plots in time\n",
    "    splitsize = Plotsplitsize\n",
    "    if splitsize <= 1:\n",
    "      return\n",
    "    Numpoints = math.floor((Num_Seq+0.001)/splitsize)\n",
    "    extraone = Num_Seq%Numpoints\n",
    "\n",
    "    neededrows = math.floor((splitsize*NumberValTrainLoops +1.1)/2)\n",
    "    iValTrain = -1\n",
    "    PredictedQuantity = 0\n",
    "    iseqnew = 0\n",
    "    counttimes = 0\n",
    "    for rowloop in range(0,neededrows):\n",
    "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "      for kplot in range (0,2):\n",
    "        if NumberValTrainLoops == 2:\n",
    "          iValTrain = kplot\n",
    "        else:\n",
    "          iValTrain = 0\n",
    "        Overalllabel = setValTrainlabel(iValTrain)\n",
    "        eachplt = ax1\n",
    "        if kplot == 1:\n",
    "          eachplt = ax2\n",
    "        sumreal = 0.0\n",
    "        sumerror = 0.0\n",
    "\n",
    "        if iValTrain == 0:\n",
    "          iseqold = iseqnew\n",
    "          iseqnew = iseqold + Numpoints\n",
    "          if counttimes < extraone:\n",
    "            iseqnew +=1\n",
    "          counttimes += 1\n",
    "        for itime in range(iseqold,iseqnew):\n",
    "          sumreal += abs(real[PredictedQuantity,iValTrain,itime])\n",
    "          sumerror += error[PredictedQuantity,iValTrain,itime]\n",
    "        c_error = round(100.0*sumerror/sumreal,2)\n",
    "\n",
    "        eachplt.plot(xsmall[iseqold:iseqnew],predict[PredictedQuantity,iValTrain,iseqold:iseqnew], label='prediction')\n",
    "        eachplt.plot(xsmall[iseqold:iseqnew],real[PredictedQuantity,iValTrain,iseqold:iseqnew], label=f'real')\n",
    "        eachplt.plot(xsmall[iseqold:iseqnew],error[PredictedQuantity,iValTrain,iseqold:iseqnew], label=f'error', color=\"red\")\n",
    "\n",
    "        if fill:\n",
    "            eachplt.fill_between(xsmall[iseqold:iseqnew], predictsmall[PredictedQuantity,iValTrain,iseqold:iseqnew], real[PredictedQuantity,iseqold:iseqnew], alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(xsmall[iseqold:iseqnew], error[PredictedQuantity,iValTrain,iseqold:iseqnew], alpha=0.05, color=\"red\")\n",
    "\n",
    "        extrastring = Overalllabel + current_time + ' ' + RunName + \" \" + f\"Range={iseqold}, {iseqnew} Rel Error {c_error} Location Summed Results {Predictionbasicname[PredictedQuantity]}, \"\n",
    "        eachplt.set_title('\\n'.join(wrap(extrastring,70)))\n",
    "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
    "        eachplt.set_ylabel(Predictionbasicname[PredictedQuantity])\n",
    "        eachplt.grid(True)\n",
    "        eachplt.legend()\n",
    "      figure.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "def normalizeforplot(casesdeath,Locationindex,value):\n",
    "\n",
    "    if np.math.isnan(value):\n",
    "      return value\n",
    "    if Plotrealnumbers:\n",
    "      predaveragevaluespointer = PredictionAverageValuesPointer[casesdeath]\n",
    "      newvalue = value/QuantityStatistics[predaveragevaluespointer,2] + QuantityStatistics[predaveragevaluespointer,0]\n",
    "      rootflag = QuantityTakeroot[predaveragevaluespointer]\n",
    "      if rootflag == 2:\n",
    "        newvalue = newvalue**2\n",
    "      if rootflag == 3:\n",
    "        newvalue = newvalue**3\n",
    "    else:\n",
    "      newvalue = value\n",
    "    if PopulationNorm:\n",
    "      newvalue *= Locationpopulation[Locationindex]\n",
    "    return newvalue\n",
    "\n",
    "# PLOT individual city data\n",
    "def plot_by_fips(fips, Observations, FitPredictions, dots=True, fill=True):\n",
    "    Locationindex = FIPSintegerlookup[fips]\n",
    "    current_time = timenow()\n",
    "    print(startbold + startred + current_time + ' plot by location ' + str(Locationindex) + ' population ' + str(Locationpopulation[Locationindex]) + ' ' + str(fips) + ' ' + Locationname[Locationindex] + ' ' +RunName + ' ' + RunComment + resetfonts)\n",
    "\n",
    "    basiclength = Num_Seq\n",
    "    predictlength = LengthFutures\n",
    "    if not UseFutures:\n",
    "        predictlength = 0\n",
    "    totallength = basiclength + predictlength\n",
    "    real = np.zeros([NumpredbasicperTime,basiclength])\n",
    "    predictsmall = np.zeros([NumpredbasicperTime,basiclength])\n",
    "    predict = np.zeros([NumpredbasicperTime,totallength]) \n",
    "\n",
    "    for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "      for itime in range (0,Num_Seq):\n",
    "        if np.math.isnan(Observations[itime, Locationindex, PredictedQuantity]):\n",
    "          Observations[itime, Locationindex, PredictedQuantity] = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
    "        else:\n",
    "          real[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, Locationindex, Observations[itime, Locationindex, PredictedQuantity])\n",
    "          predict[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, Locationindex, PredictedQuantity])\n",
    "      if FuturedPointer[PredictedQuantity] >= 0:\n",
    "        for ifuture in range(0,LengthFutures):\n",
    "          jfuture = NumpredbasicperTime + NumpredFuturedperTime*ifuture\n",
    "          predict[PredictedQuantity,Num_Seq+ifuture] += normalizeforplot(PredictedQuantity,Locationindex, \n",
    "                                          FitPredictions[itime, Locationindex, FuturedPointer[PredictedQuantity] + jfuture])\n",
    "      for itime in range(0,basiclength):\n",
    "          predictsmall[PredictedQuantity,itime] = predict[PredictedQuantity,itime]\n",
    "        \n",
    "    error = np.absolute(real - predictsmall)\n",
    "    xsmall = np.arange(0,Num_Seq)\n",
    "\n",
    "    neededrows = math.floor((NumpredbasicperTime +1.1)/2)\n",
    "    iplot = -1\n",
    "    for rowloop in range(0,neededrows):\n",
    "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "      for kplot in range (0,2):\n",
    "        iplot +=1\n",
    "        if iplot > (NumpredbasicperTime-1):\n",
    "          iplot = NumpredbasicperTime-1\n",
    "        eachplt = ax1\n",
    "        if kplot == 1:\n",
    "          eachplt = ax2\n",
    "\n",
    "        sumreal = 0.0\n",
    "        sumerror = 0.0\n",
    "        for itime in range(0,Num_Seq):\n",
    "          sumreal += abs(real[iplot,itime])\n",
    "          sumerror += error[iplot,itime]\n",
    "        c_error = round(100.0*sumerror/sumreal,2)\n",
    "        RMSEstring = ''\n",
    "        if not Plotrealnumbers:\n",
    "          sumRMSE = 0.0\n",
    "          count = 0.0\n",
    "          for itime in range(0,Num_Seq):\n",
    "            sumRMSE += (real[iplot,itime] - predict[iplot,itime])**2\n",
    "            count += 1.0\n",
    "          RMSE_error = round(100.0*sumRMSE/count,4)\n",
    "          RMSEstring = ' RMSE ' + str(RMSE_error)\n",
    "\n",
    "        x = list(range(0, totallength))\n",
    "        if dots:\n",
    "            eachplt.scatter(x, predict[iplot])\n",
    "            eachplt.scatter(xsmall, real[iplot])\n",
    "\n",
    "        eachplt.plot(predict[iplot], label=f'{fips} prediction')\n",
    "        eachplt.plot(real[iplot], label=f'{fips} real')\n",
    "        eachplt.plot(error[iplot], label=f'{fips} error', color=\"red\")\n",
    "        if fill:\n",
    "            eachplt.fill_between(xsmall, predictsmall[iplot], real[iplot], alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(xsmall, error[iplot], alpha=0.05, color=\"red\")\n",
    "\n",
    "        name = Locationname[Locationindex]\n",
    "        if Plotrealnumbers:\n",
    "            name = \"Actual Numbers \" + name\n",
    "        stringpopulation = \" \"\n",
    "        if not Hydrology:\n",
    "          stringpopulation = \" Population \" +str(Locationpopulation[Locationindex])\n",
    "\n",
    "        titlestring = current_time + ' ' + RunName + f\" {name}, Label={fips}\" + stringpopulation + f\" Length={Num_Seq}, Abs Rel Error={c_error}%\" + RMSEstring + ' ' + RunName\n",
    "        eachplt.set_title('\\n'.join(wrap(titlestring,70)))\n",
    "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
    "        eachplt.set_ylabel(Predictionbasicname[iplot])\n",
    "        eachplt.grid(True)\n",
    "        eachplt.legend()\n",
    "\n",
    "      figure.tight_layout()\n",
    "      plt.show();\n",
    "\n",
    "\n",
    "def cumulative_error(real,predicted):\n",
    "  error = np.absolute(real-predicted).sum()\n",
    "  basevalue = np.absolute(real).sum()\n",
    "  return 100.0*error/basevalue\n",
    "\n",
    "# Plot summed results by Prediction Type\n",
    "# selectedfuture one more than usual future index\n",
    "def plot_by_futureindex(selectedfuture, Observations, FitPredictions, fill=True, extrastring=''):\n",
    "    current_time = timenow()\n",
    "    print(startbold + startred + current_time + ' plot by Future Index ' + str(selectedfuture) + ' ' + RunName + ' ' + RunComment + resetfonts)\n",
    "\n",
    "    \n",
    "    selectedfield = NumpredbasicperTime + NumpredFuturedperTime*(selectedfuture-1)\n",
    "    if selectedfuture == 0:\n",
    "      selectedfield = 0\n",
    "    real = np.zeros([NumpredFuturedperTime,Num_Seq])\n",
    "    predictsmall = np.zeros([NumpredFuturedperTime,Num_Seq])\n",
    "    validdata = 0\n",
    "\n",
    "    for PredictedQuantity in range(0,NumpredFuturedperTime):\n",
    "      for iloc in range(0,Nloc):\n",
    "        for itime in range (0,Num_Seq):\n",
    "          temp = normalizeforplot(PredictedQuantity, 0, Observations[itime, iloc, selectedfield+PredictedQuantity])\n",
    "          real[PredictedQuantity,itime] += temp#Observations[itime, iloc, selectedfield+PredictedQuantity]\n",
    "          temp2 = normalizeforplot(PredictedQuantity, 0, FitPredictions[itime, iloc, selectedfield+PredictedQuantity])\n",
    "          predictsmall[PredictedQuantity,itime] += temp2 #FitPredictions[itime, iloc, selectedfield+PredictedQuantity]\n",
    "      for itime in range (0,Num_Seq):\n",
    "        if np.math.isnan(real[PredictedQuantity,itime]):\n",
    "            real[PredictedQuantity,itime] = predictsmall[PredictedQuantity,itime]\n",
    "        else:\n",
    "            if PredictedQuantity == 0:\n",
    "              validdata += 1    \n",
    "\n",
    "    error = np.absolute(real - predictsmall)\n",
    "    xsmall = np.arange(0,Num_Seq)\n",
    "\n",
    "    neededrows = math.floor((NumpredFuturedperTime +1.1)/2)\n",
    "    iplot = -1\n",
    "    for rowloop in range(0,neededrows):\n",
    "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "      for kplot in range (0,2):\n",
    "        iplot +=1\n",
    "        if iplot > (NumpredbasicperTime-1):\n",
    "          iplot = NumpredbasicperTime-1\n",
    "        eachplt = ax1\n",
    "        if kplot == 1:\n",
    "          eachplt = ax2\n",
    "        sumreal = 0.0\n",
    "        sumerror = 0.0\n",
    "        for itime in range(0,Num_Seq):\n",
    "          sumreal += abs(real[iplot,itime])\n",
    "          sumerror += error[iplot,itime]\n",
    "        c_error = round(100.0*sumerror/sumreal,2)\n",
    "\n",
    "        eachplt.plot(predictsmall[iplot,:], label='prediction')\n",
    "        eachplt.plot(real[iplot,:], label=f'real')\n",
    "        eachplt.plot(error[iplot,:], label=f'error', color=\"red\")\n",
    "\n",
    "        if fill:\n",
    "            eachplt.fill_between(xsmall, predictsmall[iplot,:], real[iplot,:], alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(xsmall, error[iplot,:], alpha=0.05, color=\"red\")\n",
    "        errorstring= \" Error % \" + str(c_error)\n",
    "        printstring = current_time + \" Future Index \" + str(selectedfuture) + \" \" + RunName \n",
    "        printstring += \" \" + f\"Length={Num_Seq}, Location Summed Results {Predictionbasicname[iplot]}, \" + errorstring + \" \" + extrastring\n",
    "        eachplt.set_title('\\n'.join(wrap(printstring,70)))\n",
    "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
    "        eachplt.set_ylabel(Predictionbasicname[iplot])\n",
    "        eachplt.grid(True)\n",
    "        eachplt.legend()\n",
    "      figure.tight_layout()\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVxWS_-p5T_N"
   },
   "source": [
    "###Calculate NNSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "dHr9p5LC5Z-k",
    "outputId": "f9837642-0c50-482f-ca83-b6cd7cdd5a85"
   },
   "outputs": [],
   "source": [
    "# Calculate NNSE\n",
    "# Sum (Obsevations - Mean)^2 / [Sum (Obsevations - Mean)^2 + Sum(Observations-Predictions)^2]\n",
    "def FindNNSE(Observations, FitPredictions, Label=''):\n",
    "\n",
    "  NNSEList = np.empty(NpredperseqTOT, dtype = np.int)\n",
    "  NumberNNSEcalc = 0\n",
    "  for ipred in range(0,NpredperseqTOT):\n",
    "    if CalculateNNSE[ipred]:\n",
    "      NNSEList[NumberNNSEcalc] = ipred\n",
    "      NumberNNSEcalc +=1\n",
    "  if NumberNNSEcalc == 0:\n",
    "    return\n",
    "  StoreNNSE = np.zeros([Nloc,NumberNNSEcalc], dtype = np.float64)\n",
    "  basiclength = Num_Seq\n",
    "\n",
    "  current_time = timenow()\n",
    "  print(wraptotext(startbold + startred + current_time + ' Calculate NNSE ' + Label + ' ' +RunName + ' ' + RunComment + resetfonts))\n",
    "  for NNSEpredindex in range(0,NumberNNSEcalc):\n",
    "    PredictedQuantity = NNSEList[NNSEpredindex]\n",
    "    averageNNSE = 0.0\n",
    "    averageNNSETraining = 0.0\n",
    "    averageNNSEValidation = 0.0\n",
    "    line = ''\n",
    "    for Locationindex in range(0, Nloc):\n",
    "      QTObssq = 0.0\n",
    "      QTDiffsq = 0.0\n",
    "      QTObssum = 0.0\n",
    "      for itime in range (0,Num_Seq):\n",
    "        Observed = Observations[itime, Locationindex, PredictedQuantity]\n",
    "        if np.math.isnan(Observed):\n",
    "          Observed = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
    "        real = normalizeforplot(PredictedQuantity, Locationindex, Observed)\n",
    "        predict = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, \n",
    "                                    Locationindex, PredictedQuantity])\n",
    "        QTObssq += real**2\n",
    "        QTDiffsq += (real-predict)**2\n",
    "        QTObssum += real\n",
    "      Obsmeasure = QTObssq - (QTObssum**2 / Num_Seq )\n",
    "      StoreNNSE[Locationindex,NNSEpredindex] =  Obsmeasure / (Obsmeasure +QTDiffsq )\n",
    "      if MappingtoTraining[Locationindex] >= 0:\n",
    "        averageNNSETraining += StoreNNSE[Locationindex,NNSEpredindex]\n",
    "      if MappingtoValidation[Locationindex] >= 0:\n",
    "        averageNNSEValidation += StoreNNSE[Locationindex,NNSEpredindex]\n",
    "      averageNNSE += StoreNNSE[Locationindex,NNSEpredindex]\n",
    "      line += str(round(StoreNNSE[Locationindex,NNSEpredindex],3)) + ' '\n",
    "    \n",
    "    if ValidationNloc > 0:\n",
    "      averageNNSEValidation = averageNNSEValidation / ValidationNloc\n",
    "    averageNNSETraining = averageNNSETraining / TrainingNloc\n",
    "    averageNNSE = averageNNSE / Nloc\n",
    "\n",
    "# Location Summed    \n",
    "    QTObssq = 0.0\n",
    "    QTDiffsq = 0.0\n",
    "    QTObssum = 0.0\n",
    "    QTObssqT = 0.0\n",
    "    QTDiffsqT = 0.0\n",
    "    QTObssumT = 0.0\n",
    "    QTObssqV = 0.0\n",
    "    QTDiffsqV = 0.0\n",
    "    QTObssumV = 0.0\n",
    "    for itime in range (0,Num_Seq):\n",
    "      real = 0.0\n",
    "      predict = 0.0\n",
    "      realT = 0.0\n",
    "      predictT = 0.0\n",
    "      realV = 0.0\n",
    "      predictV = 0.0\n",
    "      for Locationindex in range(0, Nloc):\n",
    "        Observed = Observations[itime, Locationindex, PredictedQuantity]\n",
    "        if np.math.isnan(Observed):\n",
    "          Observed = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
    "        localreal = normalizeforplot(PredictedQuantity, Locationindex, Observed)\n",
    "        localpredict = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, \n",
    "                                    Locationindex, PredictedQuantity])\n",
    "        real += localreal\n",
    "        predict += localpredict\n",
    "        if MappingtoTraining[Locationindex] >= 0:\n",
    "          realT += localreal\n",
    "          predictT += localpredict\n",
    "        if MappingtoValidation[Locationindex] >= 0:\n",
    "          realV  += localreal\n",
    "          predictV += localpredict\n",
    "\n",
    "      QTObssq += real**2\n",
    "      QTDiffsq += (real-predict)**2\n",
    "      QTObssum += real\n",
    "      QTObssqT += realT**2\n",
    "      QTDiffsqT += (realT-predictT)**2\n",
    "      QTObssumT += realT\n",
    "      QTObssqV += realV**2\n",
    "      QTDiffsqV += (realV-predictV)**2\n",
    "      QTObssumV += realV\n",
    "    Obsmeasure = QTObssq - (QTObssum**2 / Num_Seq )\n",
    "    SummedNNSE =  Obsmeasure / (Obsmeasure +QTDiffsq )\n",
    "    ObsmeasureT = QTObssqT - (QTObssumT**2 / Num_Seq )\n",
    "    SummedNNSET =  ObsmeasureT / (ObsmeasureT +QTDiffsqT )\n",
    "    ObsmeasureV = QTObssqV - (QTObssumV**2 / Num_Seq )\n",
    "    if ValidationNloc > 0:\n",
    "      SummedNNSEV =  ObsmeasureV / (ObsmeasureV +QTDiffsqV )\n",
    "    else:\n",
    "      SummedNNSEV =  0.0\n",
    "\n",
    "    print(wraptotext('NNSE ' + startbold + Label + ' ' + str(PredictedQuantity) + ' ' + Predictionname[PredictedQuantity] + startred + ' Averaged ' +\n",
    "          str(round(averageNNSE,3)) + resetfonts + ' Training ' + str(round(averageNNSETraining,3)) +\n",
    "          ' Validation ' + str(round(averageNNSEValidation,3)) + startred + startbold + ' Summed ' +  \n",
    "          str(round(SummedNNSE,3)) + resetfonts + ' Training ' + str(round(SummedNNSET,3)) +\n",
    "          ' Validation ' + str(round(SummedNNSEV,3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "WOO6uzE1FUa1",
    "outputId": "b4fe3d30-3dd6-4961-982d-10576e275127"
   },
   "outputs": [],
   "source": [
    "def weightedcustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
    "    tupl = np.shape(y_actual)\n",
    "\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "\n",
    "def numpycustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
    "    tupl = np.shape(y_actual)\n",
    "\n",
    "    flagGCF = np.isnan(y_actual)\n",
    "    y_actual = y_actual[np.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[np.logical_not(flagGCF)]\n",
    "    sw = sample_weight[np.logical_not(flagGCF)]\n",
    "    tensordiff = np.sum(np.multiply(np.square(y_actual-y_pred),sw))\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "\n",
    "def weightedcustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
    "    tupl = np.shape(y_actual)\n",
    "\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeDyzoVynCHL"
   },
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lJylkkL9AvsV",
    "outputId": "5ce83d57-f6b2-44f8-f764-33bd5decf6c4"
   },
   "outputs": [],
   "source": [
    "def custom_lossGCF1(y_actual,y_pred):\n",
    "    tupl = np.shape(y_actual)\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.math.square(y_actual-y_pred))\n",
    "\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def custom_lossGCF1spec(y_actual,y_pred):\n",
    "    global tensorsw\n",
    "    tupl = np.shape(y_actual)\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    sw = tensorsw[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
    "\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "\n",
    "def custom_lossGCF1A(y_actual,y_pred):\n",
    "    print(np.shape(y_actual), np.shape(y_pred))\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.square(y_actual-y_pred)\n",
    "    return tf.math.reduce_mean(tensordiff)\n",
    "\n",
    "# Basic TF does NOT supply sample_weight\n",
    "def custom_lossGCF1B(y_actual,y_pred,sample_weight=None):\n",
    "    tupl = np.shape(y_actual)\n",
    "\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "    \n",
    "def custom_lossGCF4(y_actual,y_pred):\n",
    "    tensordiff = y_actual-y_pred\n",
    "    newtensordiff = tf.where(tf.math.is_nan(tensordiff), tf.zeros_like(tensordiff), tensordiff)\n",
    "    return tf.math.reduce_mean(tf.math.square(newtensordiff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIWDP9I8myNQ"
   },
   "source": [
    "### Utility: Shuffle, Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "OIB3yMlo7kFI",
    "outputId": "4bea2bfd-bc60-4fc2-bd74-4bbb2d4f0836"
   },
   "outputs": [],
   "source": [
    "def SetSpacetime(BasicTimes):\n",
    "  global GlobalTimeMask\n",
    "  Time = None\n",
    "  if (MaskingOption == 0) or (not GlobalSpacetime):\n",
    "    return Time\n",
    "  NumTOTAL = BasicTimes.shape[1]\n",
    "  BasicTimes = BasicTimes.astype(np.int16)\n",
    "  BasicTimes = np.reshape(BasicTimes,(BasicTimes.shape[0],NumTOTAL,1))\n",
    "  addons = np.arange(0,Tseq,dtype =np.int16)\n",
    "  addons = np.reshape(addons,(1,1,Tseq))\n",
    "  Time = BasicTimes+addons\n",
    "  Time = np.reshape(Time,(BasicTimes.shape[0], NumTOTAL*Tseq))\n",
    "  BasicPureTime = np.arange(0,Tseq,dtype =np.int16) \n",
    "  BasicPureTime = np.reshape(BasicPureTime,(Tseq,1))\n",
    "  GlobalTimeMask = tf.where( (BasicPureTime-np.transpose(BasicPureTime))>0, 0.0,1.0)\n",
    "  GlobalTimeMask = np.reshape(GlobalTimeMask,(1,1,1,Tseq,Tseq))\n",
    "  return Time\n",
    "\n",
    "def shuffleDLinput(Xin,yin,AuxiliaryArray=None, Spacetime=None):\n",
    " # Auxiliary array could be weight or location/time tracker\n",
    " # These are per batch so sorted axis is first\n",
    "  \n",
    "  np.random.seed(int.from_bytes(os.urandom(4), byteorder='little'))\n",
    "  trainingorder = list(range(0, len(Xin)))\n",
    "  random.shuffle(trainingorder)\n",
    "\n",
    "  Xinternal = list()\n",
    "  yinternal = list()\n",
    "  if AuxiliaryArray is not None:\n",
    "    AuxiliaryArrayinternal = list()\n",
    "  if Spacetime is not None:\n",
    "    Spacetimeinternal = list()\n",
    "  for i in trainingorder:\n",
    "    Xinternal.append(Xin[i])\n",
    "    yinternal.append(yin[i])\n",
    "    if AuxiliaryArray is not None:\n",
    "      AuxiliaryArrayinternal.append(AuxiliaryArray[i])\n",
    "    if Spacetime is not None:\n",
    "      Spacetimeinternal.append(Spacetime[i])\n",
    "  X = np.array(Xinternal)\n",
    "  y = np.array(yinternal)\n",
    "  if (AuxiliaryArray is None) and (Spacetime is None):\n",
    "    return X, y\n",
    "  if (AuxiliaryArray is not None) and (Spacetime is None):\n",
    "    AA = np.array(AuxiliaryArrayinternal)\n",
    "    return X,y,AA\n",
    "  if (AuxiliaryArray is None) and (Spacetime is not None):\n",
    "    St = np.array(Spacetimeinternal)\n",
    "    return X,y,St\n",
    "  AA = np.array(AuxiliaryArrayinternal)\n",
    "  St = np.array(Spacetimeinternal)\n",
    "  return X,y,AA,St\n",
    "\n",
    "# Simple Plot of Loss from history\n",
    "def finalizeDL(ActualModel, recordtrainloss, recordvalloss, validationfrac, X_in, y_in, modelflag, LabelFit =''):\n",
    "  \n",
    "  histlen = len(recordtrainloss)\n",
    "\n",
    "  trainloss = recordtrainloss[histlen-1]\n",
    "  plt.rcParams[\"figure.figsize\"] = [8,6]\n",
    "  plt.plot(recordtrainloss)\n",
    "  if (validationfrac > 0.001) and len(recordvalloss) > 0:\n",
    "    valloss = recordvalloss[histlen-1]\n",
    "    plt.plot(recordvalloss)\n",
    "  else:\n",
    "    valloss = 0.0\n",
    "  \n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + current_time + ' ' + RunName + ' finalizeDL ' + RunComment +resetfonts)\n",
    "  plt.title(LabelFit + ' ' + RunName+' model loss ' + str(round(trainloss,7)) + ' Val ' + str(round(valloss,7)))\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.yscale(\"log\")\n",
    "  plt.grid(True)\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  FitPredictions = DLprediction(X_in, y_in,ActualModel,modelflag, LabelFit = LabelFit)\n",
    "  for debugfips in ListofTestFIPS:\n",
    "    if debugfips != '': \n",
    "      debugfipsoutput(debugfips, FitPredictions, X_in, y_in)\n",
    "  return\n",
    "\n",
    "def debugfipsoutput(debugfips, FitPredictions, Xin, Observations):\n",
    "\n",
    "  print(startbold + startred + 'debugfipsoutput for ' + str(debugfips) + RunName + ' ' + RunComment +resetfonts)\n",
    "# Set Location Number in Arrays\n",
    "  LocationNumber = FIPSstringlookup[debugfips]\n",
    "\n",
    "  # Sequences to look at\n",
    "  Seqcount = 5\n",
    "  Seqnumber =  np.empty(Seqcount, dtype = np.int)\n",
    "  Seqnumber[0] = 0\n",
    "  Seqnumber[1] = int(Num_Seq/4)-1\n",
    "  Seqnumber[2] = int(Num_Seq/2)-1\n",
    "  Seqnumber[3] = int((3*Num_Seq)/4) -1\n",
    "  Seqnumber[4] = Num_Seq-1\n",
    "\n",
    "  # Window Positions to look at\n",
    "  Wincount = 5\n",
    "  Winnumber = np.empty(Wincount, dtype = np.int)\n",
    "  Winnumber[0] = 0\n",
    "  Winnumber[1] = int(Tseq/4)-1\n",
    "  Winnumber[2] = int(Tseq/2)-1\n",
    "  Winnumber[3] = int((3*Tseq)/4) -1\n",
    "  Winnumber[4] = Tseq-1\n",
    "\n",
    "  if SymbolicWindows:\n",
    "    InputSequences = np.empty([Seqcount,Wincount, NpropperseqTOT], dtype=np.float32)\n",
    "    for jseq in range(0,Seqcount):\n",
    "      iseq = Seqnumber[jseq]\n",
    "      for jwindow in range(0,Wincount):\n",
    "        window = Winnumber[jwindow]\n",
    "        InputSequences[jseq,jwindow] = Xin[LocationNumber,iseq+jseq]\n",
    "  else:\n",
    "    InputSequences = Xin \n",
    "\n",
    "  # Location Info\n",
    " \n",
    "  print('\\n' + startbold + startred + debugfips + ' # ' + str(LocationNumber) + ' ' +\n",
    "        Locationname[LocationNumber] + ' ' + Locationstate[LocationNumber] + ' Pop '\n",
    "        + str(Locationpopulation[LocationNumber]) + resetfonts)\n",
    "  plot_by_fips(int(debugfips), Observations, FitPredictions)\n",
    " \n",
    "  if PlotsOnlyinTestFIPS:\n",
    "    return\n",
    "    \n",
    "  # Print Input Data to Test\n",
    "  # Static Properties\n",
    "  print(startbold + startred + 'Static Properties ' + debugfips + ' ' +\n",
    "         Locationname[LocationNumber] + resetfonts)\n",
    "  line = ''\n",
    "  for iprop in range(0,NpropperTimeStatic):\n",
    "    if SymbolicWindows:\n",
    "      val = InputSequences[0,0,iprop]\n",
    "    else:\n",
    "      val = InputSequences[0,LocationNumber,0,iprop]\n",
    "    line += startbold + InputPropertyNames[PropertyNameIndex[iprop]] + resetfonts + ' ' + str(round(val,3)) + ' '\n",
    "  print('\\n'.join(wrap(line,200)))\n",
    "\n",
    " # Dynamic Properties\n",
    "  for iprop in range(NpropperTimeStatic, NpropperTime):\n",
    "    print('\\n')\n",
    "    for jwindow in range(0,Wincount):\n",
    "      window = Winnumber[jwindow]\n",
    "      line = startbold + InputPropertyNames[PropertyNameIndex[iprop]] + ' W= '+str(window) +resetfonts\n",
    "      for jseq in range(0,Seqcount):\n",
    "        iseq = Seqnumber[jseq]\n",
    "        line += startbold + startred + ' ' + str(iseq) + ')' +resetfonts\n",
    "        if SymbolicWindows:\n",
    "          val = InputSequences[jseq,jwindow,iprop]\n",
    "        else:\n",
    "          val = InputSequences[iseq,LocationNumber,window,iprop]\n",
    "        line +=   ' ' + str(round(val,3))\n",
    "      print('\\n'.join(wrap(line,200)))\n",
    "  \n",
    "\n",
    "  # Total Input\n",
    "  print('\\n')\n",
    "  line = startbold + 'Props: ' + resetfonts \n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    if iprop%5 == 0:\n",
    "      line += startbold + startred + ' ' + str(iprop) + ')' + resetfonts     \n",
    "    line += ' ' + InputPropertyNames[PropertyNameIndex[iprop]]\n",
    "  print('\\n'.join(wrap(line,200)))\n",
    "  for jseq in range(0,Seqcount):\n",
    "    iseq = Seqnumber[jseq]\n",
    "    for jwindow in range(0,Wincount):\n",
    "      window = Winnumber[jwindow]\n",
    "      line = startbold + 'Input: All in Seq ' + str(iseq) + ' W= ' + str(window) + resetfonts\n",
    "      for iprop in range(0,NpropperseqTOT):\n",
    "        if iprop%5 == 0:\n",
    "          line += startbold + startred + ' ' + str(iprop) + ')' +resetfonts\n",
    "        if SymbolicWindows:\n",
    "          val = InputSequences[jseq,jwindow,iprop]\n",
    "        else:\n",
    "          val = InputSequences[iseq,LocationNumber,window,iprop]\n",
    "        result = str(round(val,3))\n",
    "        line += ' ' + result\n",
    "      print('\\n'.join(wrap(line,200)))\n",
    "\n",
    "  # Total Prediction\n",
    "  print('\\n')\n",
    "  line = startbold + 'Preds: ' + resetfonts \n",
    "  for ipred in range(0,NpredperseqTOT):\n",
    "    if ipred%5 == 0:\n",
    "      line += startbold + startred + ' ' + str(ipred) + ')' + resetfonts     \n",
    "    line += ' ' + Predictionname[ipred]\n",
    "  for jseq in range(0,Seqcount):\n",
    "    iseq = Seqnumber[jseq]\n",
    "    line = startbold + 'Preds: All in Seq ' + str(iseq) + resetfonts\n",
    "    for ipred in range(0,NpredperseqTOT):\n",
    "      fred = Observations[iseq,LocationNumber,ipred]\n",
    "      if np.math.isnan(fred):\n",
    "        result = 'NaN'\n",
    "      else:\n",
    "        result = str(round(fred,3))\n",
    "      if ipred%5 == 0:\n",
    "          line += startbold + startred + ' ' + str(ipred) + ')' + resetfonts     \n",
    "      line += ' ' + result\n",
    "    print('\\n'.join(wrap(line,200)))   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkiL51xZ3XUr"
   },
   "source": [
    "##Set up TFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqBfMQNwQj1z"
   },
   "source": [
    "###Data and Input Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "-tUyF2ZhQ3-C",
    "outputId": "e2ac8d34-737f-4fc4-fef2-c905ed471cee"
   },
   "outputs": [],
   "source": [
    "# Type defintions\n",
    "import enum\n",
    "\n",
    "class DataTypes(enum.IntEnum):\n",
    "  \"\"\"Defines numerical types of each column.\"\"\"\n",
    "  REAL_VALUED = 0\n",
    "  CATEGORICAL = 1\n",
    "  DATE = 2\n",
    "  NULL = -1\n",
    "  STRING = 3\n",
    "\n",
    "class InputTypes(enum.IntEnum):\n",
    "  \"\"\"Defines input types of each column.\"\"\"\n",
    "  TARGET = 0\n",
    "  OBSERVED_INPUT = 1\n",
    "  KNOWN_INPUT = 2\n",
    "  STATIC_INPUT = 3\n",
    "  ID = 4  # Single column used as an entity identifier\n",
    "  TIME = 5  # Single column exclusively used as a time index\n",
    "  NULL = -1\n",
    "\n",
    "def checkdfNaN(label, AttributeSpec, y):\n",
    "  countNaN = 0\n",
    "  countnotNaN = 0\n",
    "  if y is None:\n",
    "    return\n",
    "  names = y.columns.tolist()\n",
    "  count = np.zeros(y.shape[1])\n",
    "  for j in range(0,y.shape[1]):\n",
    "    colname = names[j]\n",
    "    if AttributeSpec.loc[colname,'DataTypes'] != DataTypes.REAL_VALUED:\n",
    "      continue\n",
    "    for i in range(0,y.shape[0]):\n",
    "          if(np.math.isnan(y.iloc[i,j])):\n",
    "              countNaN += 1\n",
    "              count[j] += 1\n",
    "          else:\n",
    "              countnotNaN += 1\n",
    "\n",
    "\n",
    "  percent = (100.0*countNaN)/(countNaN + countnotNaN)\n",
    "  print(label + ' is NaN ',str(countNaN),' percent ',str(round(percent,2)),' not NaN ', str(countnotNaN))\n",
    "  for j in range(0,y.shape[1]):\n",
    "    if count[j] == 0:\n",
    "      continue\n",
    "    print(names[j] + ' has NaN ' + str(count[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOXCqWirQxZb"
   },
   "source": [
    "###Convert FFFFWNPF to TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1319
    },
    "id": "98Wz3J3y3b2z",
    "outputId": "df611794-482f-4ee1-8d0c-3af61ac07f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39737, 25)\n",
      "Index(['RawLabel', 'Location', 'Time from Start', '0 Cases Root 2',\n",
      "       '1 Deaths Root 2', '2 Age Distribution', '3 Air Pollution',\n",
      "       '4 Comorbidities', '5 Health Disparities', '6 Mobility',\n",
      "       '7 Residential Density', '8 Disease Spread', '9 Social Distancing',\n",
      "       '10 Testing', '11 Transmissible Cases', '12 VaccinationOneDose',\n",
      "       '13 Vaccination', '14 LinearSpace', '15 Constant', '16 LinearTime',\n",
      "       '17 P2-Time', '18 P3-Time', '19 P4-Time', '20 CosWeekly',\n",
      "       '21 SinWeekly'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AttributeName</th>\n",
       "      <th>DataTypes</th>\n",
       "      <th>InputTypes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AttributeName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RawLabel</th>\n",
       "      <td>RawLabel</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>Location</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time from Start</th>\n",
       "      <td>Time from Start</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 Cases Root 2</th>\n",
       "      <td>0 Cases Root 2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 Deaths Root 2</th>\n",
       "      <td>1 Deaths Root 2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 Age Distribution</th>\n",
       "      <td>2 Age Distribution</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 Air Pollution</th>\n",
       "      <td>3 Air Pollution</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 Comorbidities</th>\n",
       "      <td>4 Comorbidities</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 Health Disparities</th>\n",
       "      <td>5 Health Disparities</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Mobility</th>\n",
       "      <td>6 Mobility</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 Residential Density</th>\n",
       "      <td>7 Residential Density</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8 Disease Spread</th>\n",
       "      <td>8 Disease Spread</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9 Social Distancing</th>\n",
       "      <td>9 Social Distancing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 Testing</th>\n",
       "      <td>10 Testing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11 Transmissible Cases</th>\n",
       "      <td>11 Transmissible Cases</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12 VaccinationOneDose</th>\n",
       "      <td>12 VaccinationOneDose</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13 Vaccination</th>\n",
       "      <td>13 Vaccination</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14 LinearSpace</th>\n",
       "      <td>14 LinearSpace</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15 Constant</th>\n",
       "      <td>15 Constant</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16 LinearTime</th>\n",
       "      <td>16 LinearTime</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17 P2-Time</th>\n",
       "      <td>17 P2-Time</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18 P3-Time</th>\n",
       "      <td>18 P3-Time</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19 P4-Time</th>\n",
       "      <td>19 P4-Time</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20 CosWeekly</th>\n",
       "      <td>20 CosWeekly</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21 SinWeekly</th>\n",
       "      <td>21 SinWeekly</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 AttributeName  DataTypes  InputTypes\n",
       "AttributeName                                                        \n",
       "RawLabel                              RawLabel          0          -1\n",
       "Location                              Location          3           4\n",
       "Time from Start                Time from Start          0           5\n",
       "0 Cases Root 2                  0 Cases Root 2          0           0\n",
       "1 Deaths Root 2                1 Deaths Root 2          0           0\n",
       "2 Age Distribution          2 Age Distribution          0           3\n",
       "3 Air Pollution                3 Air Pollution          0           3\n",
       "4 Comorbidities                4 Comorbidities          0           3\n",
       "5 Health Disparities      5 Health Disparities          0           3\n",
       "6 Mobility                          6 Mobility          0           3\n",
       "7 Residential Density    7 Residential Density          0           3\n",
       "8 Disease Spread              8 Disease Spread          0           1\n",
       "9 Social Distancing        9 Social Distancing          0           1\n",
       "10 Testing                          10 Testing          0           1\n",
       "11 Transmissible Cases  11 Transmissible Cases          0           1\n",
       "12 VaccinationOneDose    12 VaccinationOneDose          0           1\n",
       "13 Vaccination                  13 Vaccination          0           1\n",
       "14 LinearSpace                  14 LinearSpace          0           2\n",
       "15 Constant                        15 Constant          0           2\n",
       "16 LinearTime                    16 LinearTime          0           2\n",
       "17 P2-Time                          17 P2-Time          0           2\n",
       "18 P3-Time                          18 P3-Time          0           2\n",
       "19 P4-Time                          19 P4-Time          0           2\n",
       "20 CosWeekly                      20 CosWeekly          0           2\n",
       "21 SinWeekly                      21 SinWeekly          0           2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction mappingt+0-Obs0 t+0-Obs1 t+1-Obs0 t+1-Obs1 t+2-Obs0 t+2-Obs1 t+3-Obs0 t+3-Obs1 t+4-Obs0 t+4-Obs1 t+5-Obs0\n",
      "t+5-Obs1 t+6-Obs0 t+6-Obs1 t+7-Obs0 t+7-Obs1 t+8-Obs0 t+8-Obs1 t+9-Obs0 t+9-Obs1 t+10-Obs0 t+10-Obs1 t+11-Obs0 t+11-Obs1\n",
      "t+12-Obs0 t+12-Obs1 t+13-Obs0 t+13-Obs1 t+14-Obs0 t+14-Obs1\n",
      "TFTNumberTargets 2 FFFFWNPFNumberTargets 2\n",
      "[('RawLabel', 0, -1), ('Location', 3, 4), ('Time from Start', 0, 5), ('0 Cases Root 2', 0, 0), ('1 Deaths Root 2', 0, 0), ('2 Age Distribution', 0, 3), ('3 Air Pollution', 0, 3), ('4 Comorbidities', 0, 3), ('5 Health Disparities', 0, 3), ('6 Mobility', 0, 3), ('7 Residential Density', 0, 3), ('8 Disease Spread', 0, 1), ('9 Social Distancing', 0, 1), ('10 Testing', 0, 1), ('11 Transmissible Cases', 0, 1), ('12 VaccinationOneDose', 0, 1), ('13 Vaccination', 0, 1), ('14 LinearSpace', 0, 2), ('15 Constant', 0, 2), ('16 LinearTime', 0, 2), ('17 P2-Time', 0, 2), ('18 P3-Time', 0, 2), ('19 P4-Time', 0, 2), ('20 CosWeekly', 0, 2), ('21 SinWeekly', 0, 2)]\n",
      "Index(['AttributeName', 'DataTypes', 'InputTypes'], dtype='object')\n",
      "Index(['RawLabel', 'Location', 'Time from Start', '0 Cases Root 2',\n",
      "       '1 Deaths Root 2', '2 Age Distribution', '3 Air Pollution',\n",
      "       '4 Comorbidities', '5 Health Disparities', '6 Mobility',\n",
      "       '7 Residential Density', '8 Disease Spread', '9 Social Distancing',\n",
      "       '10 Testing', '11 Transmissible Cases', '12 VaccinationOneDose',\n",
      "       '13 Vaccination', '14 LinearSpace', '15 Constant', '16 LinearTime',\n",
      "       '17 P2-Time', '18 P3-Time', '19 P4-Time', '20 CosWeekly',\n",
      "       '21 SinWeekly'],\n",
      "      dtype='object', name='AttributeName')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "if TFTConversion:\n",
    "# Pick Values setting InputType\n",
    "# Currently ONLY pick from properties BUT\n",
    "# If PropPick = 0 (target) then these should be selected as predictions in FFFFWNPF and futured of length LengthFutures\n",
    "\n",
    "  if ReadApril2021Covid:\n",
    "    PropPick = [0,0,3,3,3,3,1,3,3,1,3,1,1,1,3,2,2,2,2,2,2,2,2]\n",
    "    PropDataType = [0] * NpropperseqTOT\n",
    "  if ReadDecember2021:     #?\n",
    "    PropPick = [0,0,3,3,3,3,3,3,1,1,1,1,1,1,2,2,2,2,2,2,2,2]\n",
    "    PropDataType = [0] * NpropperseqTOT\n",
    "  if Earthquake:\n",
    "    PropPick = [3,3,3,3,0,1,1,1,1,1,1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2]\n",
    "    PropDataType = [0] * NpropperseqTOT\n",
    "\n",
    "# Dataframe is overall label (real starting at 0), Location Name, Time Input Properties, Predicted Properties Nloc times Num_Time values\n",
    "# Row major order in Location-Time Space\n",
    "  Totalsize = (Num_Time + TFTExtraTimes) * Nloc\n",
    "  RawLabel = np.arange(0, Totalsize, dtype =np.float32)\n",
    "  LocationLabel = []\n",
    "  FFFFWNPFUniqueLabel = []\n",
    "  RawTime = np.empty([Nloc,Num_Time + TFTExtraTimes], dtype = np.float32)\n",
    "#  print('Times ' + str(Num_Time) + ' ' + str(TFTExtraTimes))\n",
    "  ierror = 0\n",
    "  for ilocation in range(0,Nloc):\n",
    "#   locname = Locationstate[LookupLocations[ilocation]] + ' ' + Locationname[LookupLocations[ilocation]] \n",
    "    locname = Locationname[LookupLocations[ilocation]]  + ' ' + Locationstate[LookupLocations[ilocation]]\n",
    "    if locname == \"\":\n",
    "      printexit('Illegal null location name ' + str(ilocation))\n",
    "    for idupe in range(0,len(FFFFWNPFUniqueLabel)):\n",
    "      if locname == FFFFWNPFUniqueLabel[idupe]:\n",
    "        print(' Duplicate location name ' + str(ilocation) + ' ' + str(idupe) + ' ' + locname)\n",
    "        ierror += 1\n",
    "    FFFFWNPFUniqueLabel.append(locname)\n",
    "#    print(str(ilocation) + ' ' +locname)\n",
    "    for jtime in range(0,Num_Time + TFTExtraTimes):\n",
    "      RawTime[ilocation,jtime] = np.float32(jtime)\n",
    "      LocationLabel.append(locname)\n",
    "  if ierror > 0:\n",
    "    printexit(\" Duplicate Names \" + str(ierror))\n",
    "  RawTime = np.reshape(RawTime,-1)\n",
    "  TFTdf1 = pd.DataFrame(RawLabel, columns=['RawLabel'])\n",
    "  TFTdf2 = pd.DataFrame(LocationLabel, columns=['Location'])\n",
    "  TFTdf3 = pd.DataFrame(RawTime, columns=['Time from Start'])\n",
    "  TFTdfTotal = pd.concat([TFTdf1,TFTdf2,TFTdf3], axis=1)\n",
    "  TFTdfTotalSpec = pd.DataFrame([['RawLabel', DataTypes.REAL_VALUED, InputTypes.NULL]], columns=['AttributeName', 'DataTypes', 'InputTypes'])\n",
    "  TFTdfTotalSpec.loc[len(TFTdfTotalSpec.index)] = ['Location', DataTypes.STRING, InputTypes.ID]\n",
    "  TFTdfTotalSpec.loc[len(TFTdfTotalSpec.index)]  = ['Time from Start', DataTypes.REAL_VALUED, InputTypes.TIME] \n",
    "\n",
    "  ColumnsProp=[]\n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    line = str(iprop) + ' ' + InputPropertyNames[PropertyNameIndex[iprop]]  \n",
    "    jprop = PropertyAverageValuesPointer[iprop]\n",
    "    if QuantityTakeroot[jprop] > 1:\n",
    "      line += ' Root ' + str(QuantityTakeroot[jprop])\n",
    "    ColumnsProp.append(line)\n",
    "\n",
    "#  iloc = 292\n",
    "#  for jtime in range(390,424):\n",
    "#    print(str(jtime) + ' ' + str(ReshapedSequencesTOT[iloc,jtime,0]) + ' ' + str(ReshapedSequencesTOT[iloc,jtime,1])\n",
    "#     + ' ' + str(ReshapedSequencesTOT[iloc,jtime,2]) + ' ' + str(ReshapedSequencesTOT[iloc,jtime,3]))\n",
    "\n",
    "\n",
    "  TFTInputSequences = np.reshape(ReshapedSequencesTOT,(-1,NpropperseqTOT))\n",
    "  TFTNumberTargets = 0\n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    if PropPick[iprop] >= 0:\n",
    "      if PropPick[iprop] == 0:\n",
    "        TFTNumberTargets += 1\n",
    "      nextcol = TFTInputSequences[:,iprop]\n",
    "      dftemp = pd.DataFrame(nextcol, columns=[ColumnsProp[iprop]])\n",
    "      TFTdfTotal = pd.concat([TFTdfTotal,dftemp], axis=1)\n",
    "      TFTdfTotalSpec.loc[len(TFTdfTotalSpec.index)] = [ColumnsProp[iprop], PropDataType[iprop], PropPick[iprop]]\n",
    "  FFFFWNPFNumberTargets = TFTNumberTargets\n",
    "\n",
    "  ReshapedPredictionsTOT = np.transpose(RawInputPredictionsTOT,(1,0,2))\n",
    " \n",
    "  TFTdfTotalSpec = TFTdfTotalSpec.set_index('AttributeName', drop= False)\n",
    "  TFTdfTotalshape = TFTdfTotal.shape\n",
    "  TFTdfTotalcols = TFTdfTotal.columns\n",
    "  print(TFTdfTotalshape)\n",
    "  print(TFTdfTotalcols)\n",
    "  pd.set_option('display.max_rows', 100)\n",
    "  display(TFTdfTotalSpec)\n",
    "  \n",
    "  # Set Prediction mapping\n",
    "  PredictionTFTmapping =[]\n",
    "  CovidDeathonly = False\n",
    "  All = True\n",
    "  line ='Prediction mapping'\n",
    "  if CovidDeathonly:\n",
    "    FFFFWNPFNumberTargets = 2\n",
    "    odd = 0\n",
    "    countodd = 0\n",
    "    for ipred in range(0,NpredperseqTOT):\n",
    "      if odd == 0:\n",
    "        odd = 1\n",
    "        PredictionTFTmapping.append(' ')\n",
    "        text = 'skip'\n",
    "      else:\n",
    "        text = 't+{}-Death'.format(countodd)\n",
    "        PredictionTFTmapping.append(text)\n",
    "        countodd +=1\n",
    "        odd = 0\n",
    "      line += text + ' '\n",
    "  if All:\n",
    "    ifuture = 0\n",
    "    itarget = 0\n",
    "    for ipred in range(0,NpredperseqTOT):\n",
    "      text = 't+{}-Obs{}'.format(ifuture,itarget)\n",
    "      PredictionTFTmapping.append(text)\n",
    "      line += text + ' '\n",
    "      itarget += 1\n",
    "      if itarget >= TFTNumberTargets:\n",
    "        itarget = 0\n",
    "        ifuture += 1\n",
    "  print(wraptotext(line))\n",
    "  print('TFTNumberTargets ' + str(TFTNumberTargets) + ' FFFFWNPFNumberTargets ' + str(FFFFWNPFNumberTargets))\n",
    "\n",
    "\n",
    "  TFTdfTotalSpecshape = TFTdfTotalSpec.shape\n",
    "  TFTcolumn_definition = []\n",
    "  for i in range(0,TFTdfTotalSpecshape[0]):\n",
    "    TFTcolumn_definition.append((TFTdfTotalSpec.iloc[i,0],TFTdfTotalSpec.iloc[i,1],TFTdfTotalSpec.iloc[i,2]))\n",
    "  print(TFTcolumn_definition)\n",
    "  print(TFTdfTotalSpec.columns)\n",
    "  print(TFTdfTotalSpec.index)\n",
    "\n",
    "  # checkdfNaN('TFTdfTotal',TFTdfTotalSpec, TFTdfTotal)\n",
    "\n",
    "  TFTuseMSE = True\n",
    "\n",
    "  if TFTuseMSE:\n",
    "    TFTQuantiles =[1.0]\n",
    "    TFTQuantilenames = ['MSE']\n",
    "    TFTPrimaryQuantileIndex = 0\n",
    "  else:\n",
    "    TFTQuantiles = [0.1,0.5,0.9]\n",
    "    TFTQuantilenames = ['p10','p50','p90']\n",
    "    TFTPrimaryQuantileIndex = 1\n",
    "  TFTlenquantiles = len(TFTQuantiles)  \n",
    "\n",
    "  TFTMultivariate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbNT-soy5zkY"
   },
   "source": [
    "###TFT Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "qVkN0Mn9526c",
    "outputId": "4eca2639-df8e-46f7-fbef-be756925f4cb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TFTLSTMEncoderInitialMLP = 0\n",
    "TFTLSTMDecoderInitialMLP = 0\n",
    "TFTdropout_rate = 0.2\n",
    "TFThidden_layer_size = number_LSTMnodes\n",
    "TFTTransformerbatch_size = 64\n",
    "TFTTransformerepochs = 60\n",
    "\n",
    "\n",
    "TFTLSTMEncoderactivationvalue = LSTMactivationvalue\n",
    "TFTLSTMDecoderactivationvalue = LSTMactivationvalue\n",
    "TFTEncodernumber_LSTMnodes = number_LSTMnodes\n",
    "TFTDecodernumber_LSTMnodes = number_LSTMnodes\n",
    "TFTLSTMEncoderrecurrent_dropout1 = LSTMrecurrent_dropout1\n",
    "TFTLSTMDecoderrecurrent_dropout1 = LSTMrecurrent_dropout1\n",
    "TFTLSTMEncoderdropout1 = LSTMdropout1\n",
    "TFTLSTMDecoderdropout1 = LSTMdropout1\n",
    "TFTLSTMEncoderrecurrent_activation = LSTMrecurrent_activation\n",
    "TFTLSTMDecoderrecurrent_activation = LSTMrecurrent_activation\n",
    "TFTLSTMEncoderSecondLayer = True\n",
    "TFTLSTMDecoderSecondLayer = True\n",
    "TFTLSTMEncoderThirdLayer = False\n",
    "TFTLSTMDecoderThirdLayer = False\n",
    "TFTLSTMEncoderFinalMLP = 0\n",
    "TFTLSTMDecoderFinalMLP = 0\n",
    "TFTLSTMFinalMLP = 128\n",
    "TFTd_model = d_model\n",
    "TFTnum_heads = 4\n",
    "TFTnum_AttentionLayers = 1\n",
    "\n",
    "TFTFutures = 0\n",
    "if ReadApril2021Covid or ReadDecember2021:\n",
    "  TFTFutures = 1 + LengthFutures\n",
    "if TFTFutures == 0:\n",
    "  printexit('No TFT Futures defined')\n",
    "\n",
    "TFTfixed_params = {\n",
    "        'total_time_steps': Tseq + TFTFutures,\n",
    "        'num_encoder_steps': Tseq,\n",
    "        'num_epochs': TFTTransformerepochs,\n",
    "        'early_stopping_patience': 60,\n",
    "        'multiprocessing_workers': 5\n",
    "}\n",
    "\n",
    "TFTmodel_params = {\n",
    "        'dropout_rate': TFTdropout_rate,\n",
    "        'hidden_layer_size': TFTd_model,\n",
    "        'learning_rate': 0.001,\n",
    "        'minibatch_size': TFTTransformerbatch_size,\n",
    "        'max_gradient_norm': 0.01,\n",
    "        'num_heads': TFTnum_heads,\n",
    "        'stack_size': TFTnum_AttentionLayers\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBMkPML6MXY7"
   },
   "source": [
    "###Base Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "hWusLYsSMdX9",
    "outputId": "a89b4f03-de4a-4b84-ac65-7f65c27ac8bf"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class GenericDataFormatter(abc.ABC):\n",
    "  \"\"\"Abstract base class for all data formatters.\n",
    "\n",
    "  User can implement the abstract methods below to perform dataset-specific\n",
    "  manipulations.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def set_scalers(self, df):\n",
    "    \"\"\"Calibrates scalers using the data supplied.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def transform_inputs(self, df):\n",
    "    \"\"\"Performs feature transformation.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def format_predictions(self, df):\n",
    "    \"\"\"Reverts any normalisation to give predictions in original scale.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def split_data(self, df):\n",
    "    \"\"\"Performs the default train, validation and test splits.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @property\n",
    "  @abc.abstractmethod\n",
    "  def _column_definition(self):\n",
    "    \"\"\"Defines order, input type and data type of each column.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def get_fixed_params(self):\n",
    "    \"\"\"Defines the fixed parameters used by the model for training.\n",
    "\n",
    "    Requires the following keys:\n",
    "      'total_time_steps': Defines the total number of time steps used by TFT\n",
    "      'num_encoder_steps': Determines length of LSTM encoder (i.e. history)\n",
    "      'num_epochs': Maximum number of epochs for training\n",
    "      'early_stopping_patience': Early stopping param for keras\n",
    "      'multiprocessing_workers': # of cpus for data processing\n",
    "\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of fixed parameters, e.g.:\n",
    "\n",
    "      fixed_params = {\n",
    "          'total_time_steps': 252 + 5,\n",
    "          'num_encoder_steps': 252,\n",
    "          'num_epochs': 100,\n",
    "          'early_stopping_patience': 5,\n",
    "          'multiprocessing_workers': 5,\n",
    "      }\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "  # Shared functions across data-formatters\n",
    "  @property\n",
    "  def num_classes_per_cat_input(self):\n",
    "    \"\"\"Returns number of categories per relevant input.\n",
    "\n",
    "    This is seqeuently required for keras embedding layers.\n",
    "    \"\"\"\n",
    "    return self._num_classes_per_cat_input\n",
    "\n",
    "  def get_num_samples_for_calibration(self):\n",
    "    \"\"\"Gets the default number of training and validation samples.\n",
    "\n",
    "    Use to sub-sample the data for network calibration and a value of -1 uses\n",
    "    all available samples.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (training samples, validation samples)\n",
    "    \"\"\"\n",
    "    return -1, -1\n",
    "\n",
    "  def get_column_definition(self):\n",
    "    \"\"\"\"Returns formatted column definition in order expected by the TFT.\"\"\"\n",
    "\n",
    "    column_definition = self._column_definition\n",
    "\n",
    "    # Sanity checks first.\n",
    "    # Ensure only one ID and time column exist\n",
    "    def _check_single_column(input_type):\n",
    "\n",
    "      length = len([tup for tup in column_definition if tup[2] == input_type])\n",
    "\n",
    "      if length != 1:\n",
    "        raise ValueError('Illegal number of inputs ({}) of type {}'.format(\n",
    "            length, input_type))\n",
    "\n",
    "    _check_single_column(InputTypes.ID)\n",
    "    _check_single_column(InputTypes.TIME)\n",
    "\n",
    "    identifier = [tup for tup in column_definition if tup[2] == InputTypes.ID]\n",
    "    time = [tup for tup in column_definition if tup[2] == InputTypes.TIME]\n",
    "    real_inputs = [\n",
    "        tup for tup in column_definition if tup[1] == DataTypes.REAL_VALUED and\n",
    "        tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]\n",
    "    categorical_inputs = [\n",
    "        tup for tup in column_definition if tup[1] == DataTypes.CATEGORICAL and\n",
    "        tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]\n",
    "\n",
    "    return identifier + time + real_inputs + categorical_inputs\n",
    "\n",
    "  def _get_input_columns(self):\n",
    "    \"\"\"Returns names of all input columns.\"\"\"\n",
    "    return [\n",
    "        tup[0]\n",
    "        for tup in self.get_column_definition()\n",
    "        if tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]\n",
    "\n",
    "  def _get_tft_input_indices(self):\n",
    "    \"\"\"Returns the relevant indexes and input sizes required by TFT.\"\"\"\n",
    "\n",
    "    # Functions\n",
    "    def _extract_tuples_from_data_type(data_type, defn):\n",
    "      return [\n",
    "          tup for tup in defn if tup[1] == data_type and\n",
    "          tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "      ]\n",
    "\n",
    "    def _get_locations(input_types, defn):\n",
    "      return [i for i, tup in enumerate(defn) if tup[2] in input_types]\n",
    "\n",
    "    # Start extraction\n",
    "    column_definition = [\n",
    "        tup for tup in self.get_column_definition()\n",
    "        if tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]\n",
    "\n",
    "    categorical_inputs = _extract_tuples_from_data_type(DataTypes.CATEGORICAL,\n",
    "                                                        column_definition)\n",
    "    real_inputs = _extract_tuples_from_data_type(DataTypes.REAL_VALUED,\n",
    "                                                 column_definition)\n",
    "\n",
    "    locations = {\n",
    "        'input_size':\n",
    "            len(self._get_input_columns()),\n",
    "        'output_size':\n",
    "            len(_get_locations({InputTypes.TARGET}, column_definition)),\n",
    "        'category_counts':\n",
    "            self.num_classes_per_cat_input,\n",
    "        'input_obs_loc':\n",
    "            _get_locations({InputTypes.TARGET}, column_definition),\n",
    "        'static_input_loc':\n",
    "            _get_locations({InputTypes.STATIC_INPUT}, column_definition),\n",
    "        'known_regular_inputs':\n",
    "            _get_locations({InputTypes.STATIC_INPUT, InputTypes.KNOWN_INPUT},\n",
    "                           real_inputs),\n",
    "        'known_categorical_inputs':\n",
    "            _get_locations({InputTypes.STATIC_INPUT, InputTypes.KNOWN_INPUT},\n",
    "                           categorical_inputs),\n",
    "    }\n",
    "\n",
    "    return locations\n",
    "\n",
    "  def get_experiment_params(self):\n",
    "    \"\"\"Returns fixed model parameters for experiments.\"\"\"\n",
    "\n",
    "    required_keys = [\n",
    "        'total_time_steps', 'num_encoder_steps', 'num_epochs',\n",
    "        'early_stopping_patience', 'multiprocessing_workers'\n",
    "    ]\n",
    "\n",
    "    fixed_params = self.get_fixed_params()\n",
    "\n",
    "    for k in required_keys:\n",
    "      if k not in fixed_params:\n",
    "        raise ValueError('Field {}'.format(k) +\n",
    "                         ' missing from fixed parameter definitions!')\n",
    "\n",
    "    fixed_params['column_definition'] = self.get_column_definition()\n",
    "\n",
    "    fixed_params.update(self._get_tft_input_indices())\n",
    "\n",
    "    return fixed_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-k-se9TA9M2"
   },
   "source": [
    "###TFT FFFFWNPF Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "rRZ5qaEdBKzm",
    "outputId": "d0fcd3d7-6aee-4fbc-cc0a-9b5111b20187"
   },
   "outputs": [],
   "source": [
    "# Custom formatting functions for FFFFWNPF datasets.\n",
    "\n",
    "#GenericDataFormatter = data_formatters.base.GenericDataFormatter\n",
    "#DataTypes = data_formatters.base.DataTypes\n",
    "#InputTypes = data_formatters.base.InputTypes\n",
    "\n",
    "\n",
    "class FFFFWNPFFormatter(GenericDataFormatter):\n",
    "  \"\"\"\n",
    "  Defines and formats data for the Covid April 21 dataset.\n",
    "  Attributes:\n",
    "    column_definition: Defines input and data type of column used in the\n",
    "      experiment.\n",
    "    identifiers: Entity identifiers used in experiments.\n",
    "  \"\"\"\n",
    "\n",
    "  _column_definition = TFTcolumn_definition\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"Initialises formatter.\"\"\"\n",
    "\n",
    "    self.identifiers = None\n",
    "    self._real_scalers = None\n",
    "    self._cat_scalers = None\n",
    "    self._target_scaler = None\n",
    "    self._num_classes_per_cat_input = None\n",
    "    self._time_steps = self.get_fixed_params()['total_time_steps']\n",
    "\n",
    "  def split_data(self, df, valid_boundary=-1, test_boundary=-1):\n",
    "    \"\"\"Splits data frame into training-validation-test data frames.\n",
    "\n",
    "    This also calibrates scaling object, and transforms data for each split.\n",
    "\n",
    "    Args:\n",
    "      df: Source data frame to split.\n",
    "      valid_boundary: Starting time for validation data\n",
    "      test_boundary: Starting time for test data\n",
    "\n",
    "    Returns:\n",
    "      Tuple of transformed (train, valid, test) data.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Formatting train-valid-test splits.')\n",
    "\n",
    "    index = df['Time from Start']\n",
    "    train = df[index<(Num_Time-0.5)]\n",
    "    valid = df[index<(Num_Time-0.5)]\n",
    "    test = df\n",
    "    if valid_boundary > 0:\n",
    "      train = df.loc[index < valid_boundary]\n",
    "      if test_boundary > 0:\n",
    "        valid = df.loc[(index >= valid_boundary - 7) & (index < test_boundary)]\n",
    "      else:\n",
    "        valid = df.loc[(index >= valid_boundary - 7)]\n",
    "    if test_boundary > 0:\n",
    "      test = df.loc[index >= test_boundary - 7]\n",
    "\n",
    "    self.set_scalers(train)\n",
    "\n",
    "    return (self.transform_inputs(data) for data in [train, valid, test])\n",
    "\n",
    "  def set_scalers(self, df):\n",
    "    \"\"\"Calibrates scalers using the data supplied.\n",
    "\n",
    "    Args:\n",
    "      df: Data to use to calibrate scalers.\n",
    "    \"\"\"\n",
    "    print('Setting scalers with training data...')\n",
    "\n",
    "    column_definitions = self.get_column_definition()\n",
    "    print(column_definitions)\n",
    "    print(InputTypes.TARGET)\n",
    "    id_column = utils.get_single_col_by_input_type(InputTypes.ID,\n",
    "                                                   column_definitions, TFTMultivariate)\n",
    "    target_column = utils.get_single_col_by_input_type(InputTypes.TARGET,\n",
    "                                                       column_definitions, TFTMultivariate)\n",
    "\n",
    "    # Format real scalers\n",
    "    real_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.REAL_VALUED, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "    # Initialise scaler caches\n",
    "    self._real_scalers = {}\n",
    "    self._target_scaler = {}\n",
    "    identifiers = []\n",
    "    for identifier, sliced in df.groupby(id_column):\n",
    "\n",
    "      data = sliced[real_inputs].values\n",
    "      if TFTMultivariate == True:\n",
    "        targets = sliced[target_column].values\n",
    "      else:\n",
    "        targets = sliced[target_column].values\n",
    "#      self._real_scalers[identifier] = sklearn.preprocessing.StandardScaler().fit(data)\n",
    "\n",
    "#      self._target_scaler[identifier] = sklearn.preprocessing.StandardScaler().fit(targets)\n",
    "      identifiers.append(identifier)\n",
    "\n",
    "    # Format categorical scalers\n",
    "    categorical_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.CATEGORICAL, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "    categorical_scalers = {}\n",
    "    num_classes = []\n",
    "\n",
    "    # Set categorical scaler outputs\n",
    "    self._cat_scalers = categorical_scalers\n",
    "    self._num_classes_per_cat_input = num_classes\n",
    "\n",
    "    # Extract identifiers in case required\n",
    "    self.identifiers = identifiers\n",
    "\n",
    "  def transform_inputs(self, df):\n",
    "    \"\"\"Performs feature transformations.\n",
    "\n",
    "    This includes both feature engineering, preprocessing and normalisation.\n",
    "\n",
    "    Args:\n",
    "      df: Data frame to transform.\n",
    "\n",
    "    Returns:\n",
    "      Transformed data frame.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return df\n",
    "\n",
    "  def format_predictions(self, predictions):\n",
    "    \"\"\"Reverts any normalisation to give predictions in original scale.\n",
    "\n",
    "    Args:\n",
    "      predictions: Dataframe of model predictions.\n",
    "\n",
    "    Returns:\n",
    "      Data frame of unnormalised predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    return predictions\n",
    "\n",
    "  # Default params\n",
    "  def get_fixed_params(self):\n",
    "    \"\"\"Returns fixed model parameters for experiments.\"\"\"\n",
    "\n",
    "    fixed_params = TFTfixed_params\n",
    "\n",
    "    return fixed_params\n",
    "\n",
    "  def get_default_model_params(self):\n",
    "    \"\"\"Returns default optimised model parameters.\"\"\"\n",
    "\n",
    "    model_params = TFTmodel_params\n",
    "\n",
    "    return model_params\n",
    "\n",
    "  def get_num_samples_for_calibration(self):\n",
    "    \"\"\"Gets the default number of training and validation samples.\n",
    "\n",
    "    Use to sub-sample the data for network calibration and a value of -1 uses\n",
    "    all available samples.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (training samples, validation samples)\n",
    "    \"\"\"\n",
    "    numtrain = TFTdfTotalshape[0]\n",
    "    numvalid = TFTdfTotalshape[0]\n",
    "    return numtrain, numvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35crbOGqX_k-"
   },
   "source": [
    "###Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "63E8isAwYHWt",
    "outputId": "83b8ad8b-72e0-4179-8a42-027c7c6816ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal\n",
      "\u001b[1m\u001b[36mTFTCode\u001b[m\u001b[m/      \u001b[1m\u001b[36mTFTCode_arch\u001b[m\u001b[m/ \u001b[1m\u001b[36mTFTData\u001b[m\u001b[m/\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode\n",
      "Copy of script_train_fixed_params.py \u001b[1m\u001b[36mexpt_settings\u001b[m\u001b[m\n",
      "README.md                            future_weights.npy\n",
      "SCITFT-tf-gpu.yml                    \u001b[1m\u001b[36mlibs\u001b[m\u001b[m\n",
      "SCITFT_datasets.yml                  \u001b[1m\u001b[36mnumpy-arrays\u001b[m\u001b[m\n",
      "TFTdfTotal(1).csv                    requirements.txt\n",
      "TFTdfTotal.csv                       run.sh\n",
      "TFTdfTotalSpec.csv                   script_download_data.py\n",
      "Train-fixed-TFT.ipynb                script_hyperparam_opt.py\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m                          script_train_fixed_params.py\n",
      "\u001b[1m\u001b[36mdata_formatters\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "%cd \"./independent_study2/GPCE/TFToriginal/\"\n",
    "%ls\n",
    "%cd TFTCode/\n",
    "\n",
    "name = \"FFFFWNPF\"\n",
    "output_folder = \"../TFTData\" # Please don't change this path\n",
    "use_tensorflow_with_gpu = True\n",
    "\n",
    "import expt_settings.configs\n",
    "ExperimentConfig = expt_settings.configs.ExperimentConfig\n",
    "config = ExperimentConfig(name, output_folder)\n",
    "if name == 'FFFFWNPF':\n",
    "  formatter = FFFFWNPFFormatter()\n",
    "else:\n",
    "  formatter = config.make_data_formatter()\n",
    "\n",
    "import os\n",
    "!pwd\n",
    "!ls\n",
    "TFTdfTotalSpec.to_csv('TFTdfTotalSpec.csv')\n",
    "TFTdfTotal.to_csv('TFTdfTotal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBDBY1tnTH9c"
   },
   "source": [
    "###Train TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "mfOLBPu0TLoc",
    "outputId": "93808105-d401-4d8d-a368-9bac487dd7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy of script_train_fixed_params.py  \u001b[1m\u001b[36mexpt_settings\u001b[m\u001b[m/\r\n",
      "README.md                             future_weights.npy\r\n",
      "SCITFT-tf-gpu.yml                     \u001b[1m\u001b[36mlibs\u001b[m\u001b[m/\r\n",
      "SCITFT_datasets.yml                   \u001b[1m\u001b[36mnumpy-arrays\u001b[m\u001b[m/\r\n",
      "TFTdfTotal(1).csv                     requirements.txt\r\n",
      "TFTdfTotal.csv                        run.sh\r\n",
      "TFTdfTotalSpec.csv                    script_download_data.py\r\n",
      "Train-fixed-TFT.ipynb                 script_hyperparam_opt.py\r\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m/                          script_train_fixed_params.py\r\n",
      "\u001b[1m\u001b[36mdata_formatters\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime as dte\n",
    "import os\n",
    "import expt_settings.configs as TFTconfigs\n",
    "import libs.hyperparam_opt as TFTHypercode\n",
    "import libs.tft_modelDev as TFTmodelcode\n",
    "import libs.utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "ExperimentConfig = TFTconfigs.ExperimentConfig\n",
    "HyperparamOptManager = TFTHypercode.HyperparamOptManager\n",
    "ModelClass = TFTmodelcode.TemporalFusionTransformer\n",
    "%ls\n",
    "\n",
    "def train(expt_name,\n",
    "         use_gpu,\n",
    "         model_folder,\n",
    "         data_csv_path,\n",
    "         data_formatter,\n",
    "         restore=False,\n",
    "         use_testing_mode=False,\n",
    "         num_reps=TFTMode):\n",
    "  \"\"\"Trains tft based on defined model params.\n",
    "\n",
    "  Args:\n",
    "    expt_name: Name of experiment\n",
    "    use_gpu: Whether to run tensorflow with GPU operations\n",
    "    model_folder: Folder path where models are serialized\n",
    "    data_csv_path: Path to csv file containing data\n",
    "    data_formatter: Dataset-specific data fromatter (see\n",
    "      expt_settings.dataformatter.GenericDataFormatter)\n",
    "    use_testing_mode: Uses a smaller models and data sizes for testing purposes\n",
    "      only -- switch to False to use original default settings\n",
    "  \"\"\"\n",
    "\n",
    "  current_time = timenow()\n",
    "  runtype = ''\n",
    "  reloadresults = False\n",
    "  num_repeats = num_reps\n",
    "  if num_repeats == 0:\n",
    "    reloadresults = True\n",
    "    runtype = 'Restarted'\n",
    "  print(startbold + current_time + ' ' + startpurple +  'Start TFT ' + runtype + ' ' + RunName + ' ' + RunComment + resetfonts)\n",
    "\n",
    "  if not isinstance(data_formatter, GenericDataFormatter):\n",
    "    raise ValueError(\n",
    "        \"Data formatters should inherit from\" +\n",
    "        \"AbstractDataFormatter! Type={}\".format(type(data_formatter)))\n",
    "\n",
    "  # Tensorflow setup\n",
    "  default_keras_session = tf.keras.backend.get_session()\n",
    "\n",
    "  if use_gpu:\n",
    "    tf_config = utils.get_default_tensorflow_config(tf_device=\"gpu\", gpu_id=0)\n",
    "\n",
    "  else:\n",
    "    tf_config = utils.get_default_tensorflow_config(tf_device=\"cpu\")\n",
    "\n",
    "  print(\"*** Training from defined parameters for {} ***\".format(expt_name))\n",
    "\n",
    "  print(\"Loading & splitting data...\")\n",
    "  if expt_name == 'FFFFWNPF':\n",
    "    raw_data = data_csv_path\n",
    "  else :  \n",
    "    raw_data = pd.read_csv(data_csv_path, index_col=0)\n",
    "  \n",
    "  train, valid, test = data_formatter.split_data(raw_data)\n",
    "  train_samples, valid_samples = data_formatter.get_num_samples_for_calibration(\n",
    "  )\n",
    "  #print(str(train.shape),str(valid.shape),str(test.shape))\n",
    "  # checkdfNaN('Train',TFTdfTotalSpec,train)\n",
    "  # checkdfNaN('Valid',TFTdfTotalSpec,valid)\n",
    "  # checkdfNaN('Test',TFTdfTotalSpec,test)\n",
    "\n",
    "  # Sets up default params\n",
    "  fixed_params = data_formatter.get_experiment_params()\n",
    "  params = data_formatter.get_default_model_params()\n",
    "  params[\"model_folder\"] = model_folder\n",
    "  fixed_params[\"quantiles\"] = TFTQuantiles\n",
    "  fixed_params[\"quantilenames\"] = TFTQuantilenames\n",
    "  fixed_params[\"quantileindex\"] = TFTPrimaryQuantileIndex\n",
    "  fixed_params[\"TFTLSTMFinalMLP\"] = TFTLSTMFinalMLP\n",
    "  fixed_params[\"TFTOption1\"] = 1\n",
    "  fixed_params['TFTMultivariate'] = TFTMultivariate\n",
    "  TFTFinalGatingOption = 1\n",
    "  fixed_params['TFTFinalGatingOption'] = TFTFinalGatingOption\n",
    "  TFTSymbolicWindows = False\n",
    "  fixed_params['TFTSymbolicWindows'] = TFTSymbolicWindows\n",
    "\n",
    "  # for k in fixed_params:\n",
    "  #  print(\"{}: {}\".format(k, fixed_params[k]))\n",
    "\n",
    "  # Parameter overrides for testing only! Small sizes used to speed up script.\n",
    "  if use_testing_mode:\n",
    "    fixed_params[\"num_epochs\"] = 1\n",
    "    params[\"hidden_layer_size\"] = 5\n",
    "    train_samples, valid_samples = 100, 10\n",
    "\n",
    "  # Sets up hyperparam manager\n",
    "  print(\"*** Loading hyperparm manager ***\")\n",
    "  opt_manager = HyperparamOptManager({k: [params[k]] for k in params},\n",
    "                                     fixed_params, model_folder)\n",
    "  if reloadresults:\n",
    "    success = opt_manager.load_results()\n",
    "    print(' Load Previous Results ' + str(success))\n",
    "    num_repeats = 0\n",
    "\n",
    "  # Training -- one iteration only\n",
    "  print(\"*** Running calibration ***\")\n",
    "  print(\"Params Selected:\")\n",
    "  #for k in params:\n",
    "  #  print(\"{}: {}\".format(k, params[k]))\n",
    "\n",
    "  best_loss = np.Inf\n",
    "  for _ in range(num_repeats):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\n",
    "\n",
    "      tf.keras.backend.set_session(sess)\n",
    "\n",
    "    #  params = opt_manager.get_next_parameters()\n",
    "      for k in fixed_params:\n",
    "        params[k] = fixed_params[k]\n",
    "      model = ModelClass(params, use_cudnn=use_gpu)\n",
    "      print('Train samples ' +str(train_samples) + ' Valid samples ' +str(valid_samples))\n",
    "\n",
    "     # ram_gb = virtual_memory().total / 1e9\n",
    "     # print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "      if not model.training_data_cached():\n",
    "        model.cache_batched_data(train, \"train\", num_samples=train_samples)\n",
    "       # ram_gb = virtual_memory().total / 1e9\n",
    "      #  print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "        model.cache_batched_data(valid, \"valid\", num_samples=valid_samples)\n",
    "       # ram_gb = virtual_memory().total / 1e9\n",
    "       # print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "      model.fit()\n",
    "\n",
    "      val_loss = model.evaluate()\n",
    "\n",
    "      if val_loss < best_loss:\n",
    "        opt_manager.update_score(params, val_loss, model)\n",
    "        best_loss = val_loss\n",
    "\n",
    "      tf.keras.backend.set_session(default_keras_session)\n",
    "\n",
    "  print(\"*** Running tests ***\")\n",
    "  tf.reset_default_graph()\n",
    "  with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\n",
    "    tf.keras.backend.set_session(sess)\n",
    "    best_params = opt_manager.get_best_params()\n",
    "    model = ModelClass(best_params, use_cudnn=use_gpu)\n",
    "#    model.printsummary()\n",
    "\n",
    "    model.load(opt_manager.hyperparam_folder)\n",
    "\n",
    "    print(\"Computing best validation loss\")\n",
    "    val_loss = model.evaluate(valid)\n",
    "\n",
    "    print(\"Computing test loss\")\n",
    "    if test is None:\n",
    "      output_map = model.predict(train, return_targets=True)\n",
    "    else:\n",
    "      output_map = model.predict(test, return_targets=True)\n",
    "    print(output_map.keys)\n",
    "    print(output_map[TFTQuantilenames[TFTPrimaryQuantileIndex]].columns)\n",
    "    targets = data_formatter.format_predictions(output_map[\"targets\"])\n",
    "\n",
    "    pXX_forecast = [None]*TFTlenquantiles\n",
    "    for k,quantilename in enumerate(TFTQuantilenames):\n",
    "      pXX_forecast[k] = data_formatter.format_predictions(output_map[quantilename])\n",
    "\n",
    "    def extract_numerical_data(data):\n",
    "      \"\"\"Strips out forecast time and identifier columns.\"\"\"\n",
    "      return data[[\n",
    "          col for col in data.columns\n",
    "          if col not in {\"forecast_time\", \"identifier\"}\n",
    "      ]]\n",
    "    if not TFTuseMSE:\n",
    "      pXX_loss =  [None]*TFTlenquantiles\n",
    "      for k in range(0,TFTlenquantiles):\n",
    "        pXX_loss[k] = utils.numpy_normalised_quantile_loss(\n",
    "            extract_numerical_data(targets), extract_numerical_data(pXX_forecast[k]), TFTQuantiles[k])\n",
    "        print(\"Normalized Quantile Loss for Test Data: \" + TFTQuantilenames[k] + ' ' + str(pXX_loss[k]))\n",
    "\n",
    "    tf.keras.backend.set_session(default_keras_session)\n",
    "\n",
    "  print(\"Training completed @ {}\".format(dte.datetime.now()))\n",
    "  print(\"Best validation loss = {}\".format(val_loss))\n",
    "  print(\"Params:\")\n",
    "\n",
    "  for k in best_params:\n",
    "    print(k, \" = \", best_params[k])\n",
    "  ram_gb = virtual_memory().total / 1e9\n",
    "  print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "  return output_map\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "sFp7X2FF_0eg",
    "outputId": "0fdae098-c17e-4cd9-b683-c864b657d761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy of script_train_fixed_params.py  \u001b[1m\u001b[36mexpt_settings\u001b[m\u001b[m/\r\n",
      "README.md                             future_weights.npy\r\n",
      "SCITFT-tf-gpu.yml                     \u001b[1m\u001b[36mlibs\u001b[m\u001b[m/\r\n",
      "SCITFT_datasets.yml                   \u001b[1m\u001b[36mnumpy-arrays\u001b[m\u001b[m/\r\n",
      "TFTdfTotal(1).csv                     requirements.txt\r\n",
      "TFTdfTotal.csv                        run.sh\r\n",
      "TFTdfTotalSpec.csv                    script_download_data.py\r\n",
      "Train-fixed-TFT.ipynb                 script_hyperparam_opt.py\r\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m/                          script_train_fixed_params.py\r\n",
      "\u001b[1m\u001b[36mdata_formatters\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ExperimentConfig = TFTconfigs.ExperimentConfig\n",
    "HyperparamOptManager = TFTHypercode.HyperparamOptManager\n",
    "ModelClass = TFTmodelcode.TemporalFusionTransformer\n",
    "%ls\n",
    "\n",
    "def loadT(expt_name,\n",
    "         use_gpu,\n",
    "         model_folder,\n",
    "         data_csv_path,\n",
    "         data_formatter,\n",
    "         restore=False,\n",
    "         use_testing_mode=False,\n",
    "         num_reps=TFTMode):\n",
    "  \"\"\"Trains tft based on defined model params.\n",
    "\n",
    "  Args:\n",
    "    expt_name: Name of experiment\n",
    "    use_gpu: Whether to run tensorflow with GPU operations\n",
    "    model_folder: Folder path where models are serialized\n",
    "    data_csv_path: Path to csv file containing data\n",
    "    data_formatter: Dataset-specific data fromatter (see\n",
    "      expt_settings.dataformatter.GenericDataFormatter)\n",
    "    use_testing_mode: Uses a smaller models and data sizes for testing purposes\n",
    "      only -- switch to False to use original default settings\n",
    "  \"\"\"\n",
    "\n",
    "  counts = 0\n",
    "  interpretability_weights = {k: None for k in ['decoder_self_attn',\n",
    "                                              'static_flags', 'historical_flags', 'future_flags']}\n",
    "\n",
    "\n",
    "  current_time = timenow()\n",
    "  runtype = ''\n",
    "  reloadresults = False\n",
    "  num_repeats = num_reps\n",
    "  if num_repeats == 0:\n",
    "    reloadresults = True\n",
    "    runtype = 'Restarted'\n",
    "  print(startbold + current_time + ' ' + startpurple +  'Start TFT ' + runtype + ' ' + RunName + ' ' + RunComment + resetfonts)\n",
    "\n",
    "  if not isinstance(data_formatter, GenericDataFormatter):\n",
    "    raise ValueError(\n",
    "        \"Data formatters should inherit from\" +\n",
    "        \"AbstractDataFormatter! Type={}\".format(type(data_formatter)))\n",
    "\n",
    "  # Tensorflow setup\n",
    "  default_keras_session = tf.keras.backend.get_session()\n",
    "\n",
    "  if use_gpu:\n",
    "    tf_config = utils.get_default_tensorflow_config(tf_device=\"gpu\", gpu_id=0)\n",
    "\n",
    "  else:\n",
    "    tf_config = utils.get_default_tensorflow_config(tf_device=\"cpu\")\n",
    "\n",
    "  print(\"*** Training from defined parameters for {} ***\".format(expt_name))\n",
    "\n",
    "  print(\"Loading & splitting data...\")\n",
    "  if expt_name == 'FFFFWNPF':\n",
    "    raw_data = data_csv_path\n",
    "  else :  \n",
    "    raw_data = pd.read_csv(data_csv_path, index_col=0)\n",
    "  \n",
    "  train, valid, test = data_formatter.split_data(raw_data)\n",
    "  train_samples, valid_samples = data_formatter.get_num_samples_for_calibration(\n",
    "  )\n",
    "  #print(str(train.shape),str(valid.shape),str(test.shape))\n",
    "  # checkdfNaN('Train',TFTdfTotalSpec,train)\n",
    "  # checkdfNaN('Valid',TFTdfTotalSpec,valid)\n",
    "  # checkdfNaN('Test',TFTdfTotalSpec,test)\n",
    "\n",
    "  # Sets up default params\n",
    "  fixed_params = data_formatter.get_experiment_params()\n",
    "  params = data_formatter.get_default_model_params()\n",
    "  params[\"model_folder\"] = model_folder\n",
    "  fixed_params[\"quantiles\"] = TFTQuantiles\n",
    "  fixed_params[\"quantilenames\"] = TFTQuantilenames\n",
    "  fixed_params[\"quantileindex\"] = TFTPrimaryQuantileIndex\n",
    "  fixed_params[\"TFTLSTMFinalMLP\"] = TFTLSTMFinalMLP\n",
    "  fixed_params[\"TFTOption1\"] = 1\n",
    "  fixed_params['TFTMultivariate'] = TFTMultivariate\n",
    "  TFTFinalGatingOption = 1\n",
    "  fixed_params['TFTFinalGatingOption'] = TFTFinalGatingOption\n",
    "  TFTSymbolicWindows = False\n",
    "  fixed_params['TFTSymbolicWindows'] = TFTSymbolicWindows\n",
    "\n",
    "  # for k in fixed_params:\n",
    "  #  print(\"{}: {}\".format(k, fixed_params[k]))\n",
    "\n",
    "  # Parameter overrides for testing only! Small sizes used to speed up script.\n",
    "  if use_testing_mode:\n",
    "    fixed_params[\"num_epochs\"] = 1\n",
    "    params[\"hidden_layer_size\"] = 5\n",
    "    train_samples, valid_samples = 100, 10\n",
    "\n",
    "  # Sets up hyperparam manager\n",
    "  print(\"*** Loading hyperparm manager ***\")\n",
    "  opt_manager = HyperparamOptManager({k: [params[k]] for k in params},\n",
    "                                     fixed_params, model_folder)\n",
    "  if reloadresults:\n",
    "    success = opt_manager.load_results()\n",
    "    print(' Load Previous Results ' + str(success))\n",
    "    num_repeats = 0\n",
    "\n",
    "  print(\"*** Running calibration ***\")\n",
    "  print(\"Params Selected:\")\n",
    "  best_loss = np.Inf\n",
    "  \n",
    "\n",
    "  print(\"*** Running tests ***\")\n",
    "  tf.reset_default_graph()\n",
    "  with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\n",
    "    tf.keras.backend.set_session(sess)\n",
    "    best_params = opt_manager.get_best_params()\n",
    "    model = ModelClass(best_params, use_cudnn=use_gpu)\n",
    "#    model.printsummary()\n",
    "\n",
    "    model.load(opt_manager.hyperparam_folder)\n",
    "    weights = model.get_attention(train)\n",
    "\n",
    "    \n",
    "    # for k in interpretability_weights:\n",
    "    #   w = weights[k]\n",
    "    #   if k == 'decoder_self_attn':\n",
    "    #     w = w.mean(axis=0)\n",
    "    #     batch_size,_,_ = w.shape\n",
    "    #     counts+=batch_size\n",
    "    #   if interpretability_weights[k] is None:\n",
    "    #     interpretability_weights[k] = w.sum(axis=0)\n",
    "    #   else:\n",
    "    #     interpretability_weights[k] += w.sum(axis=0)\n",
    "\n",
    "\n",
    "  # interpretability_weight = {k: interpretability_weights[k]/counts for k in interpretability_weights}\n",
    "\n",
    "  print('Done.')\n",
    "  return weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiDimensional Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First lets take the data and load it into a tf.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39737, 25)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFTdfTotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mappings = {'Static':['2 Age Distribution','3 Air Pollution','4 Comorbidities','5 Health Disparities',\n",
    "                         '6 Mobility','7 Residential Density'],\n",
    "                'ID':['Location'],\n",
    "                'Time': ['Time from Start'],\n",
    "                'Target':['0 Cases Root 2'],\n",
    "                'Future':['14 LinearSpace', '15 Constant', '16 LinearTime','17 P2-Time',\n",
    "                          '18 P3-Time', '19 P4-Time', '20 CosWeekly','21 SinWeekly'],\n",
    "                'Known Regular':['2 Age Distribution','3 Air Pollution','4 Comorbidities','5 Health Disparities',\n",
    "                         '6 Mobility','7 Residential Density','8 Disease Spread', '9 Social Distancing',\n",
    "       '10 Testing', '11 Transmissible Cases', '12 VaccinationOneDose',\n",
    "       '13 Vaccination']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RawLabel</th>\n",
       "      <th>Location</th>\n",
       "      <th>Time from Start</th>\n",
       "      <th>0 Cases Root 2</th>\n",
       "      <th>1 Deaths Root 2</th>\n",
       "      <th>2 Age Distribution</th>\n",
       "      <th>3 Air Pollution</th>\n",
       "      <th>4 Comorbidities</th>\n",
       "      <th>5 Health Disparities</th>\n",
       "      <th>6 Mobility</th>\n",
       "      <th>...</th>\n",
       "      <th>12 VaccinationOneDose</th>\n",
       "      <th>13 Vaccination</th>\n",
       "      <th>14 LinearSpace</th>\n",
       "      <th>15 Constant</th>\n",
       "      <th>16 LinearTime</th>\n",
       "      <th>17 P2-Time</th>\n",
       "      <th>18 P3-Time</th>\n",
       "      <th>19 P4-Time</th>\n",
       "      <th>20 CosWeekly</th>\n",
       "      <th>21 SinWeekly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Coffee County Alabama</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Coffee County Alabama</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.988072</td>\n",
       "      <td>-0.976215</td>\n",
       "      <td>0.960515</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Coffee County Alabama</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.976191</td>\n",
       "      <td>-0.952666</td>\n",
       "      <td>0.921738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Coffee County Alabama</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>0.964358</td>\n",
       "      <td>-0.929354</td>\n",
       "      <td>0.883663</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Coffee County Alabama</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>0.952572</td>\n",
       "      <td>-0.906277</td>\n",
       "      <td>0.846281</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RawLabel               Location  Time from Start  0 Cases Root 2  \\\n",
       "0       0.0  Coffee County Alabama              0.0             0.0   \n",
       "1       1.0  Coffee County Alabama              1.0             0.0   \n",
       "2       2.0  Coffee County Alabama              2.0             0.0   \n",
       "3       3.0  Coffee County Alabama              3.0             0.0   \n",
       "4       4.0  Coffee County Alabama              4.0             0.0   \n",
       "\n",
       "   1 Deaths Root 2  2 Age Distribution  3 Air Pollution  4 Comorbidities  \\\n",
       "0              0.0            0.482809         0.806122         0.456746   \n",
       "1              0.0            0.482809         0.806122         0.456746   \n",
       "2              0.0            0.482809         0.806122         0.456746   \n",
       "3              0.0            0.482809         0.806122         0.456746   \n",
       "4              0.0            0.482809         0.806122         0.456746   \n",
       "\n",
       "   5 Health Disparities  6 Mobility  ...  12 VaccinationOneDose  \\\n",
       "0              0.463941     0.64972  ...                    0.0   \n",
       "1              0.463941     0.64972  ...                    0.0   \n",
       "2              0.463941     0.64972  ...                    0.0   \n",
       "3              0.463941     0.64972  ...                    0.0   \n",
       "4              0.463941     0.64972  ...                    0.0   \n",
       "\n",
       "   13 Vaccination  14 LinearSpace  15 Constant  16 LinearTime  17 P2-Time  \\\n",
       "0             0.0             0.0          0.5       0.000000    1.000000   \n",
       "1             0.0             0.0          0.5       0.001988    0.988072   \n",
       "2             0.0             0.0          0.5       0.003976    0.976191   \n",
       "3             0.0             0.0          0.5       0.005964    0.964358   \n",
       "4             0.0             0.0          0.5       0.007952    0.952572   \n",
       "\n",
       "   18 P3-Time  19 P4-Time  20 CosWeekly  21 SinWeekly  \n",
       "0   -1.000000    1.000000     -0.222521     -0.974928  \n",
       "1   -0.976215    0.960515      0.623490     -0.781832  \n",
       "2   -0.952666    0.921738      1.000000      0.000000  \n",
       "3   -0.929354    0.883663      0.623490      0.781832  \n",
       "4   -0.906277    0.846281     -0.222521      0.974928  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFTdfTotal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data):\n",
    "    \n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    def _batch_single_entity(input_data):\n",
    "        time_steps = len(input_data)\n",
    "        lags = 28\n",
    "        x = input_data.values\n",
    "        if time_steps >= lags:\n",
    "            return np.stack([x[i:time_steps - (lags - 1) + i, :] for i in range(lags)], axis=1)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    id_col = col_mappings['ID']\n",
    "    time_col = col_mappings['Time']\n",
    "    target_col = col_mappings['Target']\n",
    "    \n",
    "    input_cols = col_mappings['Known Regular'] + col_mappings['Future']\n",
    "    \n",
    "    data_map = {}\n",
    "    for _, sliced in data.groupby(id_col):\n",
    "        \n",
    "        for k in col_mappings:\n",
    "            cols = col_mappings[k]\n",
    "            arr = _batch_single_entity(sliced[cols].copy())\n",
    "            \n",
    "            if k not in data_map:\n",
    "                data_map[k] = [arr]\n",
    "            else:\n",
    "                data_map[k].append(arr)\n",
    "                \n",
    "    for k in data_map:\n",
    "        data_map[k] = np.concatenate(data_map[k],axis=0)\n",
    "        \n",
    "    data_map['Target'] = data_map['Target'][:,13:,:]\n",
    "    \n",
    "    active_entries = np.ones_like(data_map['Target'])\n",
    "    if 'active_entries' not in data_map:\n",
    "        data_map['active_entries'] = active_entries\n",
    "    else:\n",
    "        data_map['active_entries'].append(active_entries)\n",
    "        \n",
    "    return data_map\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data = batch_data(TFTdfTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = np.concatenate((batched_data['Known Regular'],batched_data['Future']),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = tf.data.Dataset.from_tensor_slices((all_inputs,batched_data['Target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = TFTdfTotal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tdf.pop('0 Cases Root 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tdf[tdf.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2 Age Distribution</th>\n",
       "      <th>3 Air Pollution</th>\n",
       "      <th>4 Comorbidities</th>\n",
       "      <th>5 Health Disparities</th>\n",
       "      <th>6 Mobility</th>\n",
       "      <th>7 Residential Density</th>\n",
       "      <th>8 Disease Spread</th>\n",
       "      <th>9 Social Distancing</th>\n",
       "      <th>10 Testing</th>\n",
       "      <th>11 Transmissible Cases</th>\n",
       "      <th>12 VaccinationOneDose</th>\n",
       "      <th>13 Vaccination</th>\n",
       "      <th>14 LinearSpace</th>\n",
       "      <th>15 Constant</th>\n",
       "      <th>16 LinearTime</th>\n",
       "      <th>17 P2-Time</th>\n",
       "      <th>18 P3-Time</th>\n",
       "      <th>19 P4-Time</th>\n",
       "      <th>20 CosWeekly</th>\n",
       "      <th>21 SinWeekly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>0.163959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>0.163959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.988072</td>\n",
       "      <td>-0.976215</td>\n",
       "      <td>0.960515</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>0.163959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.976191</td>\n",
       "      <td>-0.952666</td>\n",
       "      <td>0.921738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>0.163959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>0.964358</td>\n",
       "      <td>-0.929354</td>\n",
       "      <td>0.883663</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.456746</td>\n",
       "      <td>0.463941</td>\n",
       "      <td>0.64972</td>\n",
       "      <td>0.163959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>0.952572</td>\n",
       "      <td>-0.906277</td>\n",
       "      <td>0.846281</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2 Age Distribution  3 Air Pollution  4 Comorbidities  5 Health Disparities  \\\n",
       "0            0.482809         0.806122         0.456746              0.463941   \n",
       "1            0.482809         0.806122         0.456746              0.463941   \n",
       "2            0.482809         0.806122         0.456746              0.463941   \n",
       "3            0.482809         0.806122         0.456746              0.463941   \n",
       "4            0.482809         0.806122         0.456746              0.463941   \n",
       "\n",
       "   6 Mobility  7 Residential Density  8 Disease Spread  9 Social Distancing  \\\n",
       "0     0.64972               0.163959               0.0                  1.0   \n",
       "1     0.64972               0.163959               0.0                  1.0   \n",
       "2     0.64972               0.163959               0.0                  1.0   \n",
       "3     0.64972               0.163959               0.0                  1.0   \n",
       "4     0.64972               0.163959               0.0                  1.0   \n",
       "\n",
       "   10 Testing  11 Transmissible Cases  12 VaccinationOneDose  13 Vaccination  \\\n",
       "0       0.000                     0.0                    0.0             0.0   \n",
       "1       0.000                     0.0                    0.0             0.0   \n",
       "2       0.000                     0.0                    0.0             0.0   \n",
       "3       0.000                     0.0                    0.0             0.0   \n",
       "4       0.012                     0.0                    0.0             0.0   \n",
       "\n",
       "   14 LinearSpace  15 Constant  16 LinearTime  17 P2-Time  18 P3-Time  \\\n",
       "0             0.0          0.5       0.000000    1.000000   -1.000000   \n",
       "1             0.0          0.5       0.001988    0.988072   -0.976215   \n",
       "2             0.0          0.5       0.003976    0.976191   -0.952666   \n",
       "3             0.0          0.5       0.005964    0.964358   -0.929354   \n",
       "4             0.0          0.5       0.007952    0.952572   -0.906277   \n",
       "\n",
       "   19 P4-Time  20 CosWeekly  21 SinWeekly  \n",
       "0    1.000000     -0.222521     -0.974928  \n",
       "1    0.960515      0.623490     -0.781832  \n",
       "2    0.921738      1.000000      0.000000  \n",
       "3    0.883663      0.623490      0.781832  \n",
       "4    0.846281     -0.222521      0.974928  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 13:14:30.571091: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-08 13:14:30.571170: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# tf_data = tf.data.Dataset.from_tensor_slices((features,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1349,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return (ds.cache().batch(BATCH_SIZE))\n",
    "\n",
    "train_batches = make_batches(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The calling iterator did not fully read the dataset being cached. \n",
    "In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. \n",
    "This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. \n",
    "You should use `dataset.take(k).cache().repeat()` instead.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1351,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_batched = make_batches(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print('Attention weights are:')\n",
    "    print(temp_attn)\n",
    "    print('Output is:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_mask(attn_inputs):\n",
    "    \n",
    "    len_s = tf.shape(attn_inputs)[1]\n",
    "    bs = tf.shape(self_attn_inputs)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape = bs), 1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1403,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_seq_len,\n",
    "                 maximum_position_encoding, \n",
    "                 rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_seq_len, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_seq_len,\n",
    "                 maximum_position_encoding, \n",
    "                 rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_seq_len, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1468,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiD_EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, time_steps, known_reg_inputs,\n",
    "                 future_inputs, static_inputs,hls=64,cat_inputs=None):\n",
    "        super(HiD_EmbeddingLayer,self).__init__()\n",
    "        \n",
    "        #self.inputs = inputs\n",
    "        self.time_steps = time_steps\n",
    "        self.known_locs = known_reg_inputs\n",
    "        self.future_locs = future_inputs\n",
    "        self.static_locs = static_inputs\n",
    "        #self.target_loc = target\n",
    "        if cat_inputs:\n",
    "            self.cat = cat_inputs\n",
    "            \n",
    "        self.hls = hls\n",
    "            \n",
    "    def convert_to_real(self,x):\n",
    "        return tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.hls))(x)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        '''\n",
    "        Not set up for categorical inputs currently - therefore regular inputs = inputs\n",
    "        \n",
    "        First trial we will ignore the targets\n",
    "        '''\n",
    "        regular_inputs = inputs[0]\n",
    "        \n",
    "        times, feature = regular_inputs[0].get_shape().as_list()\n",
    "        \n",
    "        \n",
    "        static_inputs = [tf.keras.layers.Dense(self.hls)(regular_inputs[:,0,i:i+1]) \\\n",
    "                         for i in range(feature) if i in self.static_locs]\n",
    "        \n",
    "        static_inputs = tf.stack(static_inputs,axis=1)\n",
    "        \n",
    "        #Unknown embeddings\n",
    "        unknown_inputs = []\n",
    "        for i in range(feature):\n",
    "            if i not in self.known_locs:\n",
    "                e = self.convert_to_real(regular_inputs[Ellipsis, i:i+1])\n",
    "                unknown_inputs.append(e)\n",
    "                \n",
    "        unknown_inputs = tf.stack(unknown_inputs,axis=-1)\n",
    "        \n",
    "        # Target Embedding - how to do this?\n",
    "        \n",
    "        #target_output = tf.stack([convert_to_real(regular_inputs[Ellipsis, i:i+1]) for i in self.target],axis=-1)\n",
    "        \n",
    "        # A prior known\n",
    "        known_inputs = [self.convert_to_real(regular_inputs[Ellipsis, i:i+1]) \\\n",
    "                        for i in self.known_locs if i not in self.static_locs]\n",
    "        \n",
    "        known_combined_inputs = tf.stack(known_inputs, axis=-1)\n",
    "        \n",
    "        return known_combined_inputs, unknown_inputs, static_inputs#target_output, static_inputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1622,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, size, use_time_distributed, use_bias=True,activation=None):\n",
    "        \n",
    "        super(LinearLayer,self).__init__()\n",
    "        \n",
    "        self.size = size\n",
    "        self.use_time_distributed = use_time_distributed\n",
    "        self.use_bias = use_bias\n",
    "        self.activation=activation\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        \n",
    "        linear = tf.keras.layers.Dense(self.size,activation=self.activation,\n",
    "                                       use_bias =self.use_bias)\n",
    "        if self.use_time_distributed:\n",
    "            linear = tf.keras.layers.TimeDistributed(linear)\n",
    "            \n",
    "        output = linear(inputs)\n",
    "            \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1623,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, hls=64,use_time_distributed=True,dropout_rate=None,activation=None):\n",
    "        \n",
    "        super(GLU,self).__init__()\n",
    "        \n",
    "        self.hls = hls\n",
    "        self.use_time_distributed = use_time_distributed\n",
    "        self.dropout_rate=dropout_rate\n",
    "        self.activation=activation\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        \n",
    "        if self.dropout_rate:\n",
    "            inputs = tf.keras.layers.Dropout(self.dropout_rate)(inputs)\n",
    "        \n",
    "\n",
    "        if self.use_time_distributed:\n",
    "            activation_layer = tf.keras.layers.TimeDistributed\\\n",
    "            (tf.keras.layers.Dense(self.hls,activation=self.activation))(inputs)\n",
    "            \n",
    "            gate_layer = tf.keras.layers.TimeDistributed\\\n",
    "            (tf.keras.layers.Dense(self.hls, activation='sigmoid'))(inputs)\n",
    "            \n",
    "        else:\n",
    "            activation_layer = tf.keras.layers.Dense(self.hls, activation=self.activation)(inputs)\n",
    "            \n",
    "            gate_layer = tf.keras.layers.Dense(self.hls, activation='sigmoid')(inputs)\n",
    "            \n",
    "            \n",
    "        return tf.keras.layers.Multiply()([activation_layer,gate_layer]), gate_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualNetwork(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, hls=64, output_size=None,\n",
    "                 dropout_rate=None, additional_context=None,\n",
    "                 use_time_distributed=True, return_gate=False,altered_glu=False):\n",
    "        \n",
    "        super(GatedResidualNetwork,self).__init__()\n",
    "        self.hls = hls\n",
    "        self.use_time_distributed = use_time_distributed\n",
    "        self.return_gate = return_gate\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.additional_context = additional_context\n",
    "        \n",
    "        \n",
    "        self.linear = LinearLayer(hls, activation=None,\n",
    "                             use_time_distributed=self.use_time_distributed)\n",
    "        \n",
    "        if altered_glu:\n",
    "            self.gate_layer = GLU(output_size, dropout_rate = self.dropout_rate,\n",
    "                             use_time_distributed=self.use_time_distributed,activation=None)\n",
    "        else:\n",
    "            self.gate_layer = GLU(hls, dropout_rate = self.dropout_rate,\n",
    "                             use_time_distributed=self.use_time_distributed,activation=None)\n",
    "        \n",
    "\n",
    "    def add_and_norm(self,inputs):\n",
    "        \n",
    "        tmp = tf.keras.layers.add(inputs)\n",
    "        tmp = tf.keras.layers.LayerNormalization()(tmp)\n",
    "        return tmp\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "       \n",
    "        if not self.output_size:\n",
    "            self.output_size = self.hls\n",
    "            skip = inputs\n",
    "        else:\n",
    "\n",
    "            linear = tf.keras.layers.Dense(self.output_size)\n",
    "            if self.use_time_distributed:\n",
    "                linear = tf.keras.layers.TimeDistributed(linear)\n",
    "            skip = linear(inputs)\n",
    "\n",
    "        \n",
    "        hidden = self.linear(inputs)\n",
    "    \n",
    "        if self.additional_context is not None:\n",
    "            hidden = hidden + LinearLayer(self.hls, activation=None,\n",
    "                                         use_time_distributed=self.use_time_distributed,\n",
    "                                         use_bias=False)(self.additional_context)\n",
    "        \n",
    "\n",
    "        hidden = tf.keras.layers.Activation('elu')(hidden)\n",
    "        hidden = LinearLayer(self.hls, activation=None,\n",
    "                            use_time_distributed=self.use_time_distributed)(hidden)\n",
    "\n",
    "        gating_layer, gate = self.gate_layer(hidden)\n",
    "\n",
    "        if self.return_gate:\n",
    "            return self.add_and_norm([skip, gating_layer]), gate\n",
    "        else:\n",
    "            return self.add_and_norm([skip, gating_layer])\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1767,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticVSN(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, hls=64, dropout_rate=.1):\n",
    "        \n",
    "        super(StaticVSN,self).__init__()\n",
    "        self.hls = hls\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        \n",
    "        embedding = inputs\n",
    "        \n",
    "        _, num_static, _ = embedding.get_shape().as_list()\n",
    "        flatten = tf.keras.layers.Flatten()(embedding)#embedding#\n",
    "\n",
    "        mlp_outputs = GatedResidualNetwork(hls=self.hls, output_size=num_static,\n",
    "                                      dropout_rate=self.dropout_rate,\n",
    "                                      use_time_distributed=False,\n",
    "                                      additional_context=None,altered_glu=True)(flatten)\n",
    "\n",
    "\n",
    "        sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "        sparse_weights = tf.expand_dims(sparse_weights, axis=-1)\n",
    "\n",
    "        trans_emb_list = []\n",
    "        for i in range(num_static):\n",
    "\n",
    "            e = GatedResidualNetwork(hls=self.hls,\n",
    "                                    dropout_rate=self.dropout_rate,\n",
    "                                    use_time_distributed=False)(embedding[:,i:i+1,:])\n",
    "            trans_emb_list.append(e)\n",
    "\n",
    "    \n",
    "        transformed_embedding = tf.concat(trans_emb_list,axis=1)\n",
    "        combined = tf.keras.layers.Multiply()([sparse_weights,\n",
    "                                               transformed_embedding])\n",
    "\n",
    "        static_vec = tf.math.reduce_sum(combined, axis=1) #math reduce sum or ?\n",
    "\n",
    "        return static_vec, sparse_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1796,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalVSN(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, hls, dropout_rate=.1):\n",
    "        \n",
    "        super(TemporalVSN,self).__init__()\n",
    "        self.hls = hls\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, static_context):\n",
    "        \n",
    "        _, time_steps, embedding_dim, num_inputs = inputs.get_shape().as_list()\n",
    "        \n",
    "        flatten = tf.reshape(inputs, [-1, time_steps, embedding_dim * num_inputs])\n",
    "        \n",
    "        expanded_static_context = tf.expand_dims(static_context,axis=1)\n",
    "\n",
    "        mlp_outputs, static_gate = GatedResidualNetwork(self.hls,\n",
    "                                                       output_size = num_inputs,\n",
    "                                                       dropout_rate = self.dropout_rate,\n",
    "                                                       use_time_distributed=True,\n",
    "                                                       additional_context = expanded_static_context,\n",
    "                                                       return_gate=True,altered_glu=True)(flatten)\n",
    "        sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "        \n",
    "        sparse_weights = tf.expand_dims(sparse_weights, axis=2)\n",
    "        \n",
    "        trans_emb_list = []\n",
    "        for i in range(num_inputs):\n",
    "            grn_output = GatedResidualNetwork(self.hls,\n",
    "                                              dropout_rate=self.dropout_rate,\n",
    "                                              use_time_distributed=True)(inputs[Ellipsis,i])\n",
    "            trans_emb_list.append(grn_output)\n",
    "            \n",
    "            \n",
    "        transformed_embedding = tf.stack(trans_emb_list, axis=-1)\n",
    "        \n",
    "        combined = tf.keras.layers.Multiply()([sparse_weights, transformed_embedding])\n",
    "        temporal_ctx = tf.math.reduce_sum(combined, axis=-1)#sum or math reduce sum?\n",
    "        \n",
    "        return temporal_ctx, sparse_weights, static_gate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1824,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiDTransformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_layers, d_model,\n",
    "                 num_heads, dff, input_seq_len,\n",
    "                 target_seq_len, pe_input, pe_output, \n",
    "                 known_reg_inputs,future_inputs, static_inputs,\n",
    "                 hls=64,cat_inputs=None,\n",
    "                 rate=.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hls = hls\n",
    "        self.dropout_rate = rate\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = HiD_EmbeddingLayer(time_steps=target_seq_len + input_seq_len,\n",
    "                                           known_reg_inputs=known_reg_inputs, future_inputs=future_inputs,\n",
    "                                           static_inputs = static_inputs)\n",
    "        \n",
    "        # add static VSN\n",
    "        \n",
    "        self.static_vsn = StaticVSN(hls=hls, dropout_rate=rate)\n",
    "        \n",
    "        # add Temporal VSN\n",
    "        \n",
    "        self.temporal_vsn1 = TemporalVSN(hls = hls, dropout_rate = rate)\n",
    "        self.temporal_vsn2 = TemporalVSN(hls = hls, dropout_rate = rate)\n",
    "        \n",
    "        # Static GRNs - to edit if neccessayr \n",
    "        # 1 = static_context_variable_selection\n",
    "        # 2 = static_context_enrichment\n",
    "        # 3 = static_context_state_h\n",
    "        # 4 = static_context_state_c\n",
    "        \n",
    "        \n",
    "        \n",
    "        # next step would be lstm_combine_and_mask\n",
    "        \n",
    "        self.static_grn1 = GatedResidualNetwork(self.hls,\n",
    "                                                dropout_rate=self.dropout_rate,\n",
    "                                                use_time_distributed=False)\n",
    "        self.static_grn2 = GatedResidualNetwork(self.hls,\n",
    "                                                dropout_rate=self.dropout_rate,\n",
    "                                                use_time_distributed=False)\n",
    "        self.static_grn3 = GatedResidualNetwork(self.hls,\n",
    "                                                dropout_rate=self.dropout_rate,\n",
    "                                                use_time_distributed=False)\n",
    "        self.static_grn4 = GatedResidualNetwork(self.hls,\n",
    "                                                dropout_rate=self.dropout_rate,\n",
    "                                                use_time_distributed=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_seq_len,rate)\n",
    "        \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_seq_len,rate)\n",
    "        \n",
    "        \n",
    "        self.self_attn_layer = MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                             d_model=self.d_model)\n",
    "        \n",
    "    def add_and_norm(self,inputs):\n",
    "        \n",
    "        tmp = tf.keras.layers.add(inputs)\n",
    "        tmp = tf.keras.layers.LayerNormalization()(tmp)\n",
    "        return tmp\n",
    "    \n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        \n",
    "        inp,tar = inputs\n",
    "        \n",
    "        known_emb, unknown_emb, static_emb = self.embedding(inputs)\n",
    "        print('known embeddings.shape')\n",
    "        print(known_emb.shape)\n",
    "        print('static embeddings shape')\n",
    "        print(static_emb.shape)\n",
    "        print('unknown embeddings shape')\n",
    "        print(unknown_emb.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if unknown_emb is not None:\n",
    "            historical_inputs = tf.concat([unknown_emb[:,:self.input_seq_len,:],\n",
    "                                            known_emb[:,:self.input_seq_len,:]],axis=-1)\n",
    "        print('historical inputs')\n",
    "        print(historical_inputs.shape)\n",
    "        \n",
    "        future_inputs = known_emb[:, self.input_seq_len:,:]\n",
    "        print('future inputs shape')\n",
    "        print(future_inputs.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        static_encoder, static_weights = self.static_vsn(static_emb)\n",
    "        print('static encoder')\n",
    "        print(static_encoder.shape)\n",
    "        print('static weights')\n",
    "        print(static_weights.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        static_context_variable_selection = self.static_grn1(static_encoder)\n",
    "        static_context_enrichment = self.static_grn2(static_encoder)\n",
    "        static_context_state_h = self.static_grn3(static_encoder)\n",
    "        static_context_state_c = self.static_grn4(static_encoder)\n",
    "        \n",
    "        #temporal VSN\n",
    "        \n",
    "        historical_features, historical_flags, _ = self.temporal_vsn1(historical_inputs,\n",
    "                                                                     static_context_variable_selection)\n",
    "        \n",
    "        future_features, future_flags, _ = self.temporal_vsn2(future_inputs,\n",
    "                                                             static_context_variable_selection)\n",
    "        \n",
    "        print('Historical Features')\n",
    "        print(historical_features.shape)\n",
    "        print('Historical Flags')\n",
    "        print(historical_flags.shape)\n",
    "        print('Future Features')\n",
    "        print(future_features.shape)\n",
    "        print('Future Flags')\n",
    "        print(future_flags.shape)\n",
    "        \n",
    "       \n",
    "        expanded_static_context = tf.expand_dims(static_context_enrichment,axis=1)\n",
    "        \n",
    "        input_embeddings = tf.concat([historical_features,future_features],axis=1)\n",
    "        \n",
    "        enriched, _ = GatedResidualNetwork(hls=self.hls,\n",
    "                                           dropout_rate = self.dropout_rate,\n",
    "                                           use_time_distributed=True,\n",
    "                                           additional_context = expanded_static_context,\n",
    "                                           return_gate=True)(input_embeddings)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('ENRICHED')\n",
    "        print(enriched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1825,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mappings = {'Static':['2 Age Distribution','3 Air Pollution','4 Comorbidities','5 Health Disparities',\n",
    "                         '6 Mobility','7 Residential Density'],\n",
    "                'ID':['Location'],\n",
    "                'Time': ['Time from Start'],\n",
    "                'Target':['0 Cases Root 2'],\n",
    "                'Future':['14 LinearSpace', '15 Constant', '16 LinearTime','17 P2-Time',\n",
    "                          '18 P3-Time', '19 P4-Time', '20 CosWeekly','21 SinWeekly'],\n",
    "                'Known Regular':['2 Age Distribution','3 Air Pollution','4 Comorbidities','5 Health Disparities',\n",
    "                         '6 Mobility','7 Residential Density','8 Disease Spread', '9 Social Distancing',\n",
    "       '10 Testing', '11 Transmissible Cases', '12 VaccinationOneDose',\n",
    "       '13 Vaccination']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1826,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_locs = [0,1,2,3,4,5]\n",
    "future_locs = [12,13,14,15,16,17,18,19]\n",
    "known_reg_locs = static_locs + future_locs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1827,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = HiDTransformer(num_layers=1, d_model=128,\n",
    "                 num_heads=4, dff=512, input_seq_len=13,\n",
    "                 target_seq_len=15, pe_input=1000, pe_output=1000, \n",
    "                 known_reg_inputs=known_reg_locs,future_inputs=future_locs, static_inputs=static_locs,\n",
    "                 hls=64,cat_inputs=None,\n",
    "                 rate=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1828,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1829,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1830,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1831,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./independent_study2/COVIDJuly2020/checkpoints/CovidA21-TFT2Extended-JulyCutoff-TFTJupyterdir/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1832,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1833,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1834,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_step_signature = [\n",
    "#     tf.TensorSpec(shape=(None, None,None), dtype=tf.float32),\n",
    "#     tf.TensorSpec(shape=(None, None,None), dtype=tf.float32),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1835,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()#input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer([inp, tar],\n",
    "                                 training = True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1836,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known embeddings.shape\n",
      "(64, 28, 64, 8)\n",
      "static embeddings shape\n",
      "(64, 6, 64)\n",
      "unknown embeddings shape\n",
      "(64, 28, 64, 6)\n",
      "historical inputs\n",
      "(64, 13, 64, 14)\n",
      "future inputs shape\n",
      "(64, 15, 64, 8)\n",
      "static encoder\n",
      "(64, 64)\n",
      "static weights\n",
      "(64, 6, 1)\n",
      "Historical Features\n",
      "(64, 13, 64)\n",
      "Historical Flags\n",
      "(64, 13, 1, 14)\n",
      "Future Features\n",
      "(64, 15, 64)\n",
      "Future Flags\n",
      "(64, 15, 1, 8)\n",
      "ENRICHED\n",
      "(64, 28, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 18:58:51.844822: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_10083/839387107.py\", line 7, in train_step  *\n        loss = loss_function(tar_real, predictions)\n\n    NameError: name 'loss_function' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_10083/1975386664.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: in user code:\n\n    File \"/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_10083/839387107.py\", line 7, in train_step  *\n        loss = loss_function(tar_real, predictions)\n\n    NameError: name 'loss_function' is not defined\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "        print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdFLCEQL12a3"
   },
   "source": [
    "## Extract Weights for Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "2hc6Qffl4Daq",
    "outputId": "85075462-881c-404b-cbd1-51f5dd91476b"
   },
   "outputs": [],
   "source": [
    "class AttentionWeights():\n",
    "\n",
    "  def __init__(self, flagMatrix, numLoc, totTimeSteps, columnDefinition,inpSize, predSize, quantiles):\n",
    "\n",
    "    self.input_size = inpSize\n",
    "    self.prediction_size = predSize\n",
    "    self.total_sequence_length = inpSize + predSize\n",
    "    self.weights = flagMatrix\n",
    "    self.locations = numLoc \n",
    "    self.timesteps = totTimeSteps\n",
    "\n",
    "    self.quantiles = quantiles\n",
    "\n",
    "    self.full_length = totTimeSteps - 1 + inpSize + predSize\n",
    "\n",
    "    self.sequences = flagMatrix['static_flags'].shape[0]\n",
    "    \n",
    "    self.static_features = flagMatrix['static_flags'].shape[1]\n",
    "\n",
    "    self.decoder_attention = flagMatrix['decoder_self_attn']\n",
    "\n",
    "    self.column_definition = columnDefinition\n",
    "\n",
    "    self.identifiers = flagMatrix['identifiers']\n",
    "    self.timeID = flagMatrix['time']\n",
    "\n",
    "    self.known = None\n",
    "    self.observed = None\n",
    "    self.historical = None\n",
    "    self.static = None\n",
    "    self.unknown = None\n",
    "    self.target = None\n",
    "  \n",
    "    self.setKnown()\n",
    "    self.setObserved()\n",
    "    self.setStatic()\n",
    "    self.setTarget()\n",
    "\n",
    "    self.setUnknown()\n",
    "    self.setHistorical()\n",
    "    #historical_inputs = unknown + known + observed ---> unknown features in TFT should be removed?\n",
    "\n",
    "  def setKnown(self):\n",
    "\n",
    "    KnownInputs = []\n",
    "\n",
    "    for Column in self.column_definition:\n",
    "      if Column[2] == InputTypes.KNOWN_INPUT:\n",
    "        KnownInputs.append(Column)\n",
    "\n",
    "    self.known = KnownInputs\n",
    "\n",
    "  def setObserved(self):\n",
    "\n",
    "    ObservedInputs = []\n",
    "\n",
    "    for Column in self.column_definition:\n",
    "      if Column[2] == InputTypes.OBSERVED_INPUT:\n",
    "        ObservedInputs.append(Column)\n",
    "    self.observed = ObservedInputs\n",
    "\n",
    "  def setStatic(self):\n",
    "\n",
    "    StaticInputs = []\n",
    "\n",
    "    for Column in self.column_definition:\n",
    "      if Column[2] == InputTypes.STATIC_INPUT:\n",
    "        StaticInputs.append(Column)\n",
    "\n",
    "    self.static = StaticInputs\n",
    "\n",
    "  def setTarget(self):\n",
    "\n",
    "    Targets = []\n",
    "    for Column in self.column_definition:\n",
    "      if Column[2] == InputTypes.TARGET:\n",
    "        Targets.append(Column)\n",
    "    self.target = Targets\n",
    "\n",
    "\n",
    "  # This function will declare the inputs that the TFT does not recognize\n",
    "  def setUnknown(self):\n",
    "    UnknownInputs = []\n",
    "\n",
    "    for Column in self.column_definition:\n",
    "      if Column[2] not in [InputTypes.TARGET, InputTypes.KNOWN_INPUT, InputTypes.OBSERVED_INPUT,\n",
    "                        InputTypes.STATIC_INPUT]:\n",
    "        UnknownInputs.append(Column)\n",
    "\n",
    "    self.unknown = UnknownInputs\n",
    "    \n",
    "  def setHistorical(self):\n",
    "    \n",
    "    if self.unknown:\n",
    "      Historical = self.unknown + self.known + self.observed\n",
    "    else: \n",
    "      Historical = self.known + self.observed\n",
    "\n",
    "    self.historical = Historical\n",
    "    print(self.historical)\n",
    "    \n",
    "\n",
    "\n",
    "  def getStaticWeights(self):\n",
    "\n",
    "    staticMatrix = np.zeros((self.locations, self.static_features))\n",
    "    LocationCounter=0\n",
    "    \n",
    "    for Sequence in range(self.sequences):\n",
    "      if Sequence != 0 and Sequence % self.timesteps == 0:\n",
    "        LocationCounter += 1\n",
    "      staticMatrix[LocationCounter] = self.weights['static_flags'][Sequence]\n",
    "\n",
    "    SMatrix = np.zeros((self.locations, self.full_length, self.static_features))\n",
    "\n",
    "    for idx,i in enumerate(SMatrix):\n",
    "      for jdx, j in enumerate(i):\n",
    "        SMatrix[idx][jdx] = staticMatrix[idx]\n",
    "\n",
    "    return SMatrix\n",
    "\n",
    "  def getFutureWeights(self):\n",
    "    seq, times, feat = self.weights['future_flags'].shape\n",
    "    #Construct new matrix to store averages \n",
    "    #   shape = (Location x TimeSteps x Features)\n",
    "\n",
    "    futureMatrix = np.zeros((self.locations, self.timesteps+times-1, feat))\n",
    "    print(futureMatrix.shape)\n",
    "\n",
    "    locCounter=0\n",
    "    TimeCounter=0\n",
    "\n",
    "    for Sequence in range(seq):#i goes from 0 to 29964 (length = 29965)\n",
    "\n",
    "      if Sequence != 0 and Sequence % self.timesteps == 0:\n",
    "        locCounter += 1\n",
    "        TimeCounter = 0\n",
    "\n",
    "      for TimeStep in range(times):  # TimeStep goes from 0 to 14 (length = 15)\n",
    "        futureMatrix[locCounter,TimeCounter+TimeStep] += self.weights['future_flags'][Sequence, TimeStep]\n",
    "\n",
    "      TimeCounter += 1\n",
    "\n",
    "    # Divide Matrix ---> to incorporate this into the above code\n",
    "    for idx,i in enumerate(futureMatrix):\n",
    "      for jdx,j in enumerate(i):\n",
    "        if jdx >= times-1 and jdx <= futureMatrix.shape[1] - times:\n",
    "          futureMatrix[idx,jdx] = np.divide(futureMatrix[idx,jdx], times)\n",
    "        else:\n",
    "          divisor = min(abs(jdx+1), abs(futureMatrix.shape[1]-jdx))\n",
    "          futureMatrix[idx,jdx] = np.divide(futureMatrix[idx,jdx], divisor)\n",
    "\n",
    "    FMatrix = np.zeros((self.locations, self.full_length,feat))\n",
    "    FMatrix[:,self.input_size:,:] = futureMatrix\n",
    "  \n",
    "    return FMatrix\n",
    "\n",
    "  def getHistoricalWeights(self):\n",
    "    \n",
    "    seq, times, feat = self.weights['historical_flags'].shape\n",
    "    #Construct new matrix to store averages \n",
    "    #   shape = (Location x TimeSteps x Features)\n",
    "\n",
    "    historicalMatrix = np.zeros((self.locations, self.timesteps+times-1, feat))\n",
    "    print(historicalMatrix.shape)\n",
    "\n",
    "    locCounter=0\n",
    "    TimeCounter=0\n",
    "\n",
    "    for Sequence in range(seq):#i goes from 0 to 29964 (length = 29965)\n",
    "\n",
    "      if Sequence != 0 and Sequence % self.timesteps == 0:\n",
    "        locCounter += 1\n",
    "        TimeCounter = 0\n",
    "\n",
    "      for TimeStep in range(times):  # TimeStep goes from 0 to 14 (length = 15)\n",
    "        historicalMatrix[locCounter,TimeCounter+TimeStep] += self.weights['historical_flags'][Sequence, TimeStep]\n",
    "\n",
    "      TimeCounter += 1\n",
    "\n",
    "    # Divide Matrix ---> to incorporate this into the above code\n",
    "    for idx,i in enumerate(historicalMatrix):\n",
    "      for jdx,j in enumerate(i):\n",
    "        if jdx >= times-1 and jdx <= historicalMatrix.shape[1] - times:\n",
    "          historicalMatrix[idx,jdx] = np.divide(historicalMatrix[idx,jdx], times)\n",
    "        else:\n",
    "          divisor = min(abs(jdx+1), abs(historicalMatrix.shape[1]-jdx))\n",
    "          historicalMatrix[idx,jdx] = np.divide(historicalMatrix[idx,jdx], divisor)\n",
    "\n",
    "    HMatrix = np.zeros((self.locations, self.full_length, feat))\n",
    "    HMatrix[:,:self.timesteps+times-1,:] = historicalMatrix\n",
    "  \n",
    "    return HMatrix\n",
    "\n",
    "  def getTemporalImportance(self,averageAcrossHeads=True):\n",
    "      \n",
    "    TempMatrix = self.decoder_attention\n",
    "    if averageAcrossHeads:\n",
    "      TempMatrix = TempMatrix.mean(axis=0).mean(axis=0)\n",
    "    else:\n",
    "      TempMatrix = TempMatrix.mean(axis=1)\n",
    "\n",
    "    return TempMatrix\n",
    "\n",
    "  def getTemporalQuantiles(self, averageAcrossHeads=True):\n",
    "\n",
    "    TempMatrix = self.decoder_attention\n",
    "    if averageAcrossHeads:\n",
    "      TempMatrix = TempMatrix.mean(axis=0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 2502
    },
    "id": "iDTqsOeioG3Q",
    "outputId": "60e80f69-6061-43bb-cbc4-a807718aa43d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m02/08/2022, 11:41:31 UTC \u001b[35mStart TFT  CovidA21-TFT2Extended-JulyCutoff-TFTJupyter  \n",
      "              This is the TFT model fit for group number 2 in the rurality based stratification. The data is from beginning until July 2021.\n",
      "              Feature importance is included in this modeling.\n",
      "              \u001b[0m\n",
      "Selecting GPU ID=0\n",
      "*** Training from defined parameters for FFFFWNPF ***\n",
      "Loading & splitting data...\n",
      "Formatting train-valid-test splits.\n",
      "Setting scalers with training data...\n",
      "[('Location', 3, 4), ('Time from Start', 0, 5), ('RawLabel', 0, -1), ('0 Cases Root 2', 0, 0), ('1 Deaths Root 2', 0, 0), ('2 Age Distribution', 0, 3), ('3 Air Pollution', 0, 3), ('4 Comorbidities', 0, 3), ('5 Health Disparities', 0, 3), ('6 Mobility', 0, 3), ('7 Residential Density', 0, 3), ('8 Disease Spread', 0, 1), ('9 Social Distancing', 0, 1), ('10 Testing', 0, 1), ('11 Transmissible Cases', 0, 1), ('12 VaccinationOneDose', 0, 1), ('13 Vaccination', 0, 1), ('14 LinearSpace', 0, 2), ('15 Constant', 0, 2), ('16 LinearTime', 0, 2), ('17 P2-Time', 0, 2), ('18 P3-Time', 0, 2), ('19 P4-Time', 0, 2), ('20 CosWeekly', 0, 2), ('21 SinWeekly', 0, 2)]\n",
      "InputTypes.TARGET\n",
      "*** Loading hyperparm manager ***\n",
      "*** Running calibration ***\n",
      "Params Selected:\n",
      "Hello world\n",
      "Resetting temp folder...\n",
      "*** TemporalFusionTransformer params ***\n",
      "# dropout_rate = 0.2\n",
      "# hidden_layer_size = 64\n",
      "# learning_rate = 0.001\n",
      "# minibatch_size = 64\n",
      "# max_gradient_norm = 0.01\n",
      "# num_heads = 4\n",
      "# stack_size = 1\n",
      "# model_folder = ../TFTData/saved_models/FFFFWNPF/fixedCovidA21-TFT2Extended-JulyCutoff-TFTJupyter\n",
      "# total_time_steps = 28\n",
      "# num_encoder_steps = 13\n",
      "# num_epochs = 60\n",
      "# early_stopping_patience = 60\n",
      "# multiprocessing_workers = 5\n",
      "# column_definition = [('Location', 3, 4), ('Time from Start', 0, 5), ('RawLabel', 0, -1), ('0 Cases Root 2', 0, 0), ('1 Deaths Root 2', 0, 0), ('2 Age Distribution', 0, 3), ('3 Air Pollution', 0, 3), ('4 Comorbidities', 0, 3), ('5 Health Disparities', 0, 3), ('6 Mobility', 0, 3), ('7 Residential Density', 0, 3), ('8 Disease Spread', 0, 1), ('9 Social Distancing', 0, 1), ('10 Testing', 0, 1), ('11 Transmissible Cases', 0, 1), ('12 VaccinationOneDose', 0, 1), ('13 Vaccination', 0, 1), ('14 LinearSpace', 0, 2), ('15 Constant', 0, 2), ('16 LinearTime', 0, 2), ('17 P2-Time', 0, 2), ('18 P3-Time', 0, 2), ('19 P4-Time', 0, 2), ('20 CosWeekly', 0, 2), ('21 SinWeekly', 0, 2)]\n",
      "# input_size = 23\n",
      "# output_size = 2\n",
      "# category_counts = []\n",
      "# input_obs_loc = [1, 2]\n",
      "# static_input_loc = [3, 4, 5, 6, 7, 8]\n",
      "# known_regular_inputs = [3, 4, 5, 6, 7, 8, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "# known_categorical_inputs = []\n",
      "# quantiles = [1.0]\n",
      "# quantilenames = ['MSE']\n",
      "# quantileindex = 0\n",
      "# TFTLSTMFinalMLP = 128\n",
      "# TFTOption1 = 1\n",
      "# TFTMultivariate = True\n",
      "# TFTFinalGatingOption = 1\n",
      "# TFTSymbolicWindows = False\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:828 all_inputs shape=(None, 28, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:31.415639: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-08 11:41:31.415695: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-02-08 11:41:31.460938: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-08 11:41:31.460974: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-02-08 11:41:31.509312: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.511674: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.513060: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.514315: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.515693: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.516966: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.526908: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.528969: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.538170: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.539819: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.541279: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.550515: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.553087: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.560383: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.562334: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.564205: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.573382: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.575721: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.582867: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.584864: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.587118: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.596871: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.599693: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.607070: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.609429: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.611620: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.620966: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.624077: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.631826: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.634820: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.637842: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.649752: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.652981: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.661146: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.664773: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.668082: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.681613: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.685574: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.693782: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.697208: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.700282: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.736636: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.740841: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.748338: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.751621: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.754757: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.766780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.770555: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.778420: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.782223: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.785657: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.797856: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.801827: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.809106: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.812843: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.816840: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:578 regular_inputs shape=(None, 28, 23)\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:579 obs_inputs shape=(None, 28, 64, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:31.830129: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.835290: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.844638: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.849590: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.853827: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.866944: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.871416: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.878814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.882959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.887003: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.900331: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.905274: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.912706: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.917149: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.921381: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.937194: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.943004: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.951118: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.956015: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.960601: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.974470: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.980126: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.987945: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.992933: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:31.997637: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.011080: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.017093: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.024880: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.030148: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.035301: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.050308: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.056251: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.064406: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.070237: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.075586: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.089768: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.095840: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.103229: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.108944: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.114515: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.129142: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.135593: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.143052: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.148730: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.154428: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.169307: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.176204: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.184303: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.190718: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.196809: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.211841: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.218765: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.226574: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.232963: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:32.239304: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.257101: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.264913: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.273751: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.280855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.287685: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.401594: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.414111: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:832 unknown_inputs shape=(None, 28, 64, 7)\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:833 known_combined_layer shape=(None, 28, 64, 8)\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:834 obs_inputs shape=(None, 28, 64, 2)\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:835 static_inputs shape=(None, 6, 64)\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:898 static_inputs shape=(None, 6, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:32.448618: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.461811: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.472821: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.545153: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.559780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.582444: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.595640: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.607534: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.678649: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.696357: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.719208: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.733626: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.747909: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.819318: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.838175: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.861688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.878297: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.896277: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.974850: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:32.995919: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.019462: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.038773: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.057791: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.138488: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.162286: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.186030: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.207547: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.229899: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.310364: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.336609: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.362930: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.388653: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.439351: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:900 static_encoder shape=(None, 64)\n",
      "/Users/andrejerkelens/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py:901 static_weights shape=(None, 6, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:33.632007: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.674110: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.750090: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.781667: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.813815: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.845963: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.879228: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.911724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.943497: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:33.974955: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.009282: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.042050: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.075962: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.123417: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.155874: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.187562: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.269714: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.307774: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.334362: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.369000: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.404036: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.489065: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.529661: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.557255: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.594546: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.631244: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.716134: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.759059: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.786551: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.826581: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.866250: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.953258: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:34.999419: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.027246: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.070323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.111897: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.202014: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.250234: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.278372: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.322287: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.366246: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.464092: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.512563: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.541126: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.588610: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.634659: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.732406: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.783129: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.813967: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.862497: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:35.911520: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.011910: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.064494: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.093607: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.144323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.194966: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:36.297170: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.351449: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.381416: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.434027: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.488400: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.595871: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.653706: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.685491: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.740462: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.796634: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.904866: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.964751: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:36.995927: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.055070: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.111771: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.219371: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.280102: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.311851: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.372550: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.434010: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.544044: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.609672: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.642888: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.704760: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.767005: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.878900: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.943663: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:37.975924: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.040921: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.107129: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.222837: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.292019: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.326587: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.395183: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.461735: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.578588: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.649771: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.684131: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.751912: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.821983: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:38.937944: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.008598: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.045975: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.119074: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.189280: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.309982: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.383510: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.417942: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.495398: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.570356: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.641552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.712728: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.801606: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.878933: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:39.904837: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:40.028865: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.106143: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.141709: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.216380: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.291804: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.416420: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.494257: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.531389: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.608321: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.684681: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.814559: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.894254: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:40.930112: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.009018: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.089106: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.214275: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.296446: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.335643: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.416756: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.497883: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.635017: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.723173: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.762446: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.847403: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:41.933998: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.069334: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.158024: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.197117: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.281861: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.368642: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.513113: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.609540: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.651471: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.740072: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.830622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:42.968417: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.061831: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.101620: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.193097: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.282990: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.422992: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.519535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.559733: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.660256: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.793272: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.890393: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:43.918083: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:44.048139: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:44.144346: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:44.171277: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:44.293833: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:44.395394: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:44.425904: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:44.555419: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:44.661391: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:44.688539: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.108534: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.218174: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.275009: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.379167: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.482864: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.585807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.692618: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.799495: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:45.906229: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.007762: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.113242: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.215013: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.318205: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.418152: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.521222: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.740948: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.851218: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:46.900279: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.FinalLoopSize   2\n",
      "Transformer_layer  (None, 28, 64, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:41:47.240357: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:47.376155: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:47.497841: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:47.621235: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:47.746308: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:47.879240: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:48.000691: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:48.029935: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:48.149688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:41:48.271498: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs  (None, 15, 2)\n",
      "Train samples 39737 Valid samples 39737\n",
      "Getting legal sampling locations.\n",
      "train max samples 39737 actual 36419\n",
      "Max samples=39737 exceeds # available segments=36419\n",
      "Cached data \"train\" updated\n",
      "Getting legal sampling locations.\n",
      "valid max samples 39737 actual 36419\n",
      "Max samples=39737 exceeds # available segments=36419\n",
      "Cached data \"valid\" updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:42:09.495043: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Fitting TemporalFusionTransformer ***\n",
      "Getting batched_data\n",
      "Using cached training data\n",
      "Using cached validation data\n",
      "Using keras standard fit\n",
      "data labels  (36419, 28, 23) (36419, 15, 2)\n",
      "Train on 36419 samples, validate on 36419 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 11:42:16.244646: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:42:16.899080: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:42:17.325773: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36419/36419 [==============================] - ETA: 0s - loss: 0.0146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrejerkelens/miniforge3/envs/env/lib/python3.8/site-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2022-02-08 11:46:49.306823: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-08 11:47:43.714653: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36419/36419 [==============================] - 329s 9ms/sample - loss: 0.0146 - val_loss: 0.0036\n",
      "Epoch 2/60\n",
      "14080/36419 [==========>...................] - ETA: 2:25 - loss: 0.0031"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/3806250309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m output_map =train(expt_name=name,\n\u001b[0m\u001b[1;32m      2\u001b[0m      \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_tensorflow_with_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0mmodel_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fixed\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mRunName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mdata_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTFTdfTotal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m# actually dataset NOT file location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mdata_formatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/nq/ydw8sbmd0492r2npkdsbqqvc0000gn/T/ipykernel_76813/4017418300.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(expt_name, use_gpu, model_folder, data_csv_path, data_formatter, restore, use_testing_mode, num_reps)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/HiDT/independent_study2/GPCE/TFToriginal/TFTCode/libs/tft_modelDev.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_df, valid_df)\u001b[0m\n\u001b[1;32m   1278\u001b[0m             \u001b[0mPredictedValvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFTOption1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m             self.model.fit(\n\u001b[0m\u001b[1;32m   1281\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPredictedvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m     return func.fit(\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m     return fit_loop(\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4184\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4186\u001b[0;31m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[1;32m   4187\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[1;32m   4188\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1484\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_map =train(expt_name=name,\n",
    "     use_gpu=use_tensorflow_with_gpu,\n",
    "     model_folder=os.path.join(config.model_folder, \"fixed\" + RunName),\n",
    "     data_csv_path=TFTdfTotal,# actually dataset NOT file location\n",
    "     data_formatter=formatter,\n",
    "     use_testing_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1217
    },
    "id": "KZR9mAD5PiqU",
    "outputId": "04bfbbd3-bf69-4d41-99a4-07628172f82f"
   },
   "outputs": [],
   "source": [
    "int_weights = loadT(expt_name=name,\n",
    "     use_gpu=use_tensorflow_with_gpu,\n",
    "     model_folder=os.path.join(config.model_folder, \"fixed\" + RunName),\n",
    "     data_csv_path=TFTdfTotal,# actually dataset NOT file location\n",
    "     data_formatter=formatter,\n",
    "     use_testing_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "NVv9N0fHPk7C",
    "outputId": "2b7f3d85-be9e-4e41-d59e-7b22d0f7c547"
   },
   "outputs": [],
   "source": [
    "TFTAttentionWeights = AttentionWeights(flagMatrix=int_weights,\n",
    "                                       numLoc=Nloc,\n",
    "                                       totTimeSteps=Num_Seq - TFTFutures + 1 ,\n",
    "                                       columnDefinition=TFTcolumn_definition,\n",
    "                                       inpSize=Tseq,\n",
    "                                       predSize=TFTFutures,\n",
    "                                       quantiles=[.3,.5,.7]\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "HzxQDMl8Psc8",
    "outputId": "8eb60118-3abc-4244-cb7a-ea0d9879614e"
   },
   "outputs": [],
   "source": [
    "TFTWeightShapes = {}\n",
    "for i in int_weights:\n",
    "  TFTWeightShapes[i] = int_weights[i].shape\n",
    "TFTWeightShapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "NIm1b0B3O3rR",
    "outputId": "93e7fc6e-b73d-4fbe-cb1c-82fba0640f91"
   },
   "outputs": [],
   "source": [
    "test1 = np.array([[0,1,2,3],\n",
    "                 [5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nxDp3pcsO8mI",
    "outputId": "31aa2db1-784a-473f-80a0-d69113647703"
   },
   "outputs": [],
   "source": [
    "test1.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-k5AapkBPAYW",
    "outputId": "9c28de15-2c56-4a9f-bacd-2c2131b69242"
   },
   "outputs": [],
   "source": [
    "test1.sum(axis=0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "VIpLtB6aiHnT",
    "outputId": "61ef30b0-7ac5-45be-a1a3-9f70171d43db"
   },
   "outputs": [],
   "source": [
    "int_weights['static_flags'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "kaePH6aJiLAa",
    "outputId": "27424edf-9454-4155-c1b3-38e5ee2d8910"
   },
   "outputs": [],
   "source": [
    "TFTAttentionWeights.static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "wC0ucDjCiXK6",
    "outputId": "34d1b9ad-9ede-47d1-8f62-57d052f5a368"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive/independent_study2/COVIDJuly2020/CovidDecember12-2021/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "W8VVPoXziQAq",
    "outputId": "d8607d9a-3d7b-4192-a5c4-8cfaf3e3e80e"
   },
   "outputs": [],
   "source": [
    "age_dist = pd.read_csv('/content/drive/MyDrive/independent_study2/COVIDJuly2020/CovidDecember12-2021/Age Distribution.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTCOLAIB711x"
   },
   "source": [
    "### Equations for Attention Weights\n",
    "\n",
    "1a. \n",
    "\n",
    "Decoder Self Attention raw matrix $A_{h,s,d,d}$ , <br> where h is number of self attention heads, s is the total number of sequences created from timesteps and counties, and d is the day index in each sequence.\n",
    "\n",
    "Eqn 1a.\n",
    "\n",
    "$Q_u$ represents quantile $u$ of all sequences s for attention $\\alpha_{i,j}$ \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;$ $X_{q,d,d} = Q_u\\left({\\dfrac{\\sum\\limits_{h=1}^{n} \\alpha_{h,s,d,d}}{n}}\\right)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mofCjF-iNP0u"
   },
   "source": [
    "### Save and Interpret TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "budLvcXANYDw",
    "outputId": "d795eff4-0149-48be-c662-f1c50d2aa802"
   },
   "outputs": [],
   "source": [
    "class TFTSaveandInterpret():\n",
    "\n",
    "  def __init__(self,output_map, ReshapedPredictionsTOT):\n",
    "# output_map is a dictionary pointing to dataframes\n",
    "# output_map[\"targets\"]) targets are called outputs on input\n",
    "# output_map[\"p10\"] is   p10 quantile forecast\n",
    "# output_map[\"p50\"] is   p10 quantile forecast\n",
    "# output_map[\"p90\"] is   p10 quantile forecast\n",
    "#  Labelled by last real time in sequence (t-1) which starts at time Tseq-1 going up to Num_Time-1\n",
    "\n",
    "# order of Dataframe columns is 'forecast_time', 'identifier', \n",
    "#'t+0-Obs0', 't+0-Obs1', 't+1-Obs0', 't+1-Obs1', 't+2-Obs0', 't+2-Obs1', 't+3-Obs0', 't+3-Obs1', \n",
    "#'t+4-Obs0', 't+4-Obs1', 't+5-Obs0', 't+5-Obs1', 't+6-Obs0', 't+6-Obs1', 't+7-Obs0', 't+7-Obs1', \n",
    "#'t+8-Obs0', 't+8-Obs1', 't+9-Obs0', 't+9-Obs1', 't+10-Obs0', 't+10-Obs1', 't+11-Obs0', 't+11-Obs1', \n",
    "#'t+12-Obs0', 't+12-Obs1', 't+13-Obs0', 't+13-Obs1', 't+14-Obs0', 't+14-Obs1''\n",
    "\n",
    "# First time is FFFFWNPF Sequence # + Tseq-1\n",
    "# Rows of data frame are ilocation*(Num_Seq+1) + FFFFWNPF Sequence #\n",
    "# ilocation runs from 0 ... Nloc-1 in same order in both TFT and FFFFWNPF\n",
    "    Sizes = output_map[TFTQuantilenames[TFTPrimaryQuantileIndex]].shape\n",
    "    self.Numx = Sizes[0]\n",
    "    self.Numy = Sizes[1]\n",
    "    self.Num_Seq1 = 1 + Num_Seq\n",
    "    self.MaxTFTSeq = self.Num_Seq1-1\n",
    "    expectednumx = self.Num_Seq1*Nloc\n",
    "    if expectednumx != self.Numx:\n",
    "      printexit(' Wrong sizes of TFT compared to FFFFWNPF ' + str(expectednumx) + ' ' + str(self.Numx))\n",
    "    self.FFFFWNPFresults = np.zeros((self.Numx, NpredperseqTOT,3), dtype=np.float32)\n",
    "\n",
    "    mapFFFFtoTFT = np.empty(Nloc, dtype = np.int32)\n",
    "    TFTLoc = output_map[TFTQuantilenames[TFTPrimaryQuantileIndex]]['identifier'].unique()\n",
    "    FFFFWNPFLocLookup = {}\n",
    "    for i,locname in enumerate(FFFFWNPFUniqueLabel):\n",
    "      FFFFWNPFLocLookup[locname] = i\n",
    "    TFTLocLookup = {}\n",
    "    for i,locname in enumerate(TFTLoc):\n",
    "      TFTLocLookup[locname] = i\n",
    "      if FFFFWNPFLocLookup[locname] is None:\n",
    "        printexit('Missing TFT Location '+locname)\n",
    "    for i,locname in enumerate(FFFFWNPFUniqueLabel):\n",
    "      j = TFTLocLookup[locname] \n",
    "      if j is None:\n",
    "        printexit('Missing FFFFWNPF Location '+ locname)\n",
    "      mapFFFFtoTFT[i] = j\n",
    "\n",
    "\n",
    "    indexposition = np.empty(NpredperseqTOT, dtype=int)\n",
    "    output_mapcolumns = output_map[TFTQuantilenames[TFTPrimaryQuantileIndex]].columns\n",
    "    numcols = len(output_mapcolumns)\n",
    "    for ipred in range(0, NpredperseqTOT):\n",
    "      label = PredictionTFTmapping[ipred]\n",
    "      if label == ' ':\n",
    "        indexposition[ipred]=ipred\n",
    "      else:\n",
    "        findpos = -1\n",
    "        for i in range(0,numcols):\n",
    "          if label == output_mapcolumns[i]:\n",
    "            findpos = i\n",
    "        if findpos < 0:\n",
    "          printexit('Missing Output ' +str(ipred) + ' ' +label)    \n",
    "        indexposition[ipred] = findpos\n",
    "\n",
    "    for iquantile in range(0,TFTlenquantiles):\n",
    "      for ilocation in range(0,Nloc):\n",
    "        for seqnumber in range(0,self.Num_Seq1):\n",
    "          \n",
    "          for ipred in range(0,NpredperseqTOT):\n",
    "            label = PredictionTFTmapping[ipred]\n",
    "            if label == ' ': # NOT calculated by TFT\n",
    "              if seqnumber >= Num_Seq:\n",
    "                value = 0.0\n",
    "              else:\n",
    "                value = ReshapedPredictionsTOT[ilocation, seqnumber, ipred]\n",
    "\n",
    "            else:\n",
    "              ActualTFTSeq = seqnumber\n",
    "              if ActualTFTSeq <= self.MaxTFTSeq:\n",
    "                pos = indexposition[ipred]\n",
    "                dfindex = self.Num_Seq1*mapFFFFtoTFT[ilocation] + ActualTFTSeq\n",
    "                value = output_map[TFTQuantilenames[iquantile]].iloc[dfindex,pos] \n",
    "              else:\n",
    "                dfindex = self.Num_Seq1*mapFFFFtoTFT[ilocation] + self.MaxTFTSeq\n",
    "                ifuture = int(ipred/FFFFWNPFNumberTargets)\n",
    "                jfuture = ActualTFTSeq - self.MaxTFTSeq + ifuture\n",
    "                if jfuture <= LengthFutures:\n",
    "                    jpred = ipred + (jfuture-ifuture)*FFFFWNPFNumberTargets\n",
    "                    value = output_map[TFTQuantilenames[iquantile]].iloc[dfindex,indexposition[jpred]]\n",
    "                else:\n",
    "                  value = 0.0\n",
    "            \n",
    "            FFFFdfindex = self.Num_Seq1*ilocation + seqnumber\n",
    "            self.FFFFWNPFresults[FFFFdfindex,ipred,iquantile] = value           \n",
    "\n",
    "  # Default returns the median (50% quantile)\n",
    "  def __call__(self, InputVector, Time= None, training = False, Quantile = None):\n",
    "    lenvector = InputVector.shape[0]\n",
    "    result = np.empty((lenvector,NpredperseqTOT), dtype=np.float32)\n",
    "    if Quantile is None:\n",
    "      Quantile = TFTPrimaryQuantileIndex\n",
    "    for ivector in range(0,lenvector):\n",
    "      dfindex = self.Num_Seq1*InputVector[ivector,0] + InputVector[ivector,1]\n",
    "      result[ivector,:] = self.FFFFWNPFresults[dfindex, :, Quantile]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "oC1SKF7Cn9id",
    "outputId": "c41c171e-960c-4971-9a80-900e37905238"
   },
   "outputs": [],
   "source": [
    "tt = TFTSaveandInterpret(output_map, ReshapedPredictionsTOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "x2TUPZSXQh5D",
    "outputId": "2a942268-5afe-4560-d388-12531e686ee8"
   },
   "outputs": [],
   "source": [
    "DL2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "9zm2lPM2qPiB",
    "outputId": "f0080f29-8fbc-45fb-b774-56a220aabea0"
   },
   "outputs": [],
   "source": [
    "def getGet(output_map, TFTWeights):\n",
    "  MyFFFFWNPFLink = TFTSaveandInterpret(output_map, ReshapedPredictionsTOT)\n",
    "  modelflag = 2\n",
    "  FitPredictions = DLprediction(ReshapedSequencesTOT, RawInputPredictionsTOT, MyFFFFWNPFLink, modelflag, TFTWeights, LabelFit ='TFT')\n",
    "  return FitPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20000,
     "referenced_widgets": [
      "0fefd65637224b83ae31f4a25020d9b9",
      "fd4d6d89c76b4ae5b53e2a84a8540df6",
      "76089de79f784bcb8f31fed7ead393be",
      "ae521a51829149f386d0b8e6492817f2",
      "a55623b7b2ed4432bb8e64f32f0073e8",
      "27500e262ae943888d6eec960ba705e9",
      "33a6f440ddd04eeb97e97f551e059c7e",
      "4c2b55daad0e4245bc45b8e76e269571",
      "8abe47fa4cd647f5b8e18b1d553688c8",
      "bc5f196aefab4e0aa06438258730fa7d",
      "2a4b4fe25593420885d95be763b8c83b"
     ]
    },
    "id": "LmDMuS3jqZdy",
    "outputId": "c268829c-1057-46a4-eee6-ad68c21d582b"
   },
   "outputs": [],
   "source": [
    "boom = getGet(output_map, TFTAttentionWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "8i_tRpVRaQ4x",
    "outputId": "2ec40765-9a6e-402b-ddb9-5d99fcf7eb88"
   },
   "outputs": [],
   "source": [
    "  def VisualizeTFT(output_map, TFTWeights):\n",
    "    MyFFFFWNPFLink = TFTSaveandInterpret(output_map, ReshapedPredictionsTOT)\n",
    "    modelflag = 2\n",
    "    FitPredictions = DLprediction(ReshapedSequencesTOT, RawInputPredictionsTOT, MyFFFFWNPFLink, modelflag, TFTWeights, LabelFit ='TFT')\n",
    "    if DL2:\n",
    "      SensitivityAnalysis = DLprediction2F(ReshapedSequencesTOT, RawInputPredictionsTOT, MyFFFFWNPFLink, modelflag)\n",
    "    # Input Predictions RawInputPredictionsTOT for DLPrediction are ordered Sequence #, Location but\n",
    "    # Input Predictions ReshapedPredictionsTOT for TFTSaveandInterpret are ordered Location, Sequence#\n",
    "    # Note TFT maximum Sequence # is one larger than FFFFWNPF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01t-W_fPM4PW"
   },
   "source": [
    "# View TFT Results Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoq1flxOgBrB"
   },
   "source": [
    "<b> Formulae: </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20000,
     "referenced_widgets": [
      "d6917dd6667b4c16961af7c649336954",
      "04da97cf24a14850aa9af1d66e1d9069",
      "f3b5e57f79474be1a3879e46186b3b16",
      "18fdc5f446a247189ad0104ca00fee81",
      "4786adc5e0cb4bc5af1db61c58748d45",
      "003b9198837b4dcbb0445e80ad36923c",
      "86794ad5589c46699792cbfe418a1603",
      "5ed51a461429446a8182866d81261418",
      "4693e02c21d64f66a9235eaac9e4fdf7",
      "4aaa61af724540b68ddbb56368ecc015",
      "476a31016895495e95bf282eccd1342f"
     ]
    },
    "id": "gNqqi5jNn0xD",
    "outputId": "574b2f6c-a0d0-48b3-ebb4-645a7d9c5bf1"
   },
   "outputs": [],
   "source": [
    "SkipDL2F = True\n",
    "VisualizeTFT(output_map, TFTAttentionWeights)\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuARcncFTC39"
   },
   "source": [
    "### More TFT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftN756zU9gLQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "TFTnum_static = TFTScaledStaticInputs.shape[1]\n",
    "if TFTCategoryCountsStaticInputs.shape[0] != TFTnum_static:\n",
    "  printexit('Inconsistent static Shape)')\n",
    "TFTNumDynamicVariables = TFTScaledDynamicInputs.shape[2]\n",
    "if TFTCategoryCountsDynamicInputs.shape[0] != TFTNumDynamicVariables:\n",
    "  printexit('Inconsistent static Shape)')\n",
    "TFTNumDynamicHistoryVariables = 0\n",
    "TFTNumDynamicFutureVariables = 0\n",
    "\n",
    "\n",
    "\n",
    "TFTFutureInputLSTM = False\n",
    "if TFTNumDynamicFutureVariables == 0:\n",
    "  TFTFutureInputLSTM = False\n",
    "else:\n",
    "  TFTFutureInputLSTM = True"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CovidA21-TFTExtendedDates-Rurality-Rur4.5-MAD1-2-JulyCutoff",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "003b9198837b4dcbb0445e80ad36923c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04da97cf24a14850aa9af1d66e1d9069": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_003b9198837b4dcbb0445e80ad36923c",
      "placeholder": "",
      "style": "IPY_MODEL_86794ad5589c46699792cbfe418a1603",
      "value": "Predict loop: 100%"
     }
    },
    "0fefd65637224b83ae31f4a25020d9b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fd4d6d89c76b4ae5b53e2a84a8540df6",
       "IPY_MODEL_76089de79f784bcb8f31fed7ead393be",
       "IPY_MODEL_ae521a51829149f386d0b8e6492817f2"
      ],
      "layout": "IPY_MODEL_a55623b7b2ed4432bb8e64f32f0073e8"
     }
    },
    "18fdc5f446a247189ad0104ca00fee81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4aaa61af724540b68ddbb56368ecc015",
      "placeholder": "",
      "style": "IPY_MODEL_476a31016895495e95bf282eccd1342f",
      "value": " 475/475 [1:03:35&lt;00:00, 25.51sequences/s, Call=475, TotalLoss=0.00313]"
     }
    },
    "27500e262ae943888d6eec960ba705e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a4b4fe25593420885d95be763b8c83b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33a6f440ddd04eeb97e97f551e059c7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4693e02c21d64f66a9235eaac9e4fdf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "476a31016895495e95bf282eccd1342f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4786adc5e0cb4bc5af1db61c58748d45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aaa61af724540b68ddbb56368ecc015": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c2b55daad0e4245bc45b8e76e269571": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ed51a461429446a8182866d81261418": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76089de79f784bcb8f31fed7ead393be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c2b55daad0e4245bc45b8e76e269571",
      "max": 475,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8abe47fa4cd647f5b8e18b1d553688c8",
      "value": 475
     }
    },
    "86794ad5589c46699792cbfe418a1603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8abe47fa4cd647f5b8e18b1d553688c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a55623b7b2ed4432bb8e64f32f0073e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae521a51829149f386d0b8e6492817f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc5f196aefab4e0aa06438258730fa7d",
      "placeholder": "",
      "style": "IPY_MODEL_2a4b4fe25593420885d95be763b8c83b",
      "value": " 475/475 [00:40&lt;00:00, 23.07sequences/s, Call=475, TotalLoss=0.00313]"
     }
    },
    "bc5f196aefab4e0aa06438258730fa7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6917dd6667b4c16961af7c649336954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04da97cf24a14850aa9af1d66e1d9069",
       "IPY_MODEL_f3b5e57f79474be1a3879e46186b3b16",
       "IPY_MODEL_18fdc5f446a247189ad0104ca00fee81"
      ],
      "layout": "IPY_MODEL_4786adc5e0cb4bc5af1db61c58748d45"
     }
    },
    "f3b5e57f79474be1a3879e46186b3b16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ed51a461429446a8182866d81261418",
      "max": 475,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4693e02c21d64f66a9235eaac9e4fdf7",
      "value": 475
     }
    },
    "fd4d6d89c76b4ae5b53e2a84a8540df6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27500e262ae943888d6eec960ba705e9",
      "placeholder": "",
      "style": "IPY_MODEL_33a6f440ddd04eeb97e97f551e059c7e",
      "value": "Predict loop: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
